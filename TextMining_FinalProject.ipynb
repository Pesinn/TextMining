{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "appreciated-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from datetime import date, time, datetime\n",
    "\n",
    "import calendar\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fitting-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should only return 9:  returns:  9\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_number(numberFrom, numberTo, exlude):\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        rand = random.randint(numberFrom, numberTo)\n",
    "        try:\n",
    "            exlude.index(rand)\n",
    "        except:\n",
    "            found = True\n",
    "    return rand\n",
    "\n",
    "# Returns only 9\n",
    "print(\"Should only return 9: \", \"returns: \", random_number(0, 10, [0,1,2,3,4,5,6,7,8,10]))\n",
    "\n",
    "def sections_to_analyze():\n",
    "    return [\n",
    "        \"title_standardized\",\n",
    "        \"description_standardized\"\n",
    "    ]\n",
    "\n",
    "def languages():\n",
    "    return [\n",
    "        \"english\",\n",
    "        \"french\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "alien-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Leave empty if all domains should be processed\n",
    "_domain = \"\"#[\"wsj.com\"]\n",
    "_folder = \"release\"\n",
    "_dateFrom = \"2020\"\n",
    "\n",
    "_filter = ['covid-19', 'covid']\n",
    "_testThreshold = 0\n",
    "_debug = False\n",
    "\n",
    "# Number of most popular named entities displayed\n",
    "_namedEntities = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "informational-terminal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all domains if no domain is specified\n",
    "def get_domains():\n",
    "    if(_domain == \"\"):\n",
    "        domains = []\n",
    "        for name in listdir(_folder+\"/.\"):\n",
    "            domains.append(name)\n",
    "        return domains\n",
    "    else:\n",
    "        return _domain\n",
    "\n",
    "def convert_to_datetime(_date):\n",
    "    if(len(_date) <= 3 or len(_date) == 5 or len(_date) == 7 or len(_date) > 8):\n",
    "        raise Exception(f\"Input date cannot include {len(_date)} digits - it must contain 4,6 or 8\")\n",
    "    if(len(_date) == 4):\n",
    "        return date(year=int(_date[0:4]), month=1, day=1)\n",
    "    if(len(_date) == 6):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=1)\n",
    "    if(len(_date) == 8):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=int(_date[6:8]))\n",
    "    \n",
    "def compare_date_to_inputdate(_date, dateFrom, dateTo):\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        return True\n",
    "\n",
    "    criteria = convert_to_datetime(_date)\n",
    "    _dateFrom = convert_to_datetime(\"19000101\")\n",
    "    _dateTo = date.today()\n",
    "\n",
    "    if(dateFrom != \"\"):\n",
    "        _dateFrom = convert_to_datetime(dateFrom) \n",
    "    if(dateTo != \"\"):\n",
    "        _dateTo = convert_to_datetime(dateTo)     \n",
    "        \n",
    "    if(_dateFrom <= criteria and criteria <= _dateTo):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\"\"\" Creates an array with all necessary information for eah article\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "domains: list\n",
    "    Example: ['france24.com', 'bbc.com']\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "list\n",
    "    List of articles content with\n",
    "    {id, domain, title, description}\n",
    "\n",
    "\"\"\"\n",
    "def get_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    if(dateFrom == \"\"):\n",
    "        returnDateFrom = convert_to_datetime(\"19000101\")\n",
    "    else:\n",
    "        returnDateFrom = convert_to_datetime(dateFrom)\n",
    "\n",
    "    if(dateTo == \"\"):\n",
    "        returnDateTo = date.today()\n",
    "    else:\n",
    "        returnDateTo = convert_to_datetime(dateTo)\n",
    "\n",
    "    articles = []\n",
    "    article = {}\n",
    "    alreadyProcessed = []\n",
    "    getAll = False\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        getAll = True\n",
    "\n",
    "    for domain in domains:\n",
    "        for f in listdir(join(_folder+\"/\"+domain, \"per_day\")):\n",
    "            # Takes first 4 numbers from filename (e.g. filename: 20190104.gz)\n",
    "            if(getAll or compare_date_to_inputdate(f[0:8], dateFrom, dateTo)):\n",
    "                try:\n",
    "                    d = json.load(gzip.open(join(_folder+\"/\"+domain, \"per_day\", f)))\n",
    "                except:\n",
    "                    continue\n",
    "                for i in d:\n",
    "                    # Prevent articles to be added more than once\n",
    "                    if i not in alreadyProcessed:\n",
    "                        alreadyProcessed.append(i)         \n",
    "                        articles.append({\n",
    "                            \"id\": i,\n",
    "                            \"domain\": domain,\n",
    "                            \"title\": d[i][\"title\"],\n",
    "                            \"description\": d[i][\"description\"],\n",
    "                            \"date\": f[0:8]\n",
    "                        })\n",
    "                    else:\n",
    "                        continue\n",
    "    return articles, returnDateFrom, returnDateTo\n",
    "\n",
    "def get_articles_by_month(domains, year, month):\n",
    "    start, dayTo = calendar.monthrange(int(year), int(month))\n",
    "\n",
    "    if(dayTo < 10):\n",
    "        dayTo = \"0\"+str(dayTo)\n",
    "    else:\n",
    "        dayTo = str(dayTo)\n",
    "\n",
    "    # Adding in front of single digit\n",
    "    if(len(month) == 1):\n",
    "        month = \"0\"+month\n",
    "\n",
    "    dateFrom = year+month+\"01\"\n",
    "    dateTo = year+month+dayTo\n",
    "\n",
    "    return get_articles(domains, dateFrom, dateTo)\n",
    "\n",
    "#get_articles_by_month(get_domains(), \"2018\", \"2\")\n",
    "#get_articles_by_month([\"wsj.com\"], \"2020\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "young-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization NLTK encapsulation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "_wordnetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _sentence_tokenize_NLTK(text):\n",
    "    return sent_tokenize(text)\n",
    "    \n",
    "def _word_tokenize_NLTK(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# {playing, plays, played = play}, {am, are, is = be}\n",
    "def _lemmatize_NLTK(text):\n",
    "    return _wordnetLemmatizer.lemmatize(text)\n",
    "\n",
    "def _speech_tag_NLTK(wordList):\n",
    "    return nltk.pos_tag(wordList)\n",
    "\n",
    "def _named_entities_chunk_NLTK(taggedList):\n",
    "    return nltk.ne_chunk(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "binary-cemetery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "# Standardization default logic\n",
    "\n",
    "\n",
    "\n",
    "# Gets all letters before the index value\n",
    "def get_prefix(text, i):\n",
    "    return text[:i]\n",
    "\n",
    "# Gets all letters after the index value\n",
    "def get_postfix(text, i):\n",
    "    try:\n",
    "        return text[i+1:]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_postfix_including_current(text, i):\n",
    "    try:\n",
    "        return text[i:]\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def isUpper(text, i):\n",
    "    try:\n",
    "        if(text[i].isupper()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def isLower(text, i):\n",
    "    try:\n",
    "        if(text[i].islower()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def isNumber(text, i):\n",
    "    try:\n",
    "        if(text[i].isnumeric()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_punctuation_for_sentence_tokenize(letter):\n",
    "    punctuations = \".?!\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_punctuation_for_word_tokenize(letter):\n",
    "    punctuations = \".?'’:!,\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_end_of_sentence(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "    # If this is the last character\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If we are at space character\n",
    "    if(text[i] == \" \"):\n",
    "        if(prefix != \"\"):\n",
    "            if(is_punctuation_for_sentence_tokenize(text[i-1])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def string_contains_only(_str, char):\n",
    "    for i in _str:\n",
    "        if(i != char):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Standardization default encapsulation\n",
    "def _sentence_tokenize_DEFAULT(text, _list=[]):\n",
    "    i = 0\n",
    "    for l in text:\n",
    "        if(is_end_of_sentence(text, i)):\n",
    "            _list.append(get_prefix(text, i+1).strip())\n",
    "            _sentence_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "# Only returns true for last letter in acronym\n",
    "# E.g. \n",
    "# Letter: A   Prefix: U.S.   Postfix: .\n",
    "# A is the last letter in U.S.A. therefore it's True\n",
    "def is_last_in_acronym(letter, prefix, postfix):\n",
    "    if(letter.isalpha() == False):\n",
    "        return False\n",
    "\n",
    "    if(postfix != \"\"):\n",
    "        if(postfix[0] == \".\"):\n",
    "            if(prefix != \"\"):\n",
    "                if(prefix[len(prefix)-1] == \".\"):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def is_end_of_word(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "\n",
    "    # If last letter\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If punctuation and next letter is space\n",
    "    if(is_punctuation_for_word_tokenize(text[i])):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        else:\n",
    "            # Handle \"...ab\" to \"... ab\"\n",
    "            if(prefix == \"\" or is_punctuation_for_word_tokenize(prefix[i-1])):\n",
    "                if(postfix[0].isalpha() or postfix[0].isnumeric()):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # If we have a letter or number\n",
    "    if(text[i].isalpha() or text[i].isnumeric()):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        # If next letter is a punctuation\n",
    "        elif(is_punctuation_for_word_tokenize(postfix[0])):\n",
    "            innerPostfix = get_postfix(text, i+1)\n",
    "            # Spot 's ending of words\n",
    "            if(postfix[0] == \"'\" or postfix[0] == \"’\"):\n",
    "                if(innerPostfix != \"\"):\n",
    "                    if(innerPostfix[0] == \"s\"):\n",
    "                        return True\n",
    "            \n",
    "            # Handles \"Acronyms\"\n",
    "            if(postfix[0] == \".\"):\n",
    "                # Check if this belongs to a acronym, then return False\n",
    "                if(is_last_in_acronym(text[i], prefix, postfix)):\n",
    "                    return False\n",
    "\n",
    "            # If second next letter is space (to cover someURL.com)\n",
    "            if(innerPostfix != \"\"):\n",
    "                if(is_punctuation_for_word_tokenize(innerPostfix[0])):\n",
    "                    return True\n",
    "                if(innerPostfix[0] != \" \"):\n",
    "                    return False\n",
    "                return True\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def word_tokenize_DEFAULT(text, _list=[]):\n",
    "    i=0\n",
    "    word = \"\"\n",
    "    for l in text:\n",
    "        word+=l\n",
    "        if(is_end_of_word(text, i)):\n",
    "            if(word != \"\"):\n",
    "                _list.append(word.strip())\n",
    "            word_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "def _word_tokenize_DEFAULT(text):\n",
    "    return word_tokenize_DEFAULT(text, [])\n",
    "\n",
    "# {playing, plays, played = play}, {am, are, is = be}\n",
    "def _lemmatize_DEFAULT(text):\n",
    "    return 1\n",
    "\n",
    "def _speech_tag_DEFAULT(wordList):\n",
    "    return 1\n",
    "\n",
    "def _named_entities_chunk_DEFAULT(taggedList):\n",
    "    return 1\n",
    "\n",
    "\n",
    "#print(is_end_of_sentence(\"Or what.\", 7))\n",
    "#print(is_punctuation(\".\"))\n",
    "#print(_sentence_tokenize_NLTK(\"This is the U.S. Coast Guard. Why do we care? Or what.\"))#\n",
    "#print(_sentence_tokenize_DEFAULT(\"This is the U.S. Coast Guard. Why do we care? Or what.\"))\n",
    "\n",
    "#print(_word_tokenize_DEFAULT(\"Trying. To hold the center can be lonely when colleagues disagree. Here are four lessons from the playbook of Supreme Court “umpire” John Roberts.\"))\n",
    "s1 = \"U.S.A.\"\n",
    "\n",
    "#ss = \"'\"\n",
    "#if(ss[0] == \"'\"):\n",
    "#    print(\"true\")\n",
    "print(_word_tokenize_DEFAULT(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "focal-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "_useDefault = True\n",
    "\n",
    "def _sentence_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _sentence_tokenize_DEFAULT(text, [])\n",
    "    else:\n",
    "        return _sentence_tokenize_NLTK(text)\n",
    "\n",
    "def _word_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _word_tokenize_DEFAULT(text)\n",
    "    else:\n",
    "        return _word_tokenize_NLTK(text)\n",
    "    \n",
    "def _lemmatize(text):\n",
    "    if(False):\n",
    "        return _lemmatize_DEFAULT(text)\n",
    "    else:\n",
    "        return _lemmatize_NLTK(text)\n",
    "\n",
    "def _stopwords():\n",
    "    stopWordsList = set()\n",
    "    for l in languages():\n",
    "        stopWordsList = set.union(stopWordsList, set(stopwords.words(l)))\n",
    "    return stopWordsList\n",
    "\n",
    "def _speech_tag(wordList):\n",
    "    if(False):\n",
    "        return _speech_tag_DEFAULT(wordList)\n",
    "    else:\n",
    "        return _speech_tag_NLTK(wordList)\n",
    "\n",
    "def _named_entities_chunk(taggedList):\n",
    "    if(False):\n",
    "        return _named_entities_chunk_DEFAULT(taggedList)\n",
    "    else:\n",
    "        return _named_entities_chunk_NLTK(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "sexual-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Airlines', 'likely', 'have', 'enough', 'cash', 'to', 'withstand', 'a', 'prolonged', 'downturn', ',', 'analysts', 'say', '.']\n",
      "['Airlines', 'likely', 'have', 'enough', 'cash', 'to', 'withstand', 'a', 'prolonged', 'downturn', ',', 'analysts', 'say', '.']\n",
      "['But', 'it', 'will', 'be', 'years', 'before', 'passenger', 'demand', 'recovers', ',', 'chief', 'United', 'and', 'Delta', 'executives', 'said', 'this', 'week', '.']\n",
      "['But', 'it', 'will', 'be', 'years', 'before', 'passenger', 'demand', 'recovers', ',', 'chief', 'United', 'and', 'Delta', 'executives', 'said', 'this', 'week', '.']\n",
      "['View', 'membership', 'options', 'and', 'subscribe', 'to', 'America', \"'\", 's', 'most', 'trusted', 'newspaper', 'for', 'full', 'access', 'to', 'WSJ.com', ',', 'WSJ', 'apps', ',', 'WSJplus', 'premium', 'benefits', 'and', 'curated', 'newsletters', '.']\n",
      "['View', 'membership', 'options', 'and', 'subscribe', 'to', 'America', \"'s\", 'most', 'trusted', 'newspaper', 'for', 'full', 'access', 'to', 'WSJ.com', ',', 'WSJ', 'apps', ',', 'WSJplus', 'premium', 'benefits', 'and', 'curated', 'newsletters', '.']\n",
      "['Dow', 'Jones']\n",
      "['Dow', 'Jones']\n",
      "['To', 'stimulate', 'its', 'pandemic-hit', 'economy', ',', 'a', 'province', 'in', 'South', 'Korea', 'has', 'been', 'experimenting', 'with', 'universal', 'basic', 'income', 'programs', 'by', 'regularly', 'giving', 'out', 'cash', ',', 'no', 'questions', 'asked', '.']\n",
      "['To', 'stimulate', 'its', 'pandemic-hit', 'economy', ',', 'a', 'province', 'in', 'South', 'Korea', 'has', 'been', 'experimenting', 'with', 'universal', 'basic', 'income', 'programs', 'by', 'regularly', 'giving', 'out', 'cash', ',', 'no', 'questions', 'asked', '.']\n",
      "['Now', ',', 'some', 'politicians', 'want', 'to', 'go', 'national', 'with', 'the', 'concept', '.']\n",
      "['Now', ',', 'some', 'politicians', 'want', 'to', 'go', 'national', 'with', 'the', 'concept', '.']\n",
      "['Illustration', ':', 'Crystal', 'Tai/WSJ']\n",
      "['Illustration', ':', 'Crystal', 'Tai/WSJ']\n",
      "['The', 'U.S.']\n",
      "['The', 'U.S', '.']\n",
      "['is', 'one', 'of', 'the', 'few', 'countries', 'that', 'requires', 'its', 'citizens', 'to', 'file', 'taxes', 'while', 'living', 'abroad', '.']\n",
      "['is', 'one', 'of', 'the', 'few', 'countries', 'that', 'requires', 'its', 'citizens', 'to', 'file', 'taxes', 'while', 'living', 'abroad', '.']\n",
      "['For', 'some', ',', 'it', 'is', 'enough', 'to', 'push', 'them', 'to', 'give', 'up', 'their', 'citizenship', '.']\n",
      "['For', 'some', ',', 'it', 'is', 'enough', 'to', 'push', 'them', 'to', 'give', 'up', 'their', 'citizenship', '.']\n",
      "['Trying', 'to', 'hold', 'the', 'center', 'can', 'be', 'lonely', 'when', 'colleagues', 'disagree', '.']\n",
      "['Trying', 'to', 'hold', 'the', 'center', 'can', 'be', 'lonely', 'when', 'colleagues', 'disagree', '.']\n",
      "['Here', 'are', 'four', 'lessons', 'from', 'the', 'playbook', 'of', 'Supreme', 'Court', '“umpire” John', 'Roberts', '.']\n",
      "['Here', 'are', 'four', 'lessons', 'from', 'the', 'playbook', 'of', 'Supreme', 'Court', '“', 'umpire', '”', 'John', 'Roberts', '.']\n",
      "['Latest', 'from', '“Future', 'View” in', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Latest', 'from', '“', 'Future', 'View', '”', 'in', 'The', 'Wall', 'Street', 'Journal']\n",
      "['The', 'amendment', ',', 'passed', 'by', 'the', 'SEC', 'on', 'Friday', ',', 'gives', 'auditors', 'more', 'discretion', 'in', 'assessing', 'conflicts', 'of', 'interest', 'in', 'their', 'relationships', 'with', 'the', 'businesses', 'they', 'audit', ',', 'in', 'situations', 'such', 'as', 'those', 'involving', 'affiliates', 'of', 'their', 'clients', 'or', 'past', 'lenders', '.']\n",
      "['The', 'amendment', ',', 'passed', 'by', 'the', 'SEC', 'on', 'Friday', ',', 'gives', 'auditors', 'more', 'discretion', 'in', 'assessing', 'conflicts', 'of', 'interest', 'in', 'their', 'relationships', 'with', 'the', 'businesses', 'they', 'audit', ',', 'in', 'situations', 'such', 'as', 'those', 'involving', 'affiliates', 'of', 'their', 'clients', 'or', 'past', 'lenders', '.']\n",
      "['The', 'rule', 'also', 'intends', 'to', 'lessen', 'the', 'burden', 'for', 'companies', 'trying', 'to', 'go', 'public', '.']\n",
      "['The', 'rule', 'also', 'intends', 'to', 'lessen', 'the', 'burden', 'for', 'companies', 'trying', 'to', 'go', 'public', '.']\n",
      "['Wuhan', ',', 'the', 'city', 'at', 'the', 'center', 'of', 'the', 'coronavirus', 'pandemic', ',', 'had', 'the', 'most', 'tourists', 'of', 'any', 'Chinese', 'city', 'during', 'a', 'public', 'holiday', 'in', 'October', '.']\n",
      "['Wuhan', ',', 'the', 'city', 'at', 'the', 'center', 'of', 'the', 'coronavirus', 'pandemic', ',', 'had', 'the', 'most', 'tourists', 'of', 'any', 'Chinese', 'city', 'during', 'a', 'public', 'holiday', 'in', 'October', '.']\n",
      "['Wuhan', 'is', 'overcoming', 'its', 'pandemic', 'past', 'and', 'benefiting', 'from', 'its', 'hero-city', 'status', 'to', 'become', 'a', 'top', 'travel', 'destination', '.']\n",
      "['Wuhan', 'is', 'overcoming', 'its', 'pandemic', 'past', 'and', 'benefiting', 'from', 'its', 'hero-city', 'status', 'to', 'become', 'a', 'top', 'travel', 'destination', '.']\n",
      "['Photo', ':', 'Getty', 'Images']\n",
      "['Photo', ':', 'Getty', 'Images']\n",
      "['Welcome', 'to', 'Noted', ',', 'a', 'news', 'and', 'culture', 'magazine', 'from', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['Welcome', 'to', 'Noted', ',', 'a', 'news', 'and', 'culture', 'magazine', 'from', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['Gerry', 'Baker', 'is', 'Editor-at-Large', 'of', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['Gerry', 'Baker', 'is', 'Editor-at-Large', 'of', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['His', 'weekly', 'column', 'for', 'the', 'editorial', 'page', ',', '“Free', 'Expression,” appears', 'in', 'The', 'Wall', 'Street', 'Journal', 'each', 'Tuesday', '.']\n",
      "['His', 'weekly', 'column', 'for', 'the', 'editorial', 'page', ',', '“', 'Free', 'Expression', ',', '”', 'appears', 'in', 'The', 'Wall', 'Street', 'Journal', 'each', 'Tuesday', '.']\n",
      "['Mr', '.']\n",
      "['Mr', '.']\n",
      "['Baker', 'is', 'also', 'host', 'of', '“WSJ', 'at', 'Large', 'with', 'Gerry', 'Baker,” a', 'weekly', 'news', 'and', 'current', 'affairs', 'interview', 'show', 'on', 'the', 'Fox', 'Business', 'Network', '.']\n",
      "['Baker', 'is', 'also', 'host', 'of', '“', 'WSJ', 'at', 'Large', 'with', 'Gerry', 'Baker', ',', '”', 'a', 'weekly', 'news', 'and', 'current', 'affairs', 'interview', 'show', 'on', 'the', 'Fox', 'Business', 'Network', '.']\n",
      "['Women', 'have', 'good', 'reason', 'to', 'be', 'more', 'anxious', 'about', 'the', 'size', 'of', 'a', 'nest', 'egg', '.']\n",
      "['Women', 'have', 'good', 'reason', 'to', 'be', 'more', 'anxious', 'about', 'the', 'size', 'of', 'a', 'nest', 'egg', '.']\n",
      "['As', 'companies', 'reopen', 'their', 'offices', 'and', 'we', 'shift', 'away', 'from', 'WFH', ',', 'there', '’', 's', 'a', 'lot', 'to', 'think', 'about—from', 'the', 'post-pandemic', 'dress', 'code', 'to', 'whether', 'your', 'boss', 'can', 'force', 'you', 'to', 'get', 'a', 'vaccine', '.']\n",
      "['As', 'companies', 'reopen', 'their', 'offices', 'and', 'we', 'shift', 'away', 'from', 'WFH', ',', 'there', '’', 's', 'a', 'lot', 'to', 'think', 'about—from', 'the', 'post-pandemic', 'dress', 'code', 'to', 'whether', 'your', 'boss', 'can', 'force', 'you', 'to', 'get', 'a', 'vaccine', '.']\n",
      "['We’ve', 'tackled', 'it', 'all', '.']\n",
      "['We', '’', 've', 'tackled', 'it', 'all', '.']\n",
      "['A', 'group', 'of', 'information', 'technology', 'companies', 'filed', 'a', 'lawsuit', 'challenging', 'a', 'Trump', 'administration', 'rule', 'raising', 'the', 'salaries', 'that', 'employers', 'must', 'pay', 'to', 'their', 'foreign', 'workers', 'on', 'H-1B', 'visas', ',', 'aimed', 'at', 'tightening', 'eligibility', 'for', 'the', 'visas', '.']\n",
      "['A', 'group', 'of', 'information', 'technology', 'companies', 'filed', 'a', 'lawsuit', 'challenging', 'a', 'Trump', 'administration', 'rule', 'raising', 'the', 'salaries', 'that', 'employers', 'must', 'pay', 'to', 'their', 'foreign', 'workers', 'on', 'H-1B', 'visas', ',', 'aimed', 'at', 'tightening', 'eligibility', 'for', 'the', 'visas', '.']\n",
      "['An', 'open', 'letter', 'criticized', 'the', 'nation', '’', 's', 'public-health', 'response', 'to', 'the', 'Covid-19', 'pandemic', 'and', 'called', 'for', 'the', 'federal', 'agency', 'to', 'play', 'a', 'more', 'central', 'role', '.']\n",
      "['An', 'open', 'letter', 'criticized', 'the', 'nation', '’', 's', 'public-health', 'response', 'to', 'the', 'Covid-19', 'pandemic', 'and', 'called', 'for', 'the', 'federal', 'agency', 'to', 'play', 'a', 'more', 'central', 'role', '.']\n",
      "['Keep', 'your', 'patrons', 'inspired', 'with', 'the', 'insights', 'they', 'need', 'from', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Keep', 'your', 'patrons', 'inspired', 'with', 'the', 'insights', 'they', 'need', 'from', 'The', 'Wall', 'Street', 'Journal']\n",
      "['In', 'an', 'already', 'strange', 'year', 'for', 'Broadway', ',', 'theatrical', 'veteran', 'Aaron', 'Tveit', 'is', 'the', 'only', 'nominee', 'for', 'best', 'lead', 'actor', 'in', 'musical; “a', 'huge', 'honor.”']\n",
      "['In', 'an', 'already', 'strange', 'year', 'for', 'Broadway', ',', 'theatrical', 'veteran', 'Aaron', 'Tveit', 'is', 'the', 'only', 'nominee', 'for', 'best', 'lead', 'actor', 'in', 'musical', ';', '“', 'a', 'huge', 'honor', '.', '”']\n",
      "['Police', 'shot', 'and', 'killed', 'a', 'suspected', 'assailant', 'after', 'he', 'attacked', 'the', 'teacher', 'with', 'a', 'knife', 'in', 'a', 'suburb', 'west', 'of', 'Paris', '.']\n",
      "['Police', 'shot', 'and', 'killed', 'a', 'suspected', 'assailant', 'after', 'he', 'attacked', 'the', 'teacher', 'with', 'a', 'knife', 'in', 'a', 'suburb', 'west', 'of', 'Paris', '.']\n",
      "['Research', 'shows', 'that', 'when', 'somebody', 'donates', 'to', 'a', 'candidate', 'they', 'are', 'likely', 'to', 'reduce', 'their', 'contributions', 'to', 'charities', '.']\n",
      "['Research', 'shows', 'that', 'when', 'somebody', 'donates', 'to', 'a', 'candidate', 'they', 'are', 'likely', 'to', 'reduce', 'their', 'contributions', 'to', 'charities', '.']\n",
      "['Read', 'the', 'latest', 'headlines', 'and', 'breaking', 'news', 'today', 'from', 'The', 'Wall', 'Street', 'Journal', ',', 'covering', 'U.S.']\n",
      "['Read', 'the', 'latest', 'headlines', 'and', 'breaking', 'news', 'today', 'from', 'The', 'Wall', 'Street', 'Journal', ',', 'covering', 'U.S', '.']\n",
      "['and', 'World', 'News', ',', 'Markets', ',', 'Business', ',', 'Economy', 'Politics', ',', 'Life', 'and', 'Arts', 'and', 'Opinion']\n",
      "['and', 'World', 'News', ',', 'Markets', ',', 'Business', ',', 'Economy', 'Politics', ',', 'Life', 'and', 'Arts', 'and', 'Opinion']\n",
      "['The', 'arrest', 'of', 'Mexico', '’', 's', 'former', 'defense', 'minister', ',', 'Salvador', 'Cienfuegos', ',', 'by', 'U.S.']\n",
      "['The', 'arrest', 'of', 'Mexico', '’', 's', 'former', 'defense', 'minister', ',', 'Salvador', 'Cienfuegos', ',', 'by', 'U.S', '.']\n",
      "['authorities', 'in', 'Los', 'Angeles', 'shows', 'that', 'corruption', 'is', 'Mexico', '’', 's', 'biggest', 'problem', ',', 'President', 'Andrés', 'Manuel', 'López', 'Obrador', 'said', '.']\n",
      "['authorities', 'in', 'Los', 'Angeles', 'shows', 'that', 'corruption', 'is', 'Mexico', '’', 's', 'biggest', 'problem', ',', 'President', 'Andrés', 'Manuel', 'López', 'Obrador', 'said', '.']\n",
      "['Nine', 'states', 'reported', 'a', 'record', 'tally', 'of', 'new', 'coronavirus', 'cases', ',', 'as', 'the', 'total', 'number', 'of', 'infections', 'detected', 'in', 'the', 'U.S.']\n",
      "['Nine', 'states', 'reported', 'a', 'record', 'tally', 'of', 'new', 'coronavirus', 'cases', ',', 'as', 'the', 'total', 'number', 'of', 'infections', 'detected', 'in', 'the', 'U.S', '.']\n",
      "['since', 'the', 'pandemic', 'began', 'ticked', 'above', '8', 'million', '.']\n",
      "['since', 'the', 'pandemic', 'began', 'ticked', 'above', '8', 'million', '.']\n",
      "['Apple', 'announced', 'four', 'new', 'iPhone', '12', 'models', 'at', 'a', 'virtual', 'event', 'on', 'Tuesday', '.']\n",
      "['Apple', 'announced', 'four', 'new', 'iPhone', '12', 'models', 'at', 'a', 'virtual', 'event', 'on', 'Tuesday', '.']\n",
      "['They', 'all', 'have', '5G', ',', 'fresh', 'designs', 'and', 'new', 'camera', 'tricks', ',', 'yet', 'they', 'differ', 'in', 'key', 'ways', '.']\n",
      "['They', 'all', 'have', '5G', ',', 'fresh', 'designs', 'and', 'new', 'camera', 'tricks', ',', 'yet', 'they', 'differ', 'in', 'key', 'ways', '.']\n",
      "['WSJ']\n",
      "['WSJ']\n",
      "['The', 'country', '’', 's', 'center-left', 'Labour', 'Party', ',', 'led', 'by', 'Jacinda', 'Ardern', ',', 'won', 'a', 'landslide', 'victory', 'in', 'national', 'elections', 'as', 'voters', 'endorsed', 'her', 'government', '’', 's', 'efforts', 'to', 'stamp', 'out', 'the', 'coronavirus', '.']\n",
      "['The', 'country', '’', 's', 'center-left', 'Labour', 'Party', ',', 'led', 'by', 'Jacinda', 'Ardern', ',', 'won', 'a', 'landslide', 'victory', 'in', 'national', 'elections', 'as', 'voters', 'endorsed', 'her', 'government', '’', 's', 'efforts', 'to', 'stamp', 'out', 'the', 'coronavirus', '.']\n",
      "['As', 'utilities', 'seek', 'approval', 'to', 'invest', 'millions', 'of', 'dollars', 'in', 'upgrading', 'their', 'infrastructure', 'to', 'accommodate', 'car', 'charging', ',', 'their', 'efforts', 'are', 'drawing', 'opposition', 'from', 'consumer', 'advocates', ',', 'oil', 'companies', 'and', 'startups', '.']\n",
      "['As', 'utilities', 'seek', 'approval', 'to', 'invest', 'millions', 'of', 'dollars', 'in', 'upgrading', 'their', 'infrastructure', 'to', 'accommodate', 'car', 'charging', ',', 'their', 'efforts', 'are', 'drawing', 'opposition', 'from', 'consumer', 'advocates', ',', 'oil', 'companies', 'and', 'startups', '.']\n",
      "['Earn', 'an', 'exclusive', 'reward', 'by', 'referring', 'your', 'friends', '.']\n",
      "['Earn', 'an', 'exclusive', 'reward', 'by', 'referring', 'your', 'friends', '.']\n",
      "['Equip', 'your', 'peers', 'for', 'success', 'with', 'award-winning', 'insight', 'from', 'the', 'Wall', 'Street', 'Journal', '.']\n",
      "['Equip', 'your', 'peers', 'for', 'success', 'with', 'award-winning', 'insight', 'from', 'the', 'Wall', 'Street', 'Journal', '.']\n",
      "['Nearly', '100', 'people', 'died', 'during', 'the', 'height', 'of', 'the', 'coronavirus', 'outbreak', 'at', 'the', 'Menlo', 'Park', 'Veterans', 'Memorial', 'Home', 'in', 'April', ',', 'more', 'than', '10', 'times', 'the', 'number', 'in', 'a', 'typical', 'month', '.']\n",
      "['Nearly', '100', 'people', 'died', 'during', 'the', 'height', 'of', 'the', 'coronavirus', 'outbreak', 'at', 'the', 'Menlo', 'Park', 'Veterans', 'Memorial', 'Home', 'in', 'April', ',', 'more', 'than', '10', 'times', 'the', 'number', 'in', 'a', 'typical', 'month', '.']\n",
      "['Among', 'those', 'who', 'died', 'were', '84-year-old', 'Isabella', 'Kovacs', 'and', '86-year-old', 'Joan', 'Williams', '.']\n",
      "['Among', 'those', 'who', 'died', 'were', '84-year-old', 'Isabella', 'Kovacs', 'and', '86-year-old', 'Joan', 'Williams', '.']\n",
      "['Their', 'stories', 'provide', 'a', 'window', 'into', 'what', 'went', 'wrong', 'at', 'the', 'New', 'Jersey', 'facility', '.']\n",
      "['Their', 'stories', 'provide', 'a', 'window', 'into', 'what', 'went', 'wrong', 'at', 'the', 'New', 'Jersey', 'facility', '.']\n",
      "['Photo', ':', 'Shari', 'Davis/Julie', 'Diaz']\n",
      "['Photo', ':', 'Shari', 'Davis/Julie', 'Diaz']\n",
      "['Topic', 'pages', 'collect', 'the', 'latest', ',', 'breaking', 'and', 'archive', 'news', ',', 'photos', ',', 'graphics', ',', 'audio', 'and', 'video', 'published', 'on', 'the', 'topic', 'in', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['Topic', 'pages', 'collect', 'the', 'latest', ',', 'breaking', 'and', 'archive', 'news', ',', 'photos', ',', 'graphics', ',', 'audio', 'and', 'video', 'published', 'on', 'the', 'topic', 'in', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['By', 'some', 'estimates', ',', 'about', '40% of', 'U.S.']\n",
      "['By', 'some', 'estimates', ',', 'about', '40', '%', 'of', 'U.S', '.']\n",
      "['day', 'cares', 'are', 'closed', 'after', 'enrollment', 'fell', 'off', 'in', 'the', 'spring', 'and', 'never', 'fully', 'bounced', 'back', '.']\n",
      "['day', 'cares', 'are', 'closed', 'after', 'enrollment', 'fell', 'off', 'in', 'the', 'spring', 'and', 'never', 'fully', 'bounced', 'back', '.']\n",
      "['Interpublic', 'Group', 'of', 'Cos', '.']\n",
      "['Interpublic', 'Group', 'of', 'Cos', '.']\n",
      "['has', 'promoted', 'Bill', 'Kolb', 'to', 'chairman', 'and', 'CEO', 'of', 'creative', 'agency', 'network', 'McCann', 'Worldgroup', ',', 'the', 'company', 'said', 'Thursday', '.']\n",
      "['has', 'promoted', 'Bill', 'Kolb', 'to', 'chairman', 'and', 'CEO', 'of', 'creative', 'agency', 'network', 'McCann', 'Worldgroup', ',', 'the', 'company', 'said', 'Thursday', '.']\n",
      "['He', 'will', 'succeed', 'Harris', 'Diamond', ',', 'who', 'plans', 'to', 'retire', 'at', 'the', 'end', 'of', 'the', 'year', ',', 'the', 'company', 'said', '.']\n",
      "['He', 'will', 'succeed', 'Harris', 'Diamond', ',', 'who', 'plans', 'to', 'retire', 'at', 'the', 'end', 'of', 'the', 'year', ',', 'the', 'company', 'said', '.']\n",
      "['The', 'shipping', 'company', 'lifted', 'its', 'full-year', 'guidance', ',', 'noting', 'a', 'faster-than-expected', 'rebound', 'in', 'shipping', 'volumes', 'and', 'freight', 'rates', ',', 'but', 'it', 'will', 'take', 'a', '$100', 'million', 'restructuring', 'charge', 'in', 'the', 'third', 'quarter', 'to', 'cover', 'the', 'costs', 'of', 'cutting', '2,000', 'jobs', '.']\n",
      "['The', 'shipping', 'company', 'lifted', 'its', 'full-year', 'guidance', ',', 'noting', 'a', 'faster-than-expected', 'rebound', 'in', 'shipping', 'volumes', 'and', 'freight', 'rates', ',', 'but', 'it', 'will', 'take', 'a', '$', '100', 'million', 'restructuring', 'charge', 'in', 'the', 'third', 'quarter', 'to', 'cover', 'the', 'costs', 'of', 'cutting', '2,000', 'jobs', '.']\n",
      "['President', 'Trump', 'was', 'pressed', 'on', 'his', 'handling', 'of', 'the', 'pandemic', 'during', 'a', 'contentious', 'town', 'hall', ',', 'while', 'Joe', 'Biden', 'was', 'asked', 'in', 'a', 'separate', 'event', 'what', 'he', 'would', 'do', 'differently', '.']\n",
      "['President', 'Trump', 'was', 'pressed', 'on', 'his', 'handling', 'of', 'the', 'pandemic', 'during', 'a', 'contentious', 'town', 'hall', ',', 'while', 'Joe', 'Biden', 'was', 'asked', 'in', 'a', 'separate', 'event', 'what', 'he', 'would', 'do', 'differently', '.']\n",
      "['The', 'findings', 'point', 'to', 'a', 'vulnerability', 'for', 'the', 'economy', 'in', 'the', 'months', 'ahead', ':', 'With', 'smaller', 'savings', 'and', 'no', 'additional', 'economic', 'relief', ',', 'nearly', '11', 'million', 'jobless', 'workers', 'may', 'tighten', 'spending', 'even', 'further', '.']\n",
      "['The', 'findings', 'point', 'to', 'a', 'vulnerability', 'for', 'the', 'economy', 'in', 'the', 'months', 'ahead', ':', 'With', 'smaller', 'savings', 'and', 'no', 'additional', 'economic', 'relief', ',', 'nearly', '11', 'million', 'jobless', 'workers', 'may', 'tighten', 'spending', 'even', 'further', '.']\n",
      "['One', 'reason', 'many', 'Northeastern', 'colleges', 'have', 'experienced', 'few', 'coronavirus', 'outbreaks', 'has', 'been', 'a', 'testing', 'operation', 'run', 'by', 'Broad', 'Institute', 'of', 'MIT', 'and', 'Harvard', 'with', '14-hour', 'turnaround', 'times', 'and', 'low-cost', 'tests', '.']\n",
      "['One', 'reason', 'many', 'Northeastern', 'colleges', 'have', 'experienced', 'few', 'coronavirus', 'outbreaks', 'has', 'been', 'a', 'testing', 'operation', 'run', 'by', 'Broad', 'Institute', 'of', 'MIT', 'and', 'Harvard', 'with', '14-hour', 'turnaround', 'times', 'and', 'low-cost', 'tests', '.']\n",
      "['Senators', 'have', 'a', 'chance', 'to', 'explain', 'why', 'an', 'open', 'society', 'matters', '.']\n",
      "['Senators', 'have', 'a', 'chance', 'to', 'explain', 'why', 'an', 'open', 'society', 'matters', '.']\n",
      "['A', 'physical', 'therapist', 'for', 'Olympians', 'shares', 'exercises', 'to', 'improve', 'core', 'and', 'shoulder', 'stability', '.']\n",
      "['A', 'physical', 'therapist', 'for', 'Olympians', 'shares', 'exercises', 'to', 'improve', 'core', 'and', 'shoulder', 'stability', '.']\n",
      "['As', 'wealthier', 'countries', 'buy', 'up', 'supplies', 'of', 'Western', 'drugmakers', '’', 'Covid-19', 'vaccines', 'that', 'are', 'still', 'in', 'development', ',', 'China', 'and', 'Russia', 'are', 'offering', 'their', 'fast-tracked', 'shots', 'to', 'poorer', 'nations', '.']\n",
      "['As', 'wealthier', 'countries', 'buy', 'up', 'supplies', 'of', 'Western', 'drugmakers', '’', 'Covid-19', 'vaccines', 'that', 'are', 'still', 'in', 'development', ',', 'China', 'and', 'Russia', 'are', 'offering', 'their', 'fast-tracked', 'shots', 'to', 'poorer', 'nations', '.']\n",
      "['Here', '’', 's', 'what', \"they're\", 'hoping', 'to', 'get', 'in', 'return', '.']\n",
      "['Here', '’', 's', 'what', 'they', \"'re\", 'hoping', 'to', 'get', 'in', 'return', '.']\n",
      "['Illustration', ':', 'Ksenia', 'Shaikhutdinova']\n",
      "['Illustration', ':', 'Ksenia', 'Shaikhutdinova']\n",
      "['The', 'appeals', 'court', 'returned', 'the', 'deadline', 'to', '8', 'p.m.']\n",
      "['The', 'appeals', 'court', 'returned', 'the', 'deadline', 'to', '8', 'p.m', '.']\n",
      "['on', 'Election', 'Day', ',', 'the', 'latest', 'decision', 'in', 'a', 'wave', 'of', 'partisan', 'court', 'battles', 'over', 'voting', 'by', 'mail', '.']\n",
      "['on', 'Election', 'Day', ',', 'the', 'latest', 'decision', 'in', 'a', 'wave', 'of', 'partisan', 'court', 'battles', 'over', 'voting', 'by', 'mail', '.']\n",
      "['The', 'South', 'offers', 'a', 'stark', 'example', 'of', 'what', 'happens\\xa0to', 'an', 'economy', 'when', 'measures', 'are', 'less', 'stringent', ':', 'lower', 'unemployment\\xa0rates', ',', 'relatively', 'strong', 'job', 'openings—but', 'a', 'bigger', 'rise', 'in', 'infections', 'and', 'deaths', '.']\n",
      "['The', 'South', 'offers', 'a', 'stark', 'example', 'of', 'what', 'happens', 'to', 'an', 'economy', 'when', 'measures', 'are', 'less', 'stringent', ':', 'lower', 'unemployment', 'rates', ',', 'relatively', 'strong', 'job', 'openings—but', 'a', 'bigger', 'rise', 'in', 'infections', 'and', 'deaths', '.']\n",
      "['The', 'high', 'court', 'said', 'it', 'would', 'decide', 'whether', 'President', 'Trump', 'can', 'exclude', 'illegal', 'immigrants', 'from', 'the', '2020', 'census', 'count', 'used', 'to', 'allocate', 'congressional', 'seats', ',', 'expediting', 'the', 'case', 'with', 'an', 'argument', 'set', 'for', 'Nov', '.']\n",
      "['The', 'high', 'court', 'said', 'it', 'would', 'decide', 'whether', 'President', 'Trump', 'can', 'exclude', 'illegal', 'immigrants', 'from', 'the', '2020', 'census', 'count', 'used', 'to', 'allocate', 'congressional', 'seats', ',', 'expediting', 'the', 'case', 'with', 'an', 'argument', 'set', 'for', 'Nov', '.']\n",
      "['30', '.']\n",
      "['30', '.']\n",
      "['Shaq', ',', 'Playboy', 'and', 'Nikola', 'are', 'all', 'on', 'the', 'blank-check', 'bandwagon', ',', 'part', 'of', 'a', 'boom', 'in', 'SPACs', 'that', 'is', 'unsettling', ',', 'writes', 'columnist', 'James', 'Mackintosh', '.']\n",
      "['Shaq', ',', 'Playboy', 'and', 'Nikola', 'are', 'all', 'on', 'the', 'blank-check', 'bandwagon', ',', 'part', 'of', 'a', 'boom', 'in', 'SPACs', 'that', 'is', 'unsettling', ',', 'writes', 'columnist', 'James', 'Mackintosh', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'surveys', 'a', 'group', 'of', 'more', 'than', '60', 'economists', 'on', 'more', 'than', '10', 'major', 'economic', 'indicators', 'on', 'a', 'monthly', 'basis', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'surveys', 'a', 'group', 'of', 'more', 'than', '60', 'economists', 'on', 'more', 'than', '10', 'major', 'economic', 'indicators', 'on', 'a', 'monthly', 'basis', '.']\n",
      "['The', 'soda', 'company', '’', 's', 'first', 'diet', 'cola', 'was', 'a', 'pop-culture', 'icon', 'in', 'the', '1970s', 'and', 'early', '’80s', ',', 'then', 'faded', 'after', 'the', 'launch', 'of', 'Diet', 'Coke', '.']\n",
      "['The', 'soda', 'company', '’', 's', 'first', 'diet', 'cola', 'was', 'a', 'pop-culture', 'icon', 'in', 'the', '1970s', 'and', 'early', '’', '80s', ',', 'then', 'faded', 'after', 'the', 'launch', 'of', 'Diet', 'Coke', '.']\n",
      "['Coca-Cola', 'kept', 'the', 'brand', 'going', 'for', 'decades', 'to', 'appease', 'a', 'fiercely', 'devoted', 'base', '.']\n",
      "['Coca-Cola', 'kept', 'the', 'brand', 'going', 'for', 'decades', 'to', 'appease', 'a', 'fiercely', 'devoted', 'base', '.']\n",
      "['Some', '14.1', 'million', 'people', 'tuned', 'in', 'to', 'watch', 'Joe', 'Biden', 'on', 'ABC', ',', 'while', '13.5', 'million', 'watched', 'President', 'Trump', 'on', 'NBC', 'platforms', ',', 'according', 'to', 'Nielsen', 'ratings', '.']\n",
      "['Some', '14.1', 'million', 'people', 'tuned', 'in', 'to', 'watch', 'Joe', 'Biden', 'on', 'ABC', ',', 'while', '13.5', 'million', 'watched', 'President', 'Trump', 'on', 'NBC', 'platforms', ',', 'according', 'to', 'Nielsen', 'ratings', '.']\n",
      "['Two', 'avid', 'travelers', 'debate', 'the', 'merits', 'and', 'madness', 'of', 'vacationing', 'overseas', 'right', 'now', '.']\n",
      "['Two', 'avid', 'travelers', 'debate', 'the', 'merits', 'and', 'madness', 'of', 'vacationing', 'overseas', 'right', 'now', '.']\n",
      "['Plus', ':', 'A', 'look', 'at', '5', 'of', 'the', 'few—and', 'especially', 'tempting—countries', 'that', 'now', 'welcome', 'U.S.']\n",
      "['Plus', ':', 'A', 'look', 'at', '5', 'of', 'the', 'few—and', 'especially', 'tempting—countries', 'that', 'now', 'welcome', 'U.S', '.']\n",
      "['tourists', '.']\n",
      "['tourists', '.']\n",
      "['Loop', 'Industries', 'denied', 'accusations', 'by', 'short', 'seller', 'Hindenburg', 'Research', 'that', 'the', 'company', 'lied', 'about', 'its', 'recyclable-plastic', 'technology', '.']\n",
      "['Loop', 'Industries', 'denied', 'accusations', 'by', 'short', 'seller', 'Hindenburg', 'Research', 'that', 'the', 'company', 'lied', 'about', 'its', 'recyclable-plastic', 'technology', '.']\n",
      "['After', 'focusing', 'on', 'a', 'trade', 'deal', ',', 'President', 'Trump', 'has', 'toughened', 'his', 'stance', 'toward', 'China', ',', 'raising', 'the', 'prominence', 'of', 'hard-liners', 'in', 'his', 'administration', '.']\n",
      "['After', 'focusing', 'on', 'a', 'trade', 'deal', ',', 'President', 'Trump', 'has', 'toughened', 'his', 'stance', 'toward', 'China', ',', 'raising', 'the', 'prominence', 'of', 'hard-liners', 'in', 'his', 'administration', '.']\n",
      "['The', 'Dow', 'and', 'S&P', '500', 'ticked', 'up', 'as', 'retail', 'sales', 'data', 'beat', 'expectations', ',', 'capping', 'a', 'volatile', 'week', 'on', 'Wall', 'Street', '.']\n",
      "['The', 'Dow', 'and', 'S', '&', 'P', '500', 'ticked', 'up', 'as', 'retail', 'sales', 'data', 'beat', 'expectations', ',', 'capping', 'a', 'volatile', 'week', 'on', 'Wall', 'Street', '.']\n",
      "[\"We're\", 'here', 'to', 'tackle', 'your', 'questions', 'about', 'money', 'amid', 'the', 'coronavirus', 'pandemic', '.']\n",
      "['We', \"'re\", 'here', 'to', 'tackle', 'your', 'questions', 'about', 'money', 'amid', 'the', 'coronavirus', 'pandemic', '.']\n",
      "['Here', \"you'll\", 'find', 'the', 'latest', 'information', 'on', 'stimulus', 'money', ',', 'student', 'loans', ',', 'mortgages', ',', 'unemployment', ',', 'and', 'taxes', '.']\n",
      "['Here', 'you', \"'ll\", 'find', 'the', 'latest', 'information', 'on', 'stimulus', 'money', ',', 'student', 'loans', ',', 'mortgages', ',', 'unemployment', ',', 'and', 'taxes', '.']\n",
      "['Check', 'your', 'account', 'status', ',', 'create', 'a', 'vacation', 'hold', ',', 'update', 'your', 'address', ',', 'renew', 'your', 'subscription', ',', 'report', 'a', 'missed', 'delivery', 'and', 'find', 'support', 'for', 'other', 'customer', 'service', 'issues', '.']\n",
      "['Check', 'your', 'account', 'status', ',', 'create', 'a', 'vacation', 'hold', ',', 'update', 'your', 'address', ',', 'renew', 'your', 'subscription', ',', 'report', 'a', 'missed', 'delivery', 'and', 'find', 'support', 'for', 'other', 'customer', 'service', 'issues', '.']\n",
      "['Chief', 'information', 'officers', 'can', 'learn', 'many', 'lessons', 'from', 'the', 'coronavirus', 'pandemic', 'when', 'it', 'comes', 'to', 'managing', 'data', ',', 'according', 'to', 'DJ', 'Patil', ',', 'a', 'former', 'U.S.']\n",
      "['Chief', 'information', 'officers', 'can', 'learn', 'many', 'lessons', 'from', 'the', 'coronavirus', 'pandemic', 'when', 'it', 'comes', 'to', 'managing', 'data', ',', 'according', 'to', 'DJ', 'Patil', ',', 'a', 'former', 'U.S', '.']\n",
      "['chief', 'data', ',', 'including', 'the', 'need', 'for', 'preparing', 'for', 'constant', 'change', 'and', 'broadening', 'who', 'counts', 'as', 'data', 'stakeholders', '.']\n",
      "['chief', 'data', ',', 'including', 'the', 'need', 'for', 'preparing', 'for', 'constant', 'change', 'and', 'broadening', 'who', 'counts', 'as', 'data', 'stakeholders', '.']\n",
      "['Our', 'columnists', 'from', 'the', 'award-winning', 'opinion', 'pages', 'at', 'The', 'Wall', 'Street', 'Journal', 'examine', 'world', 'news', 'and', 'foreign', 'affairs', 'twice', 'a', 'week', '.']\n",
      "['Our', 'columnists', 'from', 'the', 'award-winning', 'opinion', 'pages', 'at', 'The', 'Wall', 'Street', 'Journal', 'examine', 'world', 'news', 'and', 'foreign', 'affairs', 'twice', 'a', 'week', '.']\n",
      "['Get', 'the', 'information', 'you', 'need', 'to', 'understand', 'the', 'changing', 'world', ',', 'and', 'its', 'impact', 'on', 'U.S.']\n",
      "['Get', 'the', 'information', 'you', 'need', 'to', 'understand', 'the', 'changing', 'world', ',', 'and', 'its', 'impact', 'on', 'U.S', '.']\n",
      "['and', 'national', 'interests', '.']\n",
      "['and', 'national', 'interests', '.']\n",
      "['Published', 'several', 'times', 'weekly', '.']\n",
      "['Published', 'several', 'times', 'weekly', '.']\n",
      "['The', 'Big', 'Four', 'accounting', 'concern', 'reviewed', 'the', 'books', 'of', 'Wirecard', ',', 'Luckin', 'Coffee', 'and', 'other', 'companies', 'where', 'investors', 'lost', 'billions', 'when', 'scandals', 'emerged', '.']\n",
      "['The', 'Big', 'Four', 'accounting', 'concern', 'reviewed', 'the', 'books', 'of', 'Wirecard', ',', 'Luckin', 'Coffee', 'and', 'other', 'companies', 'where', 'investors', 'lost', 'billions', 'when', 'scandals', 'emerged', '.']\n",
      "['The', 'firm', ',', 'which', 'caters', 'to', 'fast-growing', 'tech', 'startups', ',', 'says', 'it', 'unearthed', 'some', 'of', 'the', 'problems', '.']\n",
      "['The', 'firm', ',', 'which', 'caters', 'to', 'fast-growing', 'tech', 'startups', ',', 'says', 'it', 'unearthed', 'some', 'of', 'the', 'problems', '.']\n",
      "['The', 'inaugural', 'Asia', 'Society', 'Triennial', 'features', 'artists', 'and', 'performers', 'who', 'have', 'often', 'been', 'overlooked', 'in', 'the', 'U.S.']\n",
      "['The', 'inaugural', 'Asia', 'Society', 'Triennial', 'features', 'artists', 'and', 'performers', 'who', 'have', 'often', 'been', 'overlooked', 'in', 'the', 'U.S', '.']\n",
      "['New', 'Jersey', 'Gov', '.']\n",
      "['New', 'Jersey', 'Gov', '.']\n",
      "['Phil', 'Murphy', 'named', 'new', 'top', 'officials', 'at', 'the', 'state', '’', 's', 'veterans', 'affairs', 'agency', ',', 'less', 'than', 'two', 'weeks', 'after', 'a', 'Wall', 'Street', 'Journal', 'investigation', 'showed', 'deadly', 'lapses', 'at', 'one', 'of', 'its', 'facilities', '.']\n",
      "['Phil', 'Murphy', 'named', 'new', 'top', 'officials', 'at', 'the', 'state', '’', 's', 'veterans', 'affairs', 'agency', ',', 'less', 'than', 'two', 'weeks', 'after', 'a', 'Wall', 'Street', 'Journal', 'investigation', 'showed', 'deadly', 'lapses', 'at', 'one', 'of', 'its', 'facilities', '.']\n",
      "['How', 'historian', 'Fred', 'Siegel', 'came', 'to', 'appreciate', 'the', 'president', '’', 's', 'defense', 'of', '‘bourgeois', 'values', '’', 'against', 'the', '‘clerisy', '.’']\n",
      "['How', 'historian', 'Fred', 'Siegel', 'came', 'to', 'appreciate', 'the', 'president', '’', 's', 'defense', 'of', '‘', 'bourgeois', 'values', '’', 'against', 'the', '‘', 'clerisy', '.', '’']\n",
      "['Among', 'them', ':', 'What', 'happens', 'to', 'my', 'art', 'collection', 'after', 'I', 'die', '?']\n",
      "['Among', 'them', ':', 'What', 'happens', 'to', 'my', 'art', 'collection', 'after', 'I', 'die', '?']\n",
      "['The', 'social-media', 'giant', 'has', 'made', 'a', 'flurry', 'of', 'new', 'rules', 'designed', 'to', 'improve', 'the', 'discourse', 'on', 'its', 'platforms', '.']\n",
      "['The', 'social-media', 'giant', 'has', 'made', 'a', 'flurry', 'of', 'new', 'rules', 'designed', 'to', 'improve', 'the', 'discourse', 'on', 'its', 'platforms', '.']\n",
      "['When', 'users', 'report', 'content', 'that', 'breaks', 'those', 'rules', ',', 'a', 'test', 'by', 'The', 'Wall', 'Street', 'Journal', 'found', ',', 'the', 'company', 'often', 'fails', 'to', 'enforce', 'them', '.']\n",
      "['When', 'users', 'report', 'content', 'that', 'breaks', 'those', 'rules', ',', 'a', 'test', 'by', 'The', 'Wall', 'Street', 'Journal', 'found', ',', 'the', 'company', 'often', 'fails', 'to', 'enforce', 'them', '.']\n",
      "['The', 'prospect', 'of', 'cuts', 'and', 'burns', ',', 'or', 'just', 'baking', 'a', 'cake', 'that', 'fails', 'to', 'rise', ',', 'doesn’t', 'have', 'to', 'keep', 'us', 'out', 'of', 'the', 'kitchen', '.']\n",
      "['The', 'prospect', 'of', 'cuts', 'and', 'burns', ',', 'or', 'just', 'baking', 'a', 'cake', 'that', 'fails', 'to', 'rise', ',', 'doesn', '’', 't', 'have', 'to', 'keep', 'us', 'out', 'of', 'the', 'kitchen', '.']\n",
      "['The', 'drug', 'giant', 'laid', 'out', 'a', 'timetable', 'for', 'reaching', 'key', 'milestones', 'in', 'the', 'development', 'of', 'its', 'Covid-19', 'vaccine', 'that', 'could', 'mean', 'the', 'shots', 'start', 'becoming', 'available', 'in', 'the', 'U.S.']\n",
      "['The', 'drug', 'giant', 'laid', 'out', 'a', 'timetable', 'for', 'reaching', 'key', 'milestones', 'in', 'the', 'development', 'of', 'its', 'Covid-19', 'vaccine', 'that', 'could', 'mean', 'the', 'shots', 'start', 'becoming', 'available', 'in', 'the', 'U.S', '.']\n",
      "['before', 'year', '’', 's', 'end', '.']\n",
      "['before', 'year', '’', 's', 'end', '.']\n",
      "['Read', 'Reading', '& Retreating', 'on', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Read', 'Reading', '&', 'Retreating', 'on', 'The', 'Wall', 'Street', 'Journal']\n",
      "['The', 'sailors', ',', 'who', 'were', 'evacuated', ',', 'were', 'serving', 'aboard', 'the', 'aircraft', 'carrier', 'that', 'earlier', 'this', 'year', 'was', 'the', 'site', 'of', 'the', 'largest', 'novel', 'coronavirus', 'outbreak', 'to', 'strike', 'the', 'U.S.']\n",
      "['The', 'sailors', ',', 'who', 'were', 'evacuated', ',', 'were', 'serving', 'aboard', 'the', 'aircraft', 'carrier', 'that', 'earlier', 'this', 'year', 'was', 'the', 'site', 'of', 'the', 'largest', 'novel', 'coronavirus', 'outbreak', 'to', 'strike', 'the', 'U.S', '.']\n",
      "['military', '.']\n",
      "['military', '.']\n",
      "['The', 'findings', 'add', 'to', 'debate', 'over', 'the', 'utility', 'of', 'the', 'medicine', 'developed', 'by', 'Gilead', 'Sciences', 'in', 'treating', 'the', 'new', 'coronavirus', '.']\n",
      "['The', 'findings', 'add', 'to', 'debate', 'over', 'the', 'utility', 'of', 'the', 'medicine', 'developed', 'by', 'Gilead', 'Sciences', 'in', 'treating', 'the', 'new', 'coronavirus', '.']\n",
      "['As', 'we', 'settle', 'into', 'working', 'and', 'learning', 'from', 'home', ',', 'repurposed', 'backyard', 'garden', 'sheds', 'and', 'prefab', 'outbuildings', 'offer', 'a', 'pleasant', 'soft', 'divide', 'between', 'family', 'and', 'work', 'life', '.']\n",
      "['As', 'we', 'settle', 'into', 'working', 'and', 'learning', 'from', 'home', ',', 'repurposed', 'backyard', 'garden', 'sheds', 'and', 'prefab', 'outbuildings', 'offer', 'a', 'pleasant', 'soft', 'divide', 'between', 'family', 'and', 'work', 'life', '.']\n",
      "['Request', 'comes', 'during', 'a', 'historic', 'fire', 'season', 'with', 'an', 'unprecedented', 'number', 'of', 'blazes', 'fueled', 'by', 'extreme', 'heat', ',', 'gusty', 'winds', 'and', 'dry', 'conditions', '.']\n",
      "['Request', 'comes', 'during', 'a', 'historic', 'fire', 'season', 'with', 'an', 'unprecedented', 'number', 'of', 'blazes', 'fueled', 'by', 'extreme', 'heat', ',', 'gusty', 'winds', 'and', 'dry', 'conditions', '.']\n",
      "['The', 'red', 'rocks', 'of', 'Moab', ',', 'Utah', 'were', 'woven', 'into', 'the', 'interior', 'and', 'exterior', 'design', 'of', 'this', 'modern', 'house', '.']\n",
      "['The', 'red', 'rocks', 'of', 'Moab', ',', 'Utah', 'were', 'woven', 'into', 'the', 'interior', 'and', 'exterior', 'design', 'of', 'this', 'modern', 'house', '.']\n",
      "['The', 'owners', 'are', 'selling', 'the', '6½', 'acre', 'property', 'for', '$3.85', 'million', '.']\n",
      "['The', 'owners', 'are', 'selling', 'the', '6½', 'acre', 'property', 'for', '$', '3.85', 'million', '.']\n",
      "['A', 'recent', 'WSJ/NBC', 'News', 'poll', 'shows', 'women', 'voters', 'favor', 'Democratic', 'nominee', 'Joe', 'Biden', 'over', 'President', 'Trump', 'by', 'a', 'large', 'margin', '.']\n",
      "['A', 'recent', 'WSJ/NBC', 'News', 'poll', 'shows', 'women', 'voters', 'favor', 'Democratic', 'nominee', 'Joe', 'Biden', 'over', 'President', 'Trump', 'by', 'a', 'large', 'margin', '.']\n",
      "['WSJ', '’', 's', 'Gerald', 'F', '.']\n",
      "['WSJ', '’', 's', 'Gerald', 'F', '.']\n",
      "['Seib', 'explains', 'why', \"we're\", 'likely', 'headed', 'into', 'an', 'election', 'with', 'a', 'gender', 'gap', 'of', 'historic', 'proportions', '.']\n",
      "['Seib', 'explains', 'why', 'we', \"'re\", 'likely', 'headed', 'into', 'an', 'election', 'with', 'a', 'gender', 'gap', 'of', 'historic', 'proportions', '.']\n",
      "['Photo', ':', 'Lori', 'King/Associated', 'Press']\n",
      "['Photo', ':', 'Lori', 'King/Associated', 'Press']\n",
      "['Commercials', 'are', 'increasingly', 'starring', 'nonprofessionals', ',', 'including', 'family', 'members', 'of', 'the', 'people', 'making', 'them', ',', 'due', 'to', 'pandemic', 'restrictions', '.']\n",
      "['Commercials', 'are', 'increasingly', 'starring', 'nonprofessionals', ',', 'including', 'family', 'members', 'of', 'the', 'people', 'making', 'them', ',', 'due', 'to', 'pandemic', 'restrictions', '.']\n",
      "['‘They’re', 'the', 'most', 'available', ',', 'but', 'definitely', 'not', 'the', 'most', 'cooperative', '.’']\n",
      "['‘', 'They', '’', 're', 'the', 'most', 'available', ',', 'but', 'definitely', 'not', 'the', 'most', 'cooperative', '.', '’']\n",
      "['Russian', 'President', 'Vladimir', 'Putin', 'wanted', 'to', 'extend', 'the', 'New', 'START', 'treaty', ',', 'but', 'the', 'Trump', 'administration', 'called', 'on', 'Moscow', 'to', 'agree', 'first', 'to', 'a', 'warhead', 'freeze', '.']\n",
      "['Russian', 'President', 'Vladimir', 'Putin', 'wanted', 'to', 'extend', 'the', 'New', 'START', 'treaty', ',', 'but', 'the', 'Trump', 'administration', 'called', 'on', 'Moscow', 'to', 'agree', 'first', 'to', 'a', 'warhead', 'freeze', '.']\n",
      "['The', 'White', 'House', 'and', 'Senate', 'Republicans', 'are', 'divided', 'over', 'the', 'size', 'of', 'last-minute', 'coronavirus', 'aid', ',', 'with', 'President', 'Trump', 'signaling', 'he', 'would', 'support', 'a\\xa0package', 'approaching', '$2', 'trillion', ',', 'narrowing', 'the', 'distance', 'with', 'House', 'Democrats', '.']\n",
      "['The', 'White', 'House', 'and', 'Senate', 'Republicans', 'are', 'divided', 'over', 'the', 'size', 'of', 'last-minute', 'coronavirus', 'aid', ',', 'with', 'President', 'Trump', 'signaling', 'he', 'would', 'support', 'a', 'package', 'approaching', '$', '2', 'trillion', ',', 'narrowing', 'the', 'distance', 'with', 'House', 'Democrats', '.']\n",
      "['From', 'multi-day', 'experiences', 'to', 'online', 'intimate', 'invitation-only', 'gatherings', ',', 'these', 'exclusive', 'events', 'are', 'where', 'top', 'executives', 'convene', 'to', 'hear', 'game-changing', 'leaders', 'and', 'senior', 'Journal', 'editors', 'discuss', 'the', 'critical', 'topics', 'making', 'headlines— all', 'through', 'the', 'insightful', ',', 'analytical', 'lens', 'the', 'Journal', 'is', 'known', 'for', '.']\n",
      "['From', 'multi-day', 'experiences', 'to', 'online', 'intimate', 'invitation-only', 'gatherings', ',', 'these', 'exclusive', 'events', 'are', 'where', 'top', 'executives', 'convene', 'to', 'hear', 'game-changing', 'leaders', 'and', 'senior', 'Journal', 'editors', 'discuss', 'the', 'critical', 'topics', 'making', 'headlines—', 'all', 'through', 'the', 'insightful', ',', 'analytical', 'lens', 'the', 'Journal', 'is', 'known', 'for', '.']\n",
      "['Daily', 'new', 'coronavirus', 'infections', 'in', 'the', 'U.S.']\n",
      "['Daily', 'new', 'coronavirus', 'infections', 'in', 'the', 'U.S', '.']\n",
      "['surged', 'to', 'their', 'highest', 'level', 'since', 'late', 'July', ',', 'approaching', 'numbers', 'seen', 'during', 'the', 'outbreak', '’', 's', 'midsummer', 'peak', '.']\n",
      "['surged', 'to', 'their', 'highest', 'level', 'since', 'late', 'July', ',', 'approaching', 'numbers', 'seen', 'during', 'the', 'outbreak', '’', 's', 'midsummer', 'peak', '.']\n",
      "['And', 'why', 'a', 'Portuguese', 'scholar', 'says', 'the', 'reality', 'principle', 'is', 'liberalism', '’', 's', 'last', 'nemesis', '.']\n",
      "['And', 'why', 'a', 'Portuguese', 'scholar', 'says', 'the', 'reality', 'principle', 'is', 'liberalism', '’', 's', 'last', 'nemesis', '.']\n",
      "['Electronic', 'trading', 'giant', 'Citadel', 'Securities', 'sued', 'the', 'Securities', 'and', 'Exchange', 'Commission', 'over', 'the', 'agency', '’', 's', 'decision', 'to', 'approve', 'a', 'new', 'mechanism', 'for', 'trading', 'stocks', 'at', 'upstart', 'exchange', 'operator', 'IEX', 'Group', '.']\n",
      "['Electronic', 'trading', 'giant', 'Citadel', 'Securities', 'sued', 'the', 'Securities', 'and', 'Exchange', 'Commission', 'over', 'the', 'agency', '’', 's', 'decision', 'to', 'approve', 'a', 'new', 'mechanism', 'for', 'trading', 'stocks', 'at', 'upstart', 'exchange', 'operator', 'IEX', 'Group', '.']\n",
      "['Supply-chain', 'software', 'provider', 'E2open', 'LLC', 'plans', 'to', 'go', 'public', 'through', 'an', 'agreement', 'with', 'a', 'blank-check', 'company', ',', 'making', 'it', 'the', 'latest', 'in', 'a', 'stream', 'of', 'companies', 'bypassing', 'the', 'traditional', 'path', 'to', 'a', 'Wall', 'Street', 'listing', '.']\n",
      "['Supply-chain', 'software', 'provider', 'E2open', 'LLC', 'plans', 'to', 'go', 'public', 'through', 'an', 'agreement', 'with', 'a', 'blank-check', 'company', ',', 'making', 'it', 'the', 'latest', 'in', 'a', 'stream', 'of', 'companies', 'bypassing', 'the', 'traditional', 'path', 'to', 'a', 'Wall', 'Street', 'listing', '.']\n",
      "['A', 'surge', 'of', 'federal', 'spending', 'to', 'combat', 'the', 'coronavirus', 'and', 'cushion', 'the', 'U.S.']\n",
      "['A', 'surge', 'of', 'federal', 'spending', 'to', 'combat', 'the', 'coronavirus', 'and', 'cushion', 'the', 'U.S', '.']\n",
      "['economy', ',', 'coupled', 'with', 'a', 'drop-off', 'in', 'federal', 'revenues', 'amid', 'widespread', 'shutdowns', 'and', 'layoffs', ',', 'contributed', 'to', 'the', 'widening', 'deficit', '.']\n",
      "['economy', ',', 'coupled', 'with', 'a', 'drop-off', 'in', 'federal', 'revenues', 'amid', 'widespread', 'shutdowns', 'and', 'layoffs', ',', 'contributed', 'to', 'the', 'widening', 'deficit', '.']\n",
      "['CEOs', 'of', 'Facebook', ',', 'Twitter', 'and', 'YouTube-parent', 'Alphabet', 'Inc', '.']\n",
      "['CEOs', 'of', 'Facebook', ',', 'Twitter', 'and', 'YouTube-parent', 'Alphabet', 'Inc', '.']\n",
      "['will', 'appear', 'before', 'a', 'Senate', 'committee', 'on', 'Oct', '.']\n",
      "['will', 'appear', 'before', 'a', 'Senate', 'committee', 'on', 'Oct', '.']\n",
      "['28', 'to', 'face', 'questioning', 'about', 'their', 'policies', 'for', 'moderating', 'content', 'on', 'their', 'internet', 'platforms', '.']\n",
      "['28', 'to', 'face', 'questioning', 'about', 'their', 'policies', 'for', 'moderating', 'content', 'on', 'their', 'internet', 'platforms', '.']\n",
      "['Even', 'when', 'the', 'world', 'returns', 'to', '“normal,” the', 'legacy', 'of', 'Covid-19', 'will', 'transform', 'everything', 'from', 'wages', 'and', 'health', 'care', 'to', 'political', 'attitudes', 'and', 'global', 'supply', 'chains', '.']\n",
      "['Even', 'when', 'the', 'world', 'returns', 'to', '“', 'normal', ',', '”', 'the', 'legacy', 'of', 'Covid-19', 'will', 'transform', 'everything', 'from', 'wages', 'and', 'health', 'care', 'to', 'political', 'attitudes', 'and', 'global', 'supply', 'chains', '.']\n",
      "['Stocks', 'are', 'booming', 'while', 'companies', 'shed', 'millions', 'of', 'workers', 'from', 'payrolls', '.']\n",
      "['Stocks', 'are', 'booming', 'while', 'companies', 'shed', 'millions', 'of', 'workers', 'from', 'payrolls', '.']\n",
      "['WSJ', 'explains', 'why', 'the', 'stock', 'market', 'seems', 'disconnected', 'from', 'economic', 'reality', 'in', 'the', 'U.S.']\n",
      "['WSJ', 'explains', 'why', 'the', 'stock', 'market', 'seems', 'disconnected', 'from', 'economic', 'reality', 'in', 'the', 'U.S', '.']\n",
      "['Photo', 'Illustration', 'by', 'Carlos', 'Waters/WSJ']\n",
      "['Photo', 'Illustration', 'by', 'Carlos', 'Waters/WSJ']\n",
      "['Organic', 'revenue', 'fell', '5.6% in', 'the', 'third', 'quarter', ',', 'less', 'than', 'the', '13% drop', 'in', 'the', 'second', 'quarter', '.']\n",
      "['Organic', 'revenue', 'fell', '5.6', '%', 'in', 'the', 'third', 'quarter', ',', 'less', 'than', 'the', '13', '%', 'drop', 'in', 'the', 'second', 'quarter', '.']\n",
      "['Covid-19', 'could', 'make', 'the', 'fourth', 'quarter', 'very', 'difficult', ',', 'CEO', 'Arthur', 'Sadoun', 'says', '.']\n",
      "['Covid-19', 'could', 'make', 'the', 'fourth', 'quarter', 'very', 'difficult', ',', 'CEO', 'Arthur', 'Sadoun', 'says', '.']\n",
      "['Settlements', 'between', 'Brazil', '’', 's', 'J&F', 'Investimentos', 'and', 'U.S.']\n",
      "['Settlements', 'between', 'Brazil', '’', 's', 'J', '&', 'F', 'Investimentos', 'and', 'U.S', '.']\n",
      "['authorities', 'to', 'resolve', 'charges', 'arising', 'from', 'an', 'alleged', 'bribery', 'scheme', 'illustrate', 'the', 'importance', 'of', 'maintaining', 'anticorruption', 'programs', 'and', 'making', 'sure', 'top', 'executives', 'undergo', 'compliance', 'training', '.']\n",
      "['authorities', 'to', 'resolve', 'charges', 'arising', 'from', 'an', 'alleged', 'bribery', 'scheme', 'illustrate', 'the', 'importance', 'of', 'maintaining', 'anticorruption', 'programs', 'and', 'making', 'sure', 'top', 'executives', 'undergo', 'compliance', 'training', '.']\n",
      "['As', 'U.S.']\n",
      "['As', 'U.S', '.']\n",
      "['jobless', 'claims', 'swell', 'during', 'the', 'pandemic', ',', 'authorities', 'are', 'urging', 'financial', 'institutions', 'to', 'watch', 'for', 'schemes', 'involving', 'unemployment', 'insurance', 'payments', '.']\n",
      "['jobless', 'claims', 'swell', 'during', 'the', 'pandemic', ',', 'authorities', 'are', 'urging', 'financial', 'institutions', 'to', 'watch', 'for', 'schemes', 'involving', 'unemployment', 'insurance', 'payments', '.']\n",
      "['Finance', 'executives', 'are', 'weighing', 'the', 'potential', 'impact', 'of', 'higher', 'taxes', 'on', 'their', 'companies', '’', 'cash', 'flows', ',', 'debt', 'and', 'other', 'core', 'items', 'on', 'the', 'balance', 'sheet', 'ahead', 'of', 'the', 'Nov', '.']\n",
      "['Finance', 'executives', 'are', 'weighing', 'the', 'potential', 'impact', 'of', 'higher', 'taxes', 'on', 'their', 'companies', '’', 'cash', 'flows', ',', 'debt', 'and', 'other', 'core', 'items', 'on', 'the', 'balance', 'sheet', 'ahead', 'of', 'the', 'Nov', '.']\n",
      "['3', 'U.S.']\n",
      "['3', 'U.S', '.']\n",
      "['elections', '.']\n",
      "['elections', '.']\n",
      "['Making', 'more', 'money', 'can', 'mean', 'much', 'higher', 'Medicare', 'premiums', 'for', 'some', 'people', '.']\n",
      "['Making', 'more', 'money', 'can', 'mean', 'much', 'higher', 'Medicare', 'premiums', 'for', 'some', 'people', '.']\n",
      "['A', 'comprehensive', 'list', 'of', 'companies', 'available', 'on', 'stock', 'exchanges', 'that', 'can', 'be', 'browsed', 'alphabetically', ',', 'by', 'sector', ',', 'or', 'by', 'country', '.']\n",
      "['A', 'comprehensive', 'list', 'of', 'companies', 'available', 'on', 'stock', 'exchanges', 'that', 'can', 'be', 'browsed', 'alphabetically', ',', 'by', 'sector', ',', 'or', 'by', 'country', '.']\n",
      "['Gen', '.']\n",
      "['Gen', '.']\n",
      "['Salvador', 'Cienfuegos', 'received', 'bribes', 'and', 'passed', 'information', 'on', 'investigations', 'to', 'crime', 'bosses', ',', 'U.S.']\n",
      "['Salvador', 'Cienfuegos', 'received', 'bribes', 'and', 'passed', 'information', 'on', 'investigations', 'to', 'crime', 'bosses', ',', 'U.S', '.']\n",
      "['prosecutors', 'allege', '.']\n",
      "['prosecutors', 'allege', '.']\n",
      "['The', 'prioritizing', 'put', 'underserved', 'businesses', ',', 'including', 'those', 'owned', 'by', 'women', 'and', 'minorities', ',', 'at', 'a', 'disadvantage', 'when', 'applying', 'for', 'a', '$670', 'billion', 'small-business', 'coronavirus', 'aid', 'program', ',', 'a', 'Democratic-led', 'congressional', 'oversight', 'subcommittee', '’', 's', 'report', 'says', '.']\n",
      "['The', 'prioritizing', 'put', 'underserved', 'businesses', ',', 'including', 'those', 'owned', 'by', 'women', 'and', 'minorities', ',', 'at', 'a', 'disadvantage', 'when', 'applying', 'for', 'a', '$', '670', 'billion', 'small-business', 'coronavirus', 'aid', 'program', ',', 'a', 'Democratic-led', 'congressional', 'oversight', 'subcommittee', '’', 's', 'report', 'says', '.']\n",
      "['Stetson', 'Bennett', 'IV', 'is', 'the', 'unlikely', 'starter', 'at', 'quarterback', 'for', 'No', '.']\n",
      "['Stetson', 'Bennett', 'IV', 'is', 'the', 'unlikely', 'starter', 'at', 'quarterback', 'for', 'No', '.']\n",
      "['3', 'Georgia', 'against', 'No', '.']\n",
      "['3', 'Georgia', 'against', 'No', '.']\n",
      "['2', 'Alabama', '.']\n",
      "['2', 'Alabama', '.']\n",
      "['A', '104-seat', 'Upper', 'Chamber', 'is', 'on', 'the', 'agenda', 'if', 'Democrats', 'sweep', 'the', 'election', '.']\n",
      "['A', '104-seat', 'Upper', 'Chamber', 'is', 'on', 'the', 'agenda', 'if', 'Democrats', 'sweep', 'the', 'election', '.']\n",
      "['Some', 'of', 'the', 'strategies', 'stem', 'from', 'the', 'economic-relief', 'legislation', 'passed', 'earlier', 'in', 'the', 'year', '.']\n",
      "['Some', 'of', 'the', 'strategies', 'stem', 'from', 'the', 'economic-relief', 'legislation', 'passed', 'earlier', 'in', 'the', 'year', '.']\n",
      "['Corrections', '& Amplifications', 'for', 'the', 'edition', 'of', 'Oct', '.']\n",
      "['Corrections', '&', 'Amplifications', 'for', 'the', 'edition', 'of', 'Oct', '.']\n",
      "['17-18', ',', '2020', '.']\n",
      "['17-18', ',', '2020', '.']\n",
      "['Tactics', 'Google', 'and', 'other', 'large', 'online-ad', 'players', 'use', 'in', 'digital', 'ad', 'auctions', 'violate', 'EU', 'privacy', 'law', ',', 'investigators', 'for', 'Belgium', '’', 's', 'privacy', 'regulator', 'wrote', 'in', 'an', 'internal', 'report', ',', 'a', 'preliminary', 'finding', 'with', 'implications', 'across', 'the', 'continent', '.']\n",
      "['Tactics', 'Google', 'and', 'other', 'large', 'online-ad', 'players', 'use', 'in', 'digital', 'ad', 'auctions', 'violate', 'EU', 'privacy', 'law', ',', 'investigators', 'for', 'Belgium', '’', 's', 'privacy', 'regulator', 'wrote', 'in', 'an', 'internal', 'report', ',', 'a', 'preliminary', 'finding', 'with', 'implications', 'across', 'the', 'continent', '.']\n",
      "['European', 'privacy', 'regulators', 'are', 'unlikely', 'to', 'issue', 'a', 'final', 'ruling', 'on', 'Twitter', '’', 's', 'handling', 'of', 'a', '2019', 'data', 'breach', 'before', 'the', 'end', 'of', 'the', 'year', ',', 'Ireland', '’', 's', 'data', 'commissioner', 'said', '.']\n",
      "['European', 'privacy', 'regulators', 'are', 'unlikely', 'to', 'issue', 'a', 'final', 'ruling', 'on', 'Twitter', '’', 's', 'handling', 'of', 'a', '2019', 'data', 'breach', 'before', 'the', 'end', 'of', 'the', 'year', ',', 'Ireland', '’', 's', 'data', 'commissioner', 'said', '.']\n",
      "['The', 'new', 'offering', 'will', 'combine', 'ServiceNow', '’', 's', 'IT', 'Service', 'Management', 'and', 'IT', 'Operations', 'Management', 'systems\\xa0with', 'IBM', '’', 's', 'recently', 'introduced', 'Watson', 'AIOps', '.']\n",
      "['The', 'new', 'offering', 'will', 'combine', 'ServiceNow', '’', 's', 'IT', 'Service', 'Management', 'and', 'IT', 'Operations', 'Management', 'systems', 'with', 'IBM', '’', 's', 'recently', 'introduced', 'Watson', 'AIOps', '.']\n",
      "['From', 'unrest', 'in', 'Belarus', 'and', 'Kyrgyzstan', 'to', 'the', 'Armenia-Azerbaijan', 'conflict', ',', 'WSJ', 'explores', 'how', 'the', 'crises', 'unfolding', 'in', 'Russia', '’', 's', 'backyard', 'mark', 'a', 'turning', 'point', 'in', 'Vladimir', 'Putin', '’', 's', 'rule', 'and', 'put', 'him', 'at', 'risk', 'of', 'losing', 'influence', 'in', 'the', 'former', 'Soviet', 'Union', '.']\n",
      "['From', 'unrest', 'in', 'Belarus', 'and', 'Kyrgyzstan', 'to', 'the', 'Armenia-Azerbaijan', 'conflict', ',', 'WSJ', 'explores', 'how', 'the', 'crises', 'unfolding', 'in', 'Russia', '’', 's', 'backyard', 'mark', 'a', 'turning', 'point', 'in', 'Vladimir', 'Putin', '’', 's', 'rule', 'and', 'put', 'him', 'at', 'risk', 'of', 'losing', 'influence', 'in', 'the', 'former', 'Soviet', 'Union', '.']\n",
      "['Video/Photo', 'Composite', ':', 'Michelle', 'Inez', 'Simon']\n",
      "['Video/Photo', 'Composite', ':', 'Michelle', 'Inez', 'Simon']\n",
      "['Facebook', '’', 's', 'CEO', 'long', 'left', 'politics', 'and', 'policy', 'to', 'deputies', '.']\n",
      "['Facebook', '’', 's', 'CEO', 'long', 'left', 'politics', 'and', 'policy', 'to', 'deputies', '.']\n",
      "['No', 'more', '.']\n",
      "['No', 'more', '.']\n",
      "['As', 'the', 'company', '’', 's', 'influence', 'has', 'grown', ',', 'and', 'with', 'it', 'controversies', ',', 'political', 'acumen', 'has', 'become', 'an', 'essential', 'tool', '.']\n",
      "['As', 'the', 'company', '’', 's', 'influence', 'has', 'grown', ',', 'and', 'with', 'it', 'controversies', ',', 'political', 'acumen', 'has', 'become', 'an', 'essential', 'tool', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'offers', 'discounted', 'membership', 'options', 'for', 'students', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'offers', 'discounted', 'membership', 'options', 'for', 'students', '.']\n",
      "['Explore', 'the', 'tools', 'you', 'need', 'to', 'turn', 'what', 'you', 'love', 'into', 'a', 'career', '.']\n",
      "['Explore', 'the', 'tools', 'you', 'need', 'to', 'turn', 'what', 'you', 'love', 'into', 'a', 'career', '.']\n",
      "['When', 'the', 'New', 'York', 'Post', 'published', 'articles', 'based', 'on', 'email', 'exchanges', 'with', 'Hunter', 'Biden', ',', 'social-media', 'companies', 'saw', 'the', 'situation', 'as', 'one', 'they', 'spent', 'years', 'preparing', 'for', '.']\n",
      "['When', 'the', 'New', 'York', 'Post', 'published', 'articles', 'based', 'on', 'email', 'exchanges', 'with', 'Hunter', 'Biden', ',', 'social-media', 'companies', 'saw', 'the', 'situation', 'as', 'one', 'they', 'spent', 'years', 'preparing', 'for', '.']\n",
      "['Their', 'actions', 'drew', 'a', 'mixture', 'of', 'support', 'and', 'criticism', '.']\n",
      "['Their', 'actions', 'drew', 'a', 'mixture', 'of', 'support', 'and', 'criticism', '.']\n",
      "['Technology', 'companies', 'are', 'set', 'to', 'end', 'the', 'year', 'with', 'their', 'greatest', 'share', 'of', 'the', 'stock', 'market', 'ever', ',', 'topping', 'a', 'dot-com', 'era', 'peak', 'in', 'the', 'latest', 'illustration', 'of', 'their', 'growing', 'influence', 'on', 'global', 'consumers', '.']\n",
      "['Technology', 'companies', 'are', 'set', 'to', 'end', 'the', 'year', 'with', 'their', 'greatest', 'share', 'of', 'the', 'stock', 'market', 'ever', ',', 'topping', 'a', 'dot-com', 'era', 'peak', 'in', 'the', 'latest', 'illustration', 'of', 'their', 'growing', 'influence', 'on', 'global', 'consumers', '.']\n",
      "['American', 'shoppers', 'boosted', 'their', 'spending', 'on', 'vehicles', ',', 'clothing', 'and', 'many', 'other', 'goods', ',', 'a', 'bright', 'spot', 'amid', 'signs', 'the', 'economic', 'recovery', 'remains', 'fragile', '.']\n",
      "['American', 'shoppers', 'boosted', 'their', 'spending', 'on', 'vehicles', ',', 'clothing', 'and', 'many', 'other', 'goods', ',', 'a', 'bright', 'spot', 'amid', 'signs', 'the', 'economic', 'recovery', 'remains', 'fragile', '.']\n",
      "['The', 'economic', 'outlook', 'has', 'worsened', 'since', 'the', 'credit-rating', 'firm', 'downgraded', 'the', 'country', '’', 's', 'sovereign', 'debt', 'in', 'September', '2017', '.']\n",
      "['The', 'economic', 'outlook', 'has', 'worsened', 'since', 'the', 'credit-rating', 'firm', 'downgraded', 'the', 'country', '’', 's', 'sovereign', 'debt', 'in', 'September', '2017', '.']\n"
     ]
    }
   ],
   "source": [
    "articles, df, dt = get_articles(['wsj.com'])\n",
    "\n",
    "for a in articles:\n",
    "    sentence = _sentence_tokenize(a[\"description\"])\n",
    "    for s in sentence:\n",
    "        print(_word_tokenize(s))\n",
    "        print(_word_tokenize_NLTK(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "healthy-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk standardization\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return _sentence_tokenize(text)\n",
    "\n",
    "# Expects a list from sentence tokenizing\n",
    "def word_list_tokenize(sentenceList):\n",
    "    tokenized = []\n",
    "    for bulk in sentenceList:\n",
    "        for w in _word_tokenize(bulk):\n",
    "            tokenized.append(w)\n",
    "    return tokenized\n",
    "\n",
    "def lemmatization(stemmedList):\n",
    "    lemmaList = []\n",
    "    for w in stemmedList:\n",
    "        lemmaList.append(_lemmatize(w))\n",
    "    return lemmaList\n",
    "\n",
    "def remove_stopwords(wordList):\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in _stopwords():  \n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def remove_punctuations(wordList):\n",
    "    punctuations = \".?:!,;‘-’|\"\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in punctuations:\n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def list_to_lower(_list):\n",
    "    returnList = []\n",
    "    for w in _list:\n",
    "        returnList.append(w.lower())\n",
    "    return returnList\n",
    "        \n",
    "\n",
    "def standardize(text):\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    lemma = lemmatization(wordTokenized)\n",
    "    noStopwords = remove_stopwords(lemma)\n",
    "    noPunctuations = remove_punctuations(noStopwords)\n",
    "    return noPunctuations\n",
    "\n",
    "\"\"\" Standardize the model\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "same list as in the input with additional attributes:\n",
    "    Example: {\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "\"\"\"\n",
    "def standardizeList(articles):\n",
    "    for article in articles:\n",
    "        article[\"title_standardized\"] = standardize(article[\"title\"])\n",
    "        article[\"description_standardized\"] = standardize(article[\"description\"])\n",
    "    return articles\n",
    "\n",
    "articles, dateFrom, dateTo = get_articles(['wsj.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "exterior-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Macron', 'Push', 'Recovery Fund Germany']\n"
     ]
    }
   ],
   "source": [
    "# Entity extraction\n",
    "from itertools import islice\n",
    "\n",
    "def entity_extraction(wordList):\n",
    "    extracted = []\n",
    "#    sentTokenized = sentence_tokenize(text)\n",
    "#    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    \n",
    "    # Tagging each word\n",
    "    tagged = _speech_tag(wordList)\n",
    "    chunk = _named_entities_chunk(tagged)\n",
    "    for c in chunk:\n",
    "        if type(c) == Tree:\n",
    "            newItems = \" \".join([token for token, pos in c.leaves()])\n",
    "            if newItems not in extracted:\n",
    "                extracted.append(newItems)\n",
    "    return extracted\n",
    "\n",
    "def count_elements(elements):\n",
    "    _dict = {}\n",
    "    if(len(elements) > 0):\n",
    "        for words in elements:\n",
    "            for w in words:\n",
    "                try:\n",
    "                    _dict[w] += 1\n",
    "                except:\n",
    "                    _dict[w] = 1    \n",
    "    return _dict\n",
    "\n",
    "def sort_dictionary_desc(_dict):\n",
    "    return sorted(_dict.items(), key=lambda x: x[1], reverse=True)   \n",
    "\n",
    "def filter_named_properties(elements):\n",
    "    filtered = []\n",
    "    _sorted = sort_dictionary_desc(elements)\n",
    "    i = 0\n",
    "    for w in _sorted:\n",
    "        filtered.append(w)\n",
    "        i += 1\n",
    "        if(i >= _namedEntities):\n",
    "            return filtered\n",
    "\n",
    "    return filtered\n",
    "    \n",
    "def get_named_properties(positive):\n",
    "    entities = []\n",
    "    for article in positive:\n",
    "        for section in sections_to_analyze():\n",
    "            entities.append(entity_extraction(article[section]))\n",
    "    return count_elements(entities)\n",
    "\n",
    "def get_most_popular_named_properties(positive):\n",
    "    namedProperties = get_named_properties(positive)\n",
    "    return filter_named_properties(namedProperties)\n",
    "\n",
    "print(entity_extraction(['Macron', 'merkel', 'Push', 'Covid-19', 'Recovery', 'Fund', 'Germany', 'take', 'presidency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "documentary-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set if we want maximum number of samples, used for debugging\n",
    "_samplesThreshold = 0\n",
    "\n",
    "def array_similarity(arr1, arr2):\n",
    "    for a1 in arr1:\n",
    "        for a2 in arr2:\n",
    "            if(a1.lower() == a2.lower()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def create_testing_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i in samples]\n",
    "\n",
    "def create_training_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i not in samples]\n",
    "\n",
    "def create_testing_and_training_data(articles):\n",
    "    # The threshold cannot be higher than the actual data\n",
    "    if(_testThreshold == 0 or len(articles) < _testThreshold):\n",
    "        testThreshold = len(articles)\n",
    "    else:\n",
    "        testThreshold = _testThreshold\n",
    "\n",
    "    train, test = train_test_split(articles, test_size=0.2)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "# Positive (articles that include '_filter' in title or description) will have index = 0\n",
    "# Negative will have index = 1\n",
    "def split_to_positive_and_negative(articles):\n",
    "    positiveResults = []\n",
    "    negativeResults = []\n",
    "    processedIndexes = []\n",
    "    \n",
    "    for a in articles:\n",
    "        for s in sections_to_analyze():\n",
    "            if(array_similarity(a[s], _filter) == True):\n",
    "                positiveResults.append(a)\n",
    "            else:\n",
    "                negativeResults.append(a)\n",
    "\n",
    "        if(_samplesThreshold > 0):\n",
    "            if(len(positiveResults) > _samplesThreshold):\n",
    "                break;\n",
    "\n",
    "    return positiveResults, negativeResults\n",
    "\n",
    "\n",
    "\"\"\" Convert to object for classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "classified: object\n",
    "    Example: {  \"testing\":  {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}}},\n",
    "                \"training\": {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}]}\n",
    "             }\n",
    "\"\"\"\n",
    "def convert_to_classification_object(articles):\n",
    "    testAndTrain = create_testing_and_training_data(articles)\n",
    "    \n",
    "    if(_debug):\n",
    "        print(\"All test:\", len(testAndTrain[0]))\n",
    "        print(\"All training: \", len(testAndTrain[1]))\n",
    "        print(\"==\")\n",
    "\n",
    "    test = split_to_positive_and_negative(testAndTrain[0])\n",
    "    train = split_to_positive_and_negative(testAndTrain[1])\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"Test, positive: \", len(test[0]))\n",
    "        print(\"Test, negative: \",len(test[1]))\n",
    "        print(\"Train, positive: \", len(train[0]))\n",
    "        print(\"Train, negative: \", len(train[1]))\n",
    "\n",
    "    testing = {\"positive\":{}, \"negative\":{}}\n",
    "    testing[\"positive\"] = test[0]\n",
    "    testing[\"negative\"] = test[1]\n",
    "\n",
    "    training = {\"positive\":{}, \"negative\":{}}\n",
    "    training[\"positive\"] = train[0]\n",
    "    training[\"negative\"] = train[1]\n",
    "\n",
    "    classified = {\"testing\": {}, \"training\": {}}\n",
    "    classified[\"testing\"] = testing\n",
    "    classified[\"training\"] = training\n",
    "    \n",
    "    return classified\n",
    "\n",
    "def count_frequencies(data, _class, freq):\n",
    "    for word in data:\n",
    "        try:\n",
    "            freq[_class][word] += 1\n",
    "        except:\n",
    "            freq[_class][word] = 1\n",
    "    return freq\n",
    "\n",
    "def count_total_frequencies(frequencies):\n",
    "    count = 0\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            count += frequencies[_class][w]\n",
    "    return count\n",
    "\n",
    "# Input: Frequency object (described below)\n",
    "# Output: {'positive': {'word':'likelihood', 'word2':'likelihood'}, 'negative': {...} }\n",
    "def calculate_likelihood(frequencies):\n",
    "    p_w = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            p_w[_class][w] = float(frequencies[_class][w]) / float(len(frequencies[_class]))\n",
    "            #p_w[_class][w] = float(frequencies[_class][w]) / count_total_frequencies(frequencies)\n",
    "    return p_w\n",
    "\n",
    "\"\"\" Calculate frequencies from the classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "Returns\n",
    "----------------\n",
    "frequencies: object\n",
    "    Example: {  \"positive\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "                \"negative\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "             }\n",
    "\"\"\"\n",
    "def word_counter(trainingData):\n",
    "    frequencies = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in trainingData:\n",
    "        for article in trainingData[_class]:\n",
    "            for section in sections_to_analyze():\n",
    "                count_frequencies(article[section], _class, frequencies)\n",
    "    return frequencies\n",
    "\n",
    "# Gets the prior probability of P(type1)\n",
    "def calculate_prior_propabilities(frequencies, type1, type2):\n",
    "    return float(len(frequencies[type1])) / float(len(frequencies[type1]) + len(frequencies[type2]))\n",
    "\n",
    "def add_to_predictiveValues(predictiveValues, _type):\n",
    "    try:\n",
    "        predictiveValues[_type] += 1\n",
    "    except:\n",
    "        predictiveValues[_type] = 1\n",
    "\n",
    "def find_predictive_parameter(testClass, articleWeight):\n",
    "    pWeight = articleWeight[\"positive\"]\n",
    "    nWeight = articleWeight[\"negative\"]\n",
    "\n",
    "    if(pWeight >= nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"TP\"\n",
    "        else:\n",
    "            return \"FP\"\n",
    "\n",
    "    if(pWeight < nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"FN\"\n",
    "        if(testClass == \"negative\"):\n",
    "            return \"TN\"\n",
    "\n",
    "def calculate_weigh_by_article(article, trainingDataLikelihood):\n",
    "    articleWeight = {}\n",
    "    for _trainingClass in trainingDataLikelihood:\n",
    "        for section in sections_to_analyze():\n",
    "            weight = 1\n",
    "            for word in article[section]:\n",
    "                try:\n",
    "                    weight *= trainingDataLikelihood[_trainingClass][word]\n",
    "                except:\n",
    "                    weight *= 0.00001\n",
    "        articleWeight[_trainingClass] = weight\n",
    "\n",
    "    return articleWeight #find_predictive_parameter(_class, combinedWeight)\n",
    "\n",
    "def calculate_accuracy(TP, FP, FN, TN):\n",
    "    numerator = TP + TN\n",
    "    denominator = TP + TN + FP + FN\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def calculate_positive_vs_negative(articlesWeight):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    for article in articlesWeight:\n",
    "        if(article['positive'] >= article['negative']):\n",
    "            countPositive += 1\n",
    "        else:\n",
    "            countNegative += 1\n",
    "    \n",
    "    return countPositive, countNegative\n",
    "\n",
    "def display_classification_results(accuracy, positive, negative, entities, displayInfo):\n",
    "    if(displayInfo is None or displayInfo == \"\"):\n",
    "        dateFrom = \"\"\n",
    "        dateTo = \"\"\n",
    "        domains = \"\"\n",
    "    else:\n",
    "        dateFrom = displayInfo[\"dateFrom\"]\n",
    "        dateTo = displayInfo[\"dateTo\"]\n",
    "        domains = displayInfo[\"domains\"]\n",
    "\n",
    "    displayAccuracy = \"{:.2f}\".format(accuracy * 100)\n",
    "    displayPositive = \"{:.2f}\".format(float(positive)/(float(negative)+float(positive)) * 100)\n",
    "\n",
    "    print(\"========\")\n",
    "    print(f\"Finished processing articles:\")\n",
    "    print(f\"DateFrom: {dateFrom}\")\n",
    "    print(f\"DateTo: {dateTo}\")\n",
    "    print(f\"Domains: {domains}\")\n",
    "    print(f\"Accuracy: {displayAccuracy} %\")\n",
    "    print(f\"Portion of positive: {displayPositive} % \")\n",
    "    print(f\"{_namedEntities} most popular named entities: {entities}\")\n",
    "    print(\"========\")\n",
    "\n",
    "'''\n",
    "testingData:{'positive': [  'id': '..', 'domain': '..', 'title': '..', 'title_standardized': '..',\n",
    "                            'description': '...', 'description_standardized': '...']}\n",
    "trainingDataLikelihood: {'positive': {'word': likelihood}, {'word2', likelihood}...,\n",
    "                         'negative': {'word': likelihood}, {'word2', likelihood}...}\n",
    "pPropability: {'positive': float_value, 'negative': float_value}\n",
    "'''\n",
    "def classification_result(testingData, trainingDataLikelihood, pPropability):\n",
    "    predictiveValues = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "    articlesWeight = []\n",
    "    \n",
    "    for _class in testingData:\n",
    "        for article in testingData[_class]:\n",
    "            articleWeight = calculate_weigh_by_article(article, trainingDataLikelihood)\n",
    "\n",
    "            predictiveParameter = find_predictive_parameter(_class, articleWeight)\n",
    "            add_to_predictiveValues(predictiveValues, predictiveParameter)\n",
    "            \n",
    "            articlesWeight.append(articleWeight)\n",
    "\n",
    "    positive, negative = calculate_positive_vs_negative(articlesWeight)\n",
    "    accuracy = calculate_accuracy(predictiveValues[\"TP\"],\n",
    "                       predictiveValues[\"FP\"],\n",
    "                       predictiveValues[\"FN\"],\n",
    "                       predictiveValues[\"TN\"])\n",
    "\n",
    "    return accuracy, positive, negative\n",
    "\n",
    "\n",
    "# Input: Standardized article object\n",
    "def process_classification(articles, displayInfo):\n",
    "    classificationObject = convert_to_classification_object(articles)\n",
    "    frequencies = word_counter(classificationObject[\"training\"])\n",
    "\n",
    "    # Prior propabilities for positive and negative\n",
    "    pPropability = {'positive': 0.0, 'negative': 0.0}\n",
    "    pPropability['positive'] = calculate_prior_propabilities(frequencies, \"positive\", \"negative\")\n",
    "    pPropability['negative'] = calculate_prior_propabilities(frequencies, \"negative\", \"positive\")\n",
    "\n",
    "    accuracy, positive, negative = classification_result(classificationObject[\"testing\"],\n",
    "                                                          calculate_likelihood(frequencies),\n",
    "                                                          pPropability)\n",
    "    allPositive = [*classificationObject[\"training\"][\"positive\"], *classificationObject[\"testing\"][\"positive\"]]\n",
    "    \n",
    "#    for i in classificationObject:\n",
    "#        for c in classificationObject[i][\"positive\"]:\n",
    "#            print(c[\"title_standardized\"])\n",
    "        \n",
    "    display_classification_results(accuracy,\n",
    "                                   positive,\n",
    "                                   negative,\n",
    "                                   get_most_popular_named_properties(allPositive),\n",
    "                                   displayInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "enormous-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for year: 2020, month: 1, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "No articles found for year: 2020, month: 2, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "No articles found for year: 2020, month: 3, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "No articles found for year: 2020, month: 4, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "No articles found for year: 2020, month: 5, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "========\n",
      "Finished processing articles:\n",
      "DateFrom: 2020-06-01\n",
      "DateTo: 2020-06-30\n",
      "Domains: ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "Accuracy: 82.91 %\n",
      "Portion of positive: 12.66 % \n",
      "20 most popular named entities: [('French', 5), ('France', 4), ('Europe', 3), ('Emmanuel Macron', 3), ('Britain', 3), ('Pompidou Centre', 2), ('Field', 2), ('French Guiana', 2), ('Germany', 2), ('EU', 2), ('Ethiopian', 2), ('Lebanon', 2), ('Global', 2), ('Reuters', 2), ('Israeli', 2), ('Buenos Aires', 2), ('How Covid-19', 2), ('Rohingya', 1), ('Asia', 1), ('Pakistan', 1)]\n",
      "========\n",
      "========\n",
      "Finished processing articles:\n",
      "DateFrom: 2020-07-01\n",
      "DateTo: 2020-07-31\n",
      "Domains: ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "Accuracy: 86.91 %\n",
      "Portion of positive: 8.14 % \n",
      "20 most popular named entities: [('US', 39), ('France', 39), ('French', 33), ('COVID', 16), ('New', 15), ('Brazil', 15), ('South Africa', 14), ('Bastille', 14), ('California', 12), ('U.S.', 11), ('United States', 11), ('Mexico', 11), ('Spain', 11), ('EU', 10), ('Emmanuel Macron', 10), ('China', 10), ('Donald Trump', 8), ('South', 8), ('Europe', 8), ('Africa', 7)]\n",
      "========\n",
      "========\n",
      "Finished processing articles:\n",
      "DateFrom: 2020-08-01\n",
      "DateTo: 2020-08-31\n",
      "Domains: ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "Accuracy: 90.85 %\n",
      "Portion of positive: 4.89 % \n",
      "20 most popular named entities: [('France', 48), ('French', 26), ('US', 15), ('Hong Kong', 14), ('Paris', 10), ('Africa', 8), ('COVID', 8), ('Brazil', 8), ('India', 8), ('Face', 7), ('Europe', 7), ('South Africa', 7), ('New Zealand', 7), ('EU', 6), ('Spanish', 6), ('South', 6), ('Covid', 6), ('Australia', 6), ('New', 5), ('Lebanon', 5)]\n",
      "========\n",
      "========\n",
      "Finished processing articles:\n",
      "DateFrom: 2020-09-01\n",
      "DateTo: 2020-09-30\n",
      "Domains: ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "Accuracy: 87.16 %\n",
      "Portion of positive: 6.90 % \n",
      "20 most popular named entities: [('France', 26), ('US', 13), ('Covid', 13), ('French', 12), ('Paris', 11), ('Europe', 10), ('Rio', 10), ('India', 8), ('Africa', 5), ('American', 5), ('Spain', 4), ('Verdasco', 4), ('Roland Garros', 4), ('England', 4), ('EU', 4), ('Hong Kong', 4), ('Asia', 4), ('Madrid', 4), ('European', 4), ('Latin', 4)]\n",
      "========\n",
      "========\n",
      "Finished processing articles:\n",
      "DateFrom: 2020-10-01\n",
      "DateTo: 2020-10-31\n",
      "Domains: ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "Accuracy: 87.97 %\n",
      "Portion of positive: 5.23 % \n",
      "20 most popular named entities: [('Covid', 52), ('Paris', 51), ('France', 49), ('Trump', 35), ('French', 35), ('Donald Trump', 28), ('Le', 27), ('US', 26), ('Macron', 19), ('Emmanuel Macron', 17), ('Les', 12), ('U.S.', 10), ('White House', 10), ('Selon', 9), ('China', 9), ('Donald', 8), ('Coronavirus', 8), ('NFL', 8), ('La', 8), ('Lille', 8)]\n",
      "========\n",
      "No articles found for year: 2020, month: 11, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "No articles found for year: 2020, month: 12, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n"
     ]
    }
   ],
   "source": [
    "def create_display_info(domains, dateFrom, dateTo):\n",
    "    return {\n",
    "        \"domains\": domains,\n",
    "        \"dateFrom\": dateFrom,\n",
    "        \"dateTo\": dateTo\n",
    "    }\n",
    "    \n",
    "\n",
    "def standardize_and_classify(articles, displayInfo):\n",
    "    standardized = standardizeList(articles)\n",
    "    process_classification(standardized, displayInfo)\n",
    "\n",
    "def text_mine_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    articles, dateFrom, dateTo = get_articles(domains,\"20200101\")\n",
    "\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found from {dateFrom} to {dateTo}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles, create_display_info(domains, dateFrom, dateTo))\n",
    "\n",
    "def text_mine_articles_by_month(domains, year, month):\n",
    "    articles, dateFrom, dateTo = get_articles_by_month(get_domains(), year, month)\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found for year: {year}, month: {month}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles, create_display_info(domains, dateFrom, dateTo))\n",
    "\n",
    "# Mine all articles from 2020\n",
    "#text_mine_articles(get_domains(), \"20200101\")\n",
    "\n",
    "# Mine all articles for each month 2020\n",
    "for a in list(range(1, 13)):\n",
    "    text_mine_articles_by_month(get_domains(), \"2020\", str(a))\n",
    "\n",
    "# Mine all artiles by an outlet\n",
    "#text_mine_articles([\"wsj.com\"])\n",
    "\n",
    "#for m in [{\"20200101\", \"20200131\"}]\n",
    "#text_mine_articles(\"20190101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "continental-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
