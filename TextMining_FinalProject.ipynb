{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "appreciated-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from datetime import date, time, datetime\n",
    "\n",
    "import calendar\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fitting-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should only return 9:  returns:  9\n",
      "['title_standardized', 'description_standardized']\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_number(numberFrom, numberTo, exlude):\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        rand = random.randint(numberFrom, numberTo)\n",
    "        try:\n",
    "            exlude.index(rand)\n",
    "        except:\n",
    "            found = True\n",
    "    return rand\n",
    "    \n",
    "# Returns only 9\n",
    "print(\"Should only return 9: \", \"returns: \", random_number(0, 10, [0,1,2,3,4,5,6,7,8,10]))\n",
    "\n",
    "def sections_to_analyze():\n",
    "    return [\"title_standardized\", \"description_standardized\"]\n",
    "\n",
    "print(sections_to_analyze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "alien-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Leave empty if all domains should be processed\n",
    "_domain = \"\"#[\"wsj.com\"]\n",
    "_folder = \"release\"\n",
    "_dateFrom = \"2020\"\n",
    "\n",
    "_filter = ['covid-19', 'covid']\n",
    "_testThreshold = 0\n",
    "_debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "informational-terminal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20180101 20180131\n",
      "1706\n"
     ]
    }
   ],
   "source": [
    "# Get all domains if no domain is specified\n",
    "def get_domains():\n",
    "    if(_domain == \"\"):\n",
    "        domains = []\n",
    "        for name in listdir(_folder+\"/.\"):\n",
    "            domains.append(name)\n",
    "        return domains\n",
    "    else:\n",
    "        return _domain\n",
    "\n",
    "def convert_to_datetime(_date):\n",
    "    if(len(_date) <= 3 or len(_date) == 5 or len(_date) == 7 or len(_date) > 8):\n",
    "        raise Exception(f\"Input date cannot include {len(_date)} digits - it must contain 4,6 or 8\")\n",
    "    if(len(_date) == 4):\n",
    "        return date(year=int(_date[0:4]), month=1, day=1)\n",
    "    if(len(_date) == 6):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=1)\n",
    "    if(len(_date) == 8):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=int(_date[6:8]))\n",
    "    \n",
    "def compare_date_to_inputdate(_date, dateFrom, dateTo):\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        return True\n",
    "    \n",
    "    criteria = convert_to_datetime(_date)\n",
    "    _dateFrom = convert_to_datetime(\"19000101\")\n",
    "    _dateTo = date.today()\n",
    "\n",
    "    if(dateFrom != \"\"):\n",
    "        _dateFrom = convert_to_datetime(dateFrom) \n",
    "    if(dateTo != \"\"):\n",
    "        _dateTo = convert_to_datetime(dateTo)     \n",
    "        \n",
    "    if(_dateFrom <= criteria and criteria <= _dateTo):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\"\"\" Creates an array with all necessary information for eah article\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "domains: list\n",
    "    Example: ['france24.com', 'bbc.com']\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "list\n",
    "    List of articles content with\n",
    "    {id, domain, title, description}\n",
    "\n",
    "\"\"\"\n",
    "def get_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    print(domains, dateFrom, dateTo)\n",
    "    articles = []\n",
    "    article = {}\n",
    "    alreadyProcessed = []\n",
    "    getAll = False\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        getAll = True\n",
    "\n",
    "    for domain in domains:\n",
    "        for f in listdir(join(_folder+\"/\"+domain, \"per_day\")):\n",
    "            # Takes first 4 numbers from filename (e.g. filename: 20190104.gz)\n",
    "            if(getAll or compare_date_to_inputdate(f[0:8], dateFrom, dateTo)):\n",
    "                try:\n",
    "                    d = json.load(gzip.open(join(_folder+\"/\"+domain, \"per_day\", f)))\n",
    "                except:\n",
    "                    continue\n",
    "                for i in d:\n",
    "                    # Prevent articles to be added more than once\n",
    "                    if i not in alreadyProcessed:\n",
    "                        alreadyProcessed.append(i)         \n",
    "                        articles.append({\n",
    "                            \"id\": i,\n",
    "                            \"domain\": domain,\n",
    "                            \"title\": d[i][\"title\"],\n",
    "                            \"description\": d[i][\"description\"],\n",
    "                            \"date\": f[0:8]\n",
    "                        })\n",
    "                    else:\n",
    "                        continue\n",
    "    return articles\n",
    "\n",
    "def get_articles_by_month(domain, year, month):\n",
    "    start, dayTo = calendar.monthrange(int(year), int(month))\n",
    "\n",
    "    if(dayTo < 10):\n",
    "        dayTo = \"0\"+str(dayTo)\n",
    "    else:\n",
    "        dayTo = str(dayTo)\n",
    "\n",
    "    # Adding in front of single digit\n",
    "    if(len(month) == 1):\n",
    "        month = \"0\"+month\n",
    "                \n",
    "    dateFrom = year+month+\"01\"\n",
    "    dateTo = year+month+dayTo\n",
    "    return get_articles(domain, dateFrom, dateTo)\n",
    "\n",
    "articles = get_articles_by_month(get_domains(), \"2018\", \"1\")\n",
    "print(len(articles))\n",
    "#get_articles_by_month([\"wsj.com\"], \"2020\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "young-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK encapsulation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "_stemmer = PorterStemmer()\n",
    "_wordnetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _sentence_tokenize(text):\n",
    "     return sent_tokenize(text)\n",
    "\n",
    "def _word_tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# {able, possible + -ity → ability, possibility}\n",
    "def _stem(text):\n",
    "    return _stemmer.stem(text)\n",
    "\n",
    "# {playing, plays, played = play}, {am, are, is = be}\n",
    "def _lemmatize(text):\n",
    "    return _wordnetLemmatizer.lemmatize(text)\n",
    "\n",
    "def _speech_tag(wordList):\n",
    "    return nltk.pos_tag(wordList)\n",
    "\n",
    "def _named_entities_chunk(taggedList):\n",
    "    return nltk.ne_chunk(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "healthy-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word standadization\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return _sentence_tokenize(text)\n",
    "\n",
    "# Expects a list from sentence tokenizing\n",
    "def word_list_tokenize(sentenceList):\n",
    "    tokenized = []\n",
    "    for bulk in sentenceList:\n",
    "        for w in _word_tokenize(bulk):\n",
    "            tokenized.append(w)\n",
    "    return tokenized\n",
    "\n",
    "def stemming(textList):\n",
    "    stemmedList = []\n",
    "    for w in textList:\n",
    "        stemmedList.append(_stem(w))\n",
    "    return stemmedList\n",
    "\n",
    "def lemmatization(stemmedList):\n",
    "    lemmaList = []\n",
    "    for w in stemmedList:\n",
    "        lemmaList.append(_lemmatize(w))\n",
    "    return lemmaList\n",
    "\n",
    "def remove_stopwords(wordList):\n",
    "    returnList = []\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    for w in wordList:\n",
    "        if w not in stopWords:  \n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def remove_punctuations(wordList):\n",
    "    punctuations = \".?:!,;‘-’|\"\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in punctuations:\n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def entity_extraction(text):\n",
    "    extracted = []\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    \n",
    "    # Tagging each word\n",
    "    tagged = _speech_tag(wordTokenized)\n",
    "    chunk = _named_entities_chunk(tagged)\n",
    "\n",
    "    for c in chunk:\n",
    "        if type(c) == Tree:\n",
    "            newItems = \" \".join([token for token, pos in c.leaves()])\n",
    "            if newItems not in extracted:\n",
    "                extracted.append(newItems)\n",
    "    return extracted\n",
    "\n",
    "def standardize(text):\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    stemmed = stemming(wordTokenized)\n",
    "    lemma = lemmatization(stemmed)\n",
    "    noStopwords = remove_stopwords(lemma)\n",
    "    noPunctuations = remove_punctuations(noStopwords)\n",
    "    return noPunctuations\n",
    "\n",
    "\"\"\" Standardize the model\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "same list as in the input with additional attributes:\n",
    "    Example: {\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "\"\"\"    \n",
    "def standardizeList(articles):\n",
    "    for article in articles:\n",
    "        article[\"title_standardized\"] = standardize(article[\"title\"])\n",
    "        #article[\"title_entities\"] = entity_extraction(article[\"title\"])\n",
    "        article[\"description_standardized\"] = standardize(article[\"description\"])\n",
    "        #article[\"description_entities\"] = entity_extraction(article[\"description\"])\n",
    "    return articles\n",
    "\n",
    "#articles = get_articles(get_domains())\n",
    "#standardizeList(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "documentary-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set if we want maximum number of samples, used for debugging\n",
    "_samplesThreshold = 0\n",
    "\n",
    "def array_similarity(arr1, arr2):\n",
    "    for a1 in arr1:\n",
    "        for a2 in arr2:\n",
    "            if(a1 == a2):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def create_testing_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i in samples]\n",
    "\n",
    "def create_training_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i not in samples]\n",
    "\n",
    "def create_testing_and_training_data(articles):\n",
    "    # The threshold cannot be higher than the actual data\n",
    "    if(_testThreshold == 0 or len(articles) < _testThreshold):\n",
    "        testThreshold = len(articles)\n",
    "    else:\n",
    "        testThreshold = _testThreshold\n",
    "\n",
    "    samples = random.sample(range(0,len(articles)), testThreshold)\n",
    "    \n",
    "    train, test = train_test_split(articles, test_size=0.2)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "def create_testing_and_training_data_2(articles):\n",
    "    # The threshold cannot be higher than the actual data\n",
    "    if(_testThreshold == 0 or len(articles) < _testThreshold):\n",
    "        testThreshold = len(articles)\n",
    "    else:\n",
    "        testThreshold = _testThreshold\n",
    "\n",
    "    samples = random.sample(range(0,len(articles)), len(articles))\n",
    "    \n",
    "    return create_testing_data(samples, articles),create_training_data(samples, articles)\n",
    "\n",
    "    \n",
    "# Positive (articles that include '_filter' in title or description) will have index = 0\n",
    "# Negative will have index = 1\n",
    "def split_to_positive_and_negative(articles):\n",
    "    positiveResults = []\n",
    "    negativeResults = []\n",
    "    processedIndexes = []\n",
    "    \n",
    "    for a in articles:\n",
    "        if(array_similarity(a[sections_to_analyze()[0]], _filter) == True):\n",
    "            positiveResults.append(a)\n",
    "        elif(array_similarity(a[sections_to_analyze()[1]], _filter) == True):\n",
    "            positiveResults.append(a)\n",
    "        else:\n",
    "            negativeResults.append(a)\n",
    "\n",
    "        if(_samplesThreshold > 0):\n",
    "            if(len(positiveResults) > _samplesThreshold):\n",
    "                break;\n",
    "\n",
    "    return positiveResults, negativeResults\n",
    "\n",
    "\n",
    "\"\"\" Convert to object for classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "classified: object\n",
    "    Example: {  \"testing\":  {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}}},\n",
    "                \"training\": {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}]}\n",
    "             }\n",
    "\"\"\"\n",
    "def convert_to_classification_object(articles):\n",
    "    testAndTrain = create_testing_and_training_data(articles)\n",
    "    \n",
    "    if(_debug):\n",
    "        print(\"All test:\", len(testAndTrain[0]))\n",
    "        print(\"All training: \", len(testAndTrain[1]))\n",
    "        print(\"==\")\n",
    "\n",
    "    test = split_to_positive_and_negative(testAndTrain[0])\n",
    "    train = split_to_positive_and_negative(testAndTrain[1])\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"Test, positive: \", len(test[0]))\n",
    "        print(\"Test, negative: \",len(test[1]))\n",
    "        print(\"Train, positive: \", len(train[0]))\n",
    "        print(\"Train, negative: \", len(train[1]))\n",
    "\n",
    "    testing = {\"positive\":{}, \"negative\":{}}\n",
    "    testing[\"positive\"] = test[0]\n",
    "    testing[\"negative\"] = test[1]\n",
    "\n",
    "    training = {\"positive\":{}, \"negative\":{}}\n",
    "    training[\"positive\"] = train[0]\n",
    "    training[\"negative\"] = train[1]\n",
    "\n",
    "    classified = {\"testing\": {}, \"training\": {}}\n",
    "    classified[\"testing\"] = testing\n",
    "    classified[\"training\"] = training\n",
    "    \n",
    "    return classified\n",
    "\n",
    "# First split to positive/negataive and then to training and testing\n",
    "def convert_to_classification_object_2(articles):\n",
    "    split = split_to_positive_and_negative(articles)\n",
    "    \n",
    "    positiveTest = create_testing_and_training_data(split[0])\n",
    "    negativeTest = create_testing_and_training_data(split[1])\n",
    "    \n",
    "    testing = {\"positive\":{}, \"negative\":{}}\n",
    "    testing[\"positive\"] = positiveTest[0]\n",
    "    testing[\"negative\"] = negativeTest[0]\n",
    "\n",
    "    training = {\"positive\":{}, \"negative\":{}}\n",
    "    training[\"positive\"] = positiveTest[1]\n",
    "    training[\"negative\"] = negativeTest[1]\n",
    "\n",
    "    classified = {\"testing\": {}, \"training\": {}}\n",
    "    classified[\"testing\"] = testing\n",
    "    classified[\"training\"] = training\n",
    "    \n",
    "    if(_debug):\n",
    "        print(\"Testing, positive:\", len(classified[\"testing\"][\"positive\"]))\n",
    "        print(\"Testing, negative:\", len(classified[\"testing\"][\"negative\"]))\n",
    "        print(\"Training, positive:\", len(classified[\"training\"][\"positive\"]))\n",
    "        print(\"Training, negative:\", len(classified[\"training\"][\"negative\"]))\n",
    "\n",
    "    return classified\n",
    "\n",
    "def count_frequencies(data, _class, freq):\n",
    "    for word in data:\n",
    "        try:\n",
    "            freq[_class][word] += 1\n",
    "        except:\n",
    "            freq[_class][word] = 1\n",
    "    return freq\n",
    "\n",
    "def count_total_frequencies(frequencies):\n",
    "    count = 0\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            count += frequencies[_class][w]\n",
    "    return count\n",
    "\n",
    "# Input: Frequency object (described below)\n",
    "# Output: {'positive': {'word':'likelihood', 'word2':'likelihood'}, 'negative': {...} }\n",
    "def calculate_likelihood(frequencies):\n",
    "    p_w = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            p_w[_class][w] = float(frequencies[_class][w]) / float(len(frequencies[_class]))\n",
    "            #p_w[_class][w] = float(frequencies[_class][w]) / count_total_frequencies(frequencies)\n",
    "    return p_w\n",
    "\n",
    "\"\"\" Calculate frequencies from the classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "Returns\n",
    "----------------\n",
    "frequencies: object\n",
    "    Example: {  \"positive\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "                \"negative\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "             }\n",
    "\"\"\"\n",
    "def word_counter(trainingData):\n",
    "    frequencies = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in trainingData:\n",
    "        for article in trainingData[_class]:\n",
    "            for section in sections_to_analyze():\n",
    "                count_frequencies(article[section], _class, frequencies)\n",
    "    return frequencies\n",
    "\n",
    "# Gets the prior probability of P(type1)\n",
    "def calculate_prior_propabilities(frequencies, type1, type2):\n",
    "    return float(len(frequencies[type1])) / float(len(frequencies[type1]) + len(frequencies[type2]))\n",
    "\n",
    "def add_to_predictiveValues(predictiveValues, _type):\n",
    "    try:\n",
    "        predictiveValues[_type] += 1\n",
    "    except:\n",
    "        predictiveValues[_type] = 1\n",
    "\n",
    "def find_predictive_parameter(testClass, articleWeight):\n",
    "    pWeight = articleWeight[\"positive\"]\n",
    "    nWeight = articleWeight[\"negative\"]\n",
    "\n",
    "    if(pWeight >= nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"TP\"\n",
    "        else:\n",
    "            return \"FP\"\n",
    "\n",
    "    if(pWeight < nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"FN\"\n",
    "        if(testClass == \"negative\"):\n",
    "            return \"TN\"\n",
    "                \n",
    "def calculate_weigh_by_article(article, trainingDataLikelihood):\n",
    "    articleWeight = {}\n",
    "    for _trainingClass in trainingDataLikelihood:\n",
    "        for section in sections_to_analyze():\n",
    "            weight = 1\n",
    "            for word in article[section]:\n",
    "                try:\n",
    "                    weight *= trainingDataLikelihood[_trainingClass][word]\n",
    "                except:\n",
    "                    weight *= 0.0001\n",
    "        articleWeight[_trainingClass] = weight\n",
    "\n",
    "    return articleWeight #find_predictive_parameter(_class, combinedWeight)\n",
    "\n",
    "def calculate_accuracy(TP, FP, FN, TN):\n",
    "    numerator = TP + TN\n",
    "    denominator = TP + TN + FP + FN\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def calculate_positive_vs_negative(articlesWeight):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    for article in articlesWeight:\n",
    "        if(article['positive'] >= article['negative']):\n",
    "            countPositive += 1\n",
    "        else:\n",
    "            countNegative += 1\n",
    "    \n",
    "    return countPositive, countNegative\n",
    "\n",
    "def display_classification_results(accuracy, positive, negative):\n",
    "    print(\"========\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Portion of positive: \", float(positive)/(float(negative)+float(positive)) * 100,'%')\n",
    "    print(\"========\")\n",
    "    \n",
    "'''\n",
    "testingData:{'positive': [  'id': '..', 'domain': '..', 'title': '..', 'title_standardized': '..',\n",
    "                            'description': '...', 'description_standardized': '...']}\n",
    "trainingDataLikelihood: {'positive': {'word': likelihood}, {'word2', likelihood}...,\n",
    "                         'negative': {'word': likelihood}, {'word2', likelihood}...}\n",
    "pPropability: {'positive': float_value, 'negative': float_value}\n",
    "'''\n",
    "def classification_result(testingData, trainingDataLikelihood, pPropability):\n",
    "    predictiveValues = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "    articlesWeight = []\n",
    "    \n",
    "    for _class in testingData:\n",
    "        for article in testingData[_class]:\n",
    "            articleWeight = calculate_weigh_by_article(article,trainingDataLikelihood)\n",
    "\n",
    "            predictiveParameter = find_predictive_parameter(_class, articleWeight)\n",
    "            add_to_predictiveValues(predictiveValues, predictiveParameter)\n",
    "            \n",
    "            articlesWeight.append(articleWeight)\n",
    "\n",
    "    positive, negative = calculate_positive_vs_negative(articlesWeight)\n",
    "    accuracy = calculate_accuracy(predictiveValues[\"TP\"],\n",
    "                       predictiveValues[\"FP\"],\n",
    "                       predictiveValues[\"FN\"],\n",
    "                       predictiveValues[\"TN\"])\n",
    "\n",
    "    return accuracy, positive, negative\n",
    "\n",
    "\n",
    "# Input: Standardized article object\n",
    "def process_classification(articles):\n",
    "    print(\"hi\")\n",
    "    classification_object = convert_to_classification_object(articles)\n",
    "    frequencies = word_counter(classification_object[\"training\"])\n",
    "\n",
    "    # Prior propabilities for positive and negative\n",
    "    pPropability = {'positive': 0.0, 'negative': 0.0}\n",
    "    pPropability['positive'] = calculate_prior_propabilities(frequencies, \"positive\", \"negative\")\n",
    "    pPropability['negative'] = calculate_prior_propabilities(frequencies, \"negative\", \"positive\")\n",
    "    \n",
    "    accuracy, positive, negative = classification_result(classification_object[\"testing\"],\n",
    "                                                          calculate_likelihood(frequencies),\n",
    "                                                          pPropability)\n",
    "    \n",
    "    display_classification_results(accuracy, positive, negative)\n",
    "    \n",
    "\n",
    "#articles = get_articles(get_domains())\n",
    "#standardized = standardizeList(articles)\n",
    "#process_classification(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "enormous-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n",
      "========\n",
      "Accuracy:  0.8667287977632805\n",
      "Portion of positive:  13.327120223671947 %\n",
      "========\n",
      "No articles found for year: 2020, month: 1, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n",
      "========\n",
      "Accuracy:  0.8786113699906803\n",
      "Portion of positive:  13.140726933830383 %\n",
      "========\n",
      "No articles found for year: 2020, month: 2, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n",
      "========\n",
      "Accuracy:  0.8825722273998136\n",
      "Portion of positive:  10.880708294501398 %\n",
      "========\n",
      "No articles found for year: 2020, month: 3, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n",
      "========\n",
      "Accuracy:  0.8802423112767941\n",
      "Portion of positive:  11.78937558247903 %\n",
      "========\n",
      "No articles found for year: 2020, month: 4, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n",
      "========\n",
      "Accuracy:  0.8690587138863001\n",
      "Portion of positive:  11.882572227399814 %\n",
      "========\n",
      "No articles found for year: 2020, month: 5, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnormalize_resource_url\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_resource_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36msplit_resource_url\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-d396a8f5dceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtext_mine_articles_by_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2020\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-d396a8f5dceb>\u001b[0m in \u001b[0;36mtext_mine_articles_by_month\u001b[0;34m(domains, year, month)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_mine_articles_by_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_mine_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-d396a8f5dceb>\u001b[0m in \u001b[0;36mtext_mine_articles\u001b[0;34m(domains, dateFrom, dateTo)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No articles found from {dateFrom} to {dateTo}, for domains {domains}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mstandardize_and_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_mine_articles_by_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-d396a8f5dceb>\u001b[0m in \u001b[0;36mstandardize_and_classify\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandardize_and_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstandardized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardizeList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprocess_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_mine_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFrom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateTo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-028030aaa48e>\u001b[0m in \u001b[0;36mstandardizeList\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title_standardized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#article[\"title_entities\"] = entity_extraction(article[\"title\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description_standardized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m#article[\"description_entities\"] = entity_extraction(article[\"description\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-028030aaa48e>\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0msentTokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mwordTokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_list_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentTokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mstemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordTokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-028030aaa48e>\u001b[0m in \u001b[0;36mword_list_tokenize\u001b[0;34m(sentenceList)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbulk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentenceList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_word_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-4ecd8d7143e7>\u001b[0m in \u001b[0;36m_word_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# {able, possible + -ity → ability, possibility}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [\n\u001b[1;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0mresource_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_resource_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0mresource_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnormalize_resource_url\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \"\"\"\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_resource_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# the resource url has no protocol, use the nltk protocol by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def standardize_and_classify(articles):\n",
    "    standardized = standardizeList(articles)\n",
    "    process_classification(standardized)\n",
    "\n",
    "def text_mine_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    articles = get_articles(domains,\"20200101\")\n",
    "    \n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found from {dateFrom} to {dateTo}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles)\n",
    "    \n",
    "def text_mine_articles_by_month(domains, year, month):\n",
    "    articles = text_mine_articles(get_domains(), year, month)\n",
    "    \n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found for year: {year}, month: {month}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles)\n",
    "\n",
    "\n",
    "# Mine all articles from 2020\n",
    "#text_mine_articles(get_domains(), \"20200101\")\n",
    "\n",
    "# Mine all articles for each month 2020\n",
    "for a in list(range(1, 13)):\n",
    "    text_mine_articles_by_month(get_domains(), \"2020\", str(a))\n",
    "\n",
    "# Mine all artiles by an outlet\n",
    "#text_mine_articles([\"wsj.com\"])\n",
    "\n",
    "\n",
    "#for m in [{\"20200101\", \"20200131\"}]\n",
    "#text_mine_articles(\"20190101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "continental-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
