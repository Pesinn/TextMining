{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "appreciated-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from datetime import date, time, datetime\n",
    "\n",
    "import calendar\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fitting-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def sections_to_analyze():\n",
    "    return [\n",
    "        \"title_standardized\",\n",
    "        \"description_standardized\"\n",
    "    ]\n",
    "\n",
    "def languages():\n",
    "    return [\n",
    "        \"english\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "alien-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Leave empty if all domains should be processed\n",
    "# Example for fetching wsj domain: [\"wsj.com\"]\n",
    "_domain = \"\"\n",
    "_folder = \"release\"\n",
    "\n",
    "_filter = ['covid-19', 'covid', 'covid19', 'coronavirus']\n",
    "_debug = False\n",
    "\n",
    "# Number of most popular named entities displayed\n",
    "_namedEntities = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "informational-terminal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all domains if no domain is specified\n",
    "def get_domains():\n",
    "    if(_domain == \"\"):\n",
    "        domains = []\n",
    "        for name in listdir(_folder+\"/.\"):\n",
    "            domains.append(name)\n",
    "        return domains\n",
    "    else:\n",
    "        return _domain\n",
    "\n",
    "def convert_to_datetime(_date):\n",
    "    if(len(_date) <= 3 or len(_date) == 5 or len(_date) == 7 or len(_date) > 8):\n",
    "        raise Exception(f\"Input date cannot include {len(_date)} digits - it must contain 4,6 or 8\")\n",
    "    if(len(_date) == 4):\n",
    "        return date(year=int(_date[0:4]), month=1, day=1)\n",
    "    if(len(_date) == 6):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=1)\n",
    "    if(len(_date) == 8):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=int(_date[6:8]))\n",
    "    \n",
    "def compare_date_to_inputdate(_date, dateFrom, dateTo):\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        return True\n",
    "\n",
    "    criteria = convert_to_datetime(_date)\n",
    "    _dateFrom = convert_to_datetime(\"19000101\")\n",
    "    _dateTo = date.today()\n",
    "\n",
    "    if(dateFrom != \"\"):\n",
    "        _dateFrom = convert_to_datetime(dateFrom) \n",
    "    if(dateTo != \"\"):\n",
    "        _dateTo = convert_to_datetime(dateTo)     \n",
    "        \n",
    "    if(_dateFrom <= criteria and criteria <= _dateTo):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\"\"\" Creates an array with all necessary information for eah article\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "domains: list\n",
    "    Example: ['france24.com', 'bbc.com']\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "list\n",
    "    List of articles content with\n",
    "    {id, domain, title, description}\n",
    "\n",
    "\"\"\"\n",
    "def get_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    if(dateFrom == \"\"):\n",
    "        returnDateFrom = convert_to_datetime(\"19000101\")\n",
    "    else:\n",
    "        returnDateFrom = convert_to_datetime(dateFrom)\n",
    "\n",
    "    if(dateTo == \"\"):\n",
    "        returnDateTo = date.today()\n",
    "    else:\n",
    "        returnDateTo = convert_to_datetime(dateTo)\n",
    "\n",
    "    articles = []\n",
    "    article = {}\n",
    "    alreadyProcessed = []\n",
    "    getAll = False\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        getAll = True\n",
    "\n",
    "    for domain in domains:\n",
    "        for f in listdir(join(_folder+\"/\"+domain, \"per_day\")):\n",
    "            # Takes first 4 numbers from filename (e.g. filename: 20190104.gz)\n",
    "            if(getAll or compare_date_to_inputdate(f[0:8], dateFrom, dateTo)):\n",
    "                try:\n",
    "                    d = json.load(gzip.open(join(_folder+\"/\"+domain, \"per_day\", f)))\n",
    "                except:\n",
    "                    continue\n",
    "                for i in d:\n",
    "                    # Prevent articles to be added more than once\n",
    "                    if i not in alreadyProcessed:\n",
    "                        alreadyProcessed.append(i)         \n",
    "                        articles.append({\n",
    "                            \"id\": i,\n",
    "                            \"domain\": domain,\n",
    "                            \"title\": d[i][\"title\"],\n",
    "                            \"description\": d[i][\"description\"],\n",
    "                            \"date\": f[0:8]\n",
    "                        })\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"Articles fetched: \", len(articles))\n",
    "\n",
    "    return articles, returnDateFrom, returnDateTo\n",
    "\n",
    "def get_articles_by_month(domains, year, month):\n",
    "    start, dayTo = calendar.monthrange(int(year), int(month))\n",
    "\n",
    "    if(dayTo < 10):\n",
    "        dayTo = \"0\"+str(dayTo)\n",
    "    else:\n",
    "        dayTo = str(dayTo)\n",
    "\n",
    "    # Adding in front of single digit\n",
    "    if(len(month) == 1):\n",
    "        month = \"0\"+month\n",
    "\n",
    "    dateFrom = year+month+\"01\"\n",
    "    dateTo = year+month+dayTo\n",
    "\n",
    "    return get_articles(domains, dateFrom, dateTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "young-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization NLTK encapsulation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "_wordnetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _sentence_tokenize_NLTK(text):\n",
    "    return sent_tokenize(text)\n",
    "    \n",
    "def _word_tokenize_NLTK(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# {cars -> car}, {rocks -> rock}\n",
    "def _lemmatize_NLTK(text):\n",
    "    result = _wordnetLemmatizer.lemmatize(text)\n",
    "    # To handle adjectives like: better -> good\n",
    "    return _wordnetLemmatizer.lemmatize(result, pos = \"a\")\n",
    "\n",
    "def _speech_tag_NLTK(wordList):\n",
    "    return nltk.pos_tag(wordList)\n",
    "\n",
    "def _named_entities_chunk_NLTK(taggedList):\n",
    "    return nltk.ne_chunk(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "binary-cemetery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardization default logic\n",
    "\n",
    "# Gets all letters before the index value\n",
    "def get_prefix(text, i):\n",
    "    return text[:i]\n",
    "\n",
    "# Gets all letters after the index value\n",
    "def get_postfix(text, i):\n",
    "    try:\n",
    "        return text[i+1:]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_postfix_including_current(text, i):\n",
    "    try:\n",
    "        return text[i:]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def is_punctuation_for_sentence_tokenize(letter):\n",
    "    punctuations = \".?!\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_punctuation_for_word_tokenize(letter):\n",
    "    punctuations = \".?'â€™:!,\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_end_of_sentence(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "    # If this is the last character\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If we are at space character\n",
    "    if(text[i] == \" \"):\n",
    "        if(prefix != \"\"):\n",
    "            if(is_punctuation_for_sentence_tokenize(text[i-1])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def string_contains_only(_str, char):\n",
    "    for i in _str:\n",
    "        if(i != char):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Standardization default encapsulation\n",
    "def _sentence_tokenize_DEFAULT(text, _list=[]):\n",
    "    i = 0\n",
    "    for l in text:\n",
    "        if(is_end_of_sentence(text, i)):\n",
    "            _list.append(get_prefix(text, i+1).strip())\n",
    "            _sentence_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "# Only returns true for last letter in acronym\n",
    "# E.g. \n",
    "# Letter: A   Prefix: U.S.   Postfix: .\n",
    "# A is the last letter in U.S.A. therefore it's True\n",
    "def is_last_in_acronym(letter, prefix, postfix):\n",
    "    if(letter.isalpha() == False):\n",
    "        return False\n",
    "\n",
    "    if(postfix != \"\"):\n",
    "        if(postfix[0] == \".\"):\n",
    "            if(prefix != \"\"):\n",
    "                if(prefix[len(prefix)-1] == \".\"):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def is_end_of_word(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "\n",
    "    # If last letter\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If punctuation and next letter is space\n",
    "    if(is_punctuation_for_word_tokenize(text[i])):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        else:\n",
    "            # Handle \"...ab\" to \"... ab\"\n",
    "            if(prefix == \"\" or is_punctuation_for_word_tokenize(prefix[i-1])):\n",
    "                if(postfix[0].isalpha() or postfix[0].isnumeric()):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # If we have a letter or number\n",
    "    if(text[i].isalpha() or text[i].isnumeric()):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        # If next letter is a punctuation\n",
    "        elif(is_punctuation_for_word_tokenize(postfix[0])):\n",
    "            innerPostfix = get_postfix(text, i+1)\n",
    "            # Spot 's ending of words\n",
    "            if(postfix[0] == \"'\" or postfix[0] == \"â€™\"):\n",
    "                if(innerPostfix != \"\"):\n",
    "                    if(innerPostfix[0] == \"s\"):\n",
    "                        return True\n",
    "            \n",
    "            # Handles \"Acronyms\"\n",
    "            if(postfix[0] == \".\"):\n",
    "                # Check if this belongs to a acronym, then return False\n",
    "                if(is_last_in_acronym(text[i], prefix, postfix)):\n",
    "                    return False\n",
    "\n",
    "            # If second next letter is space (to cover someURL.com)\n",
    "            if(innerPostfix != \"\"):\n",
    "                if(is_punctuation_for_word_tokenize(innerPostfix[0])):\n",
    "                    return True\n",
    "                if(innerPostfix[0] != \" \"):\n",
    "                    return False\n",
    "                return True\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def word_tokenize_DEFAULT(text, _list=[]):\n",
    "    i=0\n",
    "    word = \"\"\n",
    "    for l in text:\n",
    "        word+=l\n",
    "        if(is_end_of_word(text, i)):\n",
    "            if(word != \"\"):\n",
    "                _list.append(word.strip())\n",
    "            word_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "def _word_tokenize_DEFAULT(text):\n",
    "    return word_tokenize_DEFAULT(text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "focal-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "_useDefault = True\n",
    "\n",
    "def _sentence_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _sentence_tokenize_DEFAULT(text, [])\n",
    "    else:\n",
    "        return _sentence_tokenize_NLTK(text)\n",
    "\n",
    "def _word_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _word_tokenize_DEFAULT(text)\n",
    "    else:\n",
    "        return _word_tokenize_NLTK(text)\n",
    "    \n",
    "def _lemmatize(text):\n",
    "    return _lemmatize_NLTK(text)\n",
    "\n",
    "def _stopwords():\n",
    "    stopWordsList = set()\n",
    "    for l in languages():\n",
    "        stopWordsList = set.union(stopWordsList, set(stopwords.words(l)))\n",
    "    return stopWordsList\n",
    "\n",
    "def _speech_tag(wordList):\n",
    "    return _speech_tag_NLTK(wordList)\n",
    "\n",
    "def _named_entities_chunk(taggedList):\n",
    "    return _named_entities_chunk_NLTK(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "healthy-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont give a shit \n"
     ]
    }
   ],
   "source": [
    "# Bulk standardization\n",
    "text_to_remove = [\n",
    "    \"- ABC News\",\n",
    "    \"| Al Jazeera\",\n",
    "    \"| US & Canada News\",\n",
    "    \"- BBC Sport\",\n",
    "    \"- BBC Reel\",\n",
    "    \"- BBC News\",\n",
    "    \"| CBC News\",\n",
    "    \"- Washington Times\",\n",
    "    \"| Reuters\",\n",
    "    \"- The New York Times\",\n",
    "    \"| Euronews\",\n",
    "    \"| Daily Mail\",\n",
    "    \"- CBS News\"\n",
    "]\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return _sentence_tokenize(text)\n",
    "\n",
    "# Expects a list from sentence tokenizing\n",
    "def word_list_tokenize(sentenceList):\n",
    "    tokenized = []\n",
    "    for bulk in sentenceList:\n",
    "        for w in _word_tokenize(bulk):\n",
    "            tokenized.append(w)\n",
    "    return tokenized\n",
    "\n",
    "def lemmatization(stemmedList):\n",
    "    lemmaList = []\n",
    "    for w in stemmedList:\n",
    "        lemmaList.append(_lemmatize(w))\n",
    "    return lemmaList\n",
    "\n",
    "def remove_stopwords(wordList):\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in _stopwords():  \n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def remove_punctuations(wordList):\n",
    "    punctuations = \".?:!,;â€˜-â€™|\"\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in punctuations:\n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def list_to_lower(_list):\n",
    "    returnList = []\n",
    "    for w in _list:\n",
    "        returnList.append(w.lower())\n",
    "    return returnList\n",
    "\n",
    "# Clean additional information from the article\n",
    "def clean_article_information(text):\n",
    "    for t in text_to_remove:\n",
    "        if(text.find(t) > -1):\n",
    "            text = text.replace(t,'')\n",
    "    return text\n",
    "\n",
    "def standardize(text):\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    lemma = lemmatization(wordTokenized)\n",
    "    noStopwords = remove_stopwords(lemma)\n",
    "    noPunctuations = remove_punctuations(noStopwords)\n",
    "    return noPunctuations\n",
    "\n",
    "\"\"\" Standardize the model\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "same list as in the input with additional attributes:\n",
    "    Example: {\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "\"\"\"\n",
    "def standardizeList(articles):\n",
    "    for article in articles:\n",
    "        text = clean_article_information(article[\"title\"])\n",
    "        text += \" \"\n",
    "        text += clean_article_information(article[\"description\"])\n",
    "        article[\"standardized\"] = standardize(text)\n",
    "    return articles\n",
    "\n",
    "print(clean_article_information(\"I dont give a shit | US & Canada News\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "exterior-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction\n",
    "from itertools import islice\n",
    "\n",
    "def entity_extraction(wordList):\n",
    "    extracted = []\n",
    "    \n",
    "    # Tagging each word\n",
    "    tagged = _speech_tag(wordList)\n",
    "    chunk = _named_entities_chunk(tagged)\n",
    "    for c in chunk:\n",
    "        if type(c) == Tree:\n",
    "            newItems = \" \".join([token for token, pos in c.leaves()])\n",
    "            if newItems not in extracted:\n",
    "                extracted.append(newItems)\n",
    "    return extracted\n",
    "\n",
    "def count_elements(elements):\n",
    "    _dict = {}\n",
    "    if(len(elements) > 0):\n",
    "        for words in elements:\n",
    "            for w in words:\n",
    "                try:\n",
    "                    _dict[w] += 1\n",
    "                except:\n",
    "                    _dict[w] = 1    \n",
    "    return _dict\n",
    "\n",
    "def sort_dictionary_desc(_dict):\n",
    "    return sorted(_dict.items(), key=lambda x: x[1], reverse=True)   \n",
    "\n",
    "def filter_named_properties(elements):\n",
    "    filtered = []\n",
    "    _sorted = sort_dictionary_desc(elements)\n",
    "    i = 0\n",
    "    for w in _sorted:\n",
    "        filtered.append(w)\n",
    "        i += 1\n",
    "        if(i >= _namedEntities):\n",
    "            return filtered\n",
    "\n",
    "    return filtered\n",
    "    \n",
    "def get_named_properties(positive):\n",
    "    entities = []\n",
    "    for article in positive:\n",
    "        entities.append(entity_extraction(article[\"standardized\"]))\n",
    "    return count_elements(entities)\n",
    "\n",
    "# The entry point for the named property extraction\n",
    "def get_most_popular_named_properties(positive):\n",
    "    namedProperties = get_named_properties(positive)\n",
    "    return filter_named_properties(namedProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "wicked-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result\n",
    "\n",
    "# importing the required module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_graph(title, x_labels, y_labels):\n",
    "    left = list(range(1, len(x_labels)+1))\n",
    "\n",
    "    # plotting a bar chart\n",
    "    plt.bar(left, y_labels, tick_label = x_labels,\n",
    "            width = 0.8, color = ['red'])\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "\n",
    "    # plot title\n",
    "    plt.title(title)\n",
    "\n",
    "    # function to show the plot\n",
    "    plt.show()\n",
    "\n",
    "def create_plot_data_for_named_entities(entities):\n",
    "    x_axis = []\n",
    "    values = []\n",
    "    for e in entities:\n",
    "        x_axis.append(e[0])\n",
    "        values.append(e[1])\n",
    "    return x_axis, values\n",
    "\n",
    "def display_results(accuracy, positive, negative, entities, displayInfo):\n",
    "    if(displayInfo is None or displayInfo == \"\"):\n",
    "        dateFrom = \"\"\n",
    "        dateTo = \"\"\n",
    "        domains = \"\"\n",
    "    else:\n",
    "        dateFrom = displayInfo[\"dateFrom\"]\n",
    "        dateTo = displayInfo[\"dateTo\"]\n",
    "        domains = displayInfo[\"domains\"]\n",
    "\n",
    "    displayAccuracy = \"{:.2f}\".format(accuracy * 100)\n",
    "    displayPositive = \"{:.2f}\".format(float(positive)/(float(negative)+float(positive)) * 100)\n",
    "\n",
    "    x_axis, y_axis = create_plot_data_for_named_entities(entities)\n",
    "\n",
    "    print(\"========\")\n",
    "    print(f\"Finished processing articles:\")\n",
    "    print(f\"Positive: {positive}\")\n",
    "    print(f\"Negative: {negative}\")    \n",
    "    print(f\"DateFrom: {dateFrom}\")\n",
    "    print(f\"DateTo: {dateTo}\")\n",
    "    print(f\"Domains: {domains}\")\n",
    "    print(f\"Accuracy: {displayAccuracy} %\")\n",
    "    print(f\"Portion of positive: {displayPositive} % \")\n",
    "    display_graph(f\"{_namedEntities} most popular named entities from {dateFrom} to {dateTo}\", x_axis, y_axis)\n",
    "    print(\"========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "waiting-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "\n",
    "def add_to_predictiveValues(predictiveValues, _type):\n",
    "    try:\n",
    "        predictiveValues[_type] += 1\n",
    "    except:\n",
    "        predictiveValues[_type] = 1\n",
    "\n",
    "def find_predictive_parameter(testClass, articleProbability):\n",
    "    pWeight = articleProbability[\"positive\"]\n",
    "    nWeight = articleProbability[\"negative\"]\n",
    "\n",
    "    if(pWeight >= nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"TP\"\n",
    "        else:\n",
    "            return \"FP\"\n",
    "\n",
    "    if(pWeight < nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"FN\"\n",
    "        if(testClass == \"negative\"):\n",
    "            return \"TN\"\n",
    "\n",
    "def calculate_positive_vs_negative(articlesProbability):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    for article in articlesProbability:\n",
    "        if(article['positive'] >= article['negative']):\n",
    "            countPositive += 1\n",
    "        else:\n",
    "            countNegative += 1\n",
    "    \n",
    "    return countPositive, countNegative\n",
    "\n",
    "# \n",
    "def calculate_probability_by_article(article, trainingDataPropability, pProbability):\n",
    "    articleProbability = {}\n",
    "    for _trainingClass in trainingDataPropability:\n",
    "        p = 1\n",
    "        for word in article[\"standardized\"]:\n",
    "            try:\n",
    "                p *= trainingDataPropability[_trainingClass][word]\n",
    "            except:\n",
    "                p *= 0.0001\n",
    "        p *= pProbability [_trainingClass]\n",
    "        articleProbability[_trainingClass] = p\n",
    "\n",
    "    return articleProbability\n",
    "\n",
    "def calculate_accuracy(TP, FP, FN, TN):\n",
    "    numerator = TP + TN\n",
    "    denominator = TP + TN + FP + FN\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "'''\n",
    "testingData:{'positive': [  'id': '..', 'domain': '..', 'title': '..', 'title_standardized': '..',\n",
    "                            'description': '...', 'description_standardized': '...']}\n",
    "trainingDataPropability: {'positive': {'word': likelihood}, {'word2', likelihood}...,\n",
    "                         'negative': {'word': likelihood}, {'word2', likelihood}...}\n",
    "pPropability: {'positive': float_value, 'negative': float_value}\n",
    "'''\n",
    "def analize(testingData, trainingDataPropability, pPropability):\n",
    "    predictiveValues = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "    articlesProbability = []\n",
    "\n",
    "    for _class in testingData:\n",
    "        for article in testingData[_class]:\n",
    "            articleProbability = calculate_probability_by_article(article,\n",
    "                                                                  trainingDataPropability,\n",
    "                                                                  pPropability)\n",
    "\n",
    "            predictiveParameter = find_predictive_parameter(_class, articleProbability)\n",
    "            add_to_predictiveValues(predictiveValues, predictiveParameter)\n",
    "            \n",
    "            articlesProbability.append(articleProbability)\n",
    "\n",
    "    positive, negative = calculate_positive_vs_negative(articlesProbability)\n",
    "    accuracy = calculate_accuracy(predictiveValues[\"TP\"],\n",
    "                       predictiveValues[\"FP\"],\n",
    "                       predictiveValues[\"FN\"],\n",
    "                       predictiveValues[\"TN\"])\n",
    "\n",
    "    return accuracy, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "documentary-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def array_similarity(arr1, arr2):\n",
    "    for a1 in arr1:\n",
    "        for a2 in arr2:\n",
    "            if(a1.lower() == a2.lower()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def create_testing_and_training_data(articles):\n",
    "    train, test = train_test_split(articles, test_size=0.2)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Positive (articles that include '_filter' in title or description) will have index = 0\n",
    "# Negative will have index = 1\n",
    "def split_to_positive_and_negative(articles):\n",
    "    positiveResults = []\n",
    "    negativeResults = []\n",
    "    processedIndexes = []\n",
    "\n",
    "    for a in articles:\n",
    "        if(array_similarity(a[\"standardized\"], _filter) == True):\n",
    "            positiveResults.append(a)\n",
    "        else:\n",
    "            negativeResults.append(a)\n",
    "\n",
    "    return positiveResults, negativeResults\n",
    "\n",
    "\n",
    "\"\"\" Convert to object for classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "classified: object\n",
    "    Example: {  \"testing\":  {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}}},\n",
    "                \"training\": {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}]}\n",
    "             }\n",
    "\"\"\"\n",
    "def convert_to_classification_object(articles):\n",
    "    testAndTrain = create_testing_and_training_data(articles)\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"All test:\", len(testAndTrain[0]))\n",
    "        print(\"All training: \", len(testAndTrain[1]))\n",
    "        print(\"==\")\n",
    "\n",
    "    test = split_to_positive_and_negative(testAndTrain[0])\n",
    "    train = split_to_positive_and_negative(testAndTrain[1])\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"Test, positive: \", len(test[0]))\n",
    "        print(\"Test, negative: \",len(test[1]))\n",
    "        print(\"Train, positive: \", len(train[0]))\n",
    "        print(\"Train, negative: \", len(train[1]))\n",
    "\n",
    "    testing = {\"positive\":{}, \"negative\":{}}\n",
    "    testing[\"positive\"] = test[0]\n",
    "    testing[\"negative\"] = test[1]\n",
    "\n",
    "    training = {\"positive\":{}, \"negative\":{}}\n",
    "    training[\"positive\"] = train[0]\n",
    "    training[\"negative\"] = train[1]\n",
    "\n",
    "    classified = {\"testing\": {}, \"training\": {}}\n",
    "    classified[\"testing\"] = testing\n",
    "    classified[\"training\"] = training\n",
    "\n",
    "    return classified\n",
    "\n",
    "def count_total_frequencies(frequencies):\n",
    "    count = 0\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            count += frequencies[_class][w]\n",
    "    return count\n",
    "\n",
    "# Input: Frequency object (described below)\n",
    "# Output: {'positive': {'word':'likelihood', 'word2':'likelihood'}, 'negative': {...} }\n",
    "def calculate_probability(frequencies):\n",
    "    p_w = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            p_w[_class][w] = float(frequencies[_class][w]) / float(len(frequencies[_class]))\n",
    "    return p_w\n",
    "\n",
    "def count_frequencies(data, _class, freq):\n",
    "    for word in data:\n",
    "        try:\n",
    "            freq[_class][word] += 1\n",
    "        except:\n",
    "            freq[_class][word] = 1\n",
    "    return freq\n",
    "\n",
    "\"\"\" Calculate frequencies from the classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "Returns\n",
    "----------------\n",
    "frequencies: object\n",
    "    Example: {  \"positive\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "                \"negative\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "             }\n",
    "\"\"\"\n",
    "def word_counter(trainingData):\n",
    "    frequencies = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in trainingData:\n",
    "        for article in trainingData[_class]:\n",
    "            count_frequencies(article[\"standardized\"], _class, frequencies)\n",
    "    return frequencies\n",
    "\n",
    "# Gets the prior probability of P(type1)\n",
    "def calculate_prior_propabilities(frequencies, type1, type2):\n",
    "    return float(len(frequencies[type1])) / float(len(frequencies[type1]) + len(frequencies[type2]))\n",
    "\n",
    "# Input: Standardized article object\n",
    "def classify_and_analyze(articles):\n",
    "    classified = convert_to_classification_object(articles)\n",
    "    frequencies = word_counter(classified[\"training\"])\n",
    "\n",
    "    # Prior propabilities for positive and negative\n",
    "    pPropability = {'positive': 0.0, 'negative': 0.0}\n",
    "    pPropability['positive'] = calculate_prior_propabilities(frequencies, \"positive\", \"negative\")\n",
    "    pPropability['negative'] = calculate_prior_propabilities(frequencies, \"negative\", \"positive\")\n",
    "    \n",
    "    # Analize\n",
    "    accuracy, positive, negative = analize(classified[\"testing\"],\n",
    "                                              calculate_probability(frequencies),\n",
    "                                              pPropability)\n",
    "    allPositive = [*classified[\"training\"][\"positive\"], *classified[\"testing\"][\"positive\"]]\n",
    "\n",
    "    namedProperties = get_most_popular_named_properties(allPositive)\n",
    "    \n",
    "    return accuracy, positive, negative, namedProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "enormous-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Finished processing articles:\n",
      "Positive: 48\n",
      "Negative: 2248\n",
      "DateFrom: 2020-01-01\n",
      "DateTo: 2020-02-01\n",
      "Domains: ['bbc.com']\n",
      "Accuracy: 97.91 %\n",
      "Portion of positive: 2.09 % \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n"
     ]
    }
   ],
   "source": [
    "# The pipelines are used from this point\n",
    "\n",
    "def create_display_info(domains, dateFrom, dateTo):\n",
    "    return {\n",
    "        \"domains\": domains,\n",
    "        \"dateFrom\": dateFrom,\n",
    "        \"dateTo\": dateTo\n",
    "    }\n",
    "\n",
    "def standardize_and_classify(articles):\n",
    "    standardized = standardizeList(articles)\n",
    "    return classify_and_analyze(standardized)\n",
    "\n",
    "def text_mine_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    articles, dateFrom, dateTo = get_articles(domains, dateFrom, dateTo)\n",
    "\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found from {dateFrom} to {dateTo}, for domains {domains}\")\n",
    "    else:\n",
    "        accuracy, positive, negative, namedProperties = standardize_and_classify(articles)\n",
    "\n",
    "        displayInfo = create_display_info(domains, dateFrom, dateTo)\n",
    "\n",
    "        # Display results from analyzation\n",
    "        display_results(accuracy,\n",
    "                    positive,\n",
    "                    negative,\n",
    "                    namedProperties,\n",
    "                    displayInfo)\n",
    "\n",
    "def text_mine_articles_by_month(domains, year, month):\n",
    "    articles, dateFrom, dateTo = get_articles_by_month(get_domains(), year, month)\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found for year: {year}, month: {month}, for domains {domains}\")\n",
    "        return 0, 0\n",
    "    else:\n",
    "        accuracy, positive, negative, namedProperties = standardize_and_classify(articles)\n",
    "        \n",
    "        displayInfo = create_display_info(domains, dateFrom, dateTo)\n",
    "        \n",
    "        # Display results from analyzation\n",
    "        display_results(accuracy,\n",
    "                    positive,\n",
    "                    negative,\n",
    "                    namedProperties,\n",
    "                    displayInfo)\n",
    "        \n",
    "        return positive, negative\n",
    "\n",
    "# Process all articles for each month for certain year\n",
    "def each_month(year):\n",
    "    months = [\"Jan\",\"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    outcome = []\n",
    "    for a in list(range(1, 13)):\n",
    "        positive, negative = text_mine_articles_by_month(get_domains(), year, str(a))\n",
    "\n",
    "        # Create an array that is used for displaying statistics for each month\n",
    "        if(positive != 0):\n",
    "            o = float(positive)/(float(negative)+float(positive)) * 100\n",
    "            outcome.append(round(o))\n",
    "        else:\n",
    "            outcome.append(0)\n",
    "    display_graph(f\"Portion of positive for each month in year {year}\", months, outcome)\n",
    "\n",
    "# Process all articles from 2020\n",
    "#text_mine_articles(get_domains(), \"20200101\", \"20201231\")\n",
    "\n",
    "# Process all articles for each month in 2020\n",
    "#each_month(\"2020\")\n",
    "\n",
    "# Process all artiles in an outlet in 2020\n",
    "text_mine_articles([\"bbc.com\"], \"20200101\", \"20200201\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
