{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "appreciated-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from datetime import date, time, datetime\n",
    "\n",
    "import calendar\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fitting-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should only return 9:  returns:  9\n",
      "['title_standardized', 'description_standardized']\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_number(numberFrom, numberTo, exlude):\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        rand = random.randint(numberFrom, numberTo)\n",
    "        try:\n",
    "            exlude.index(rand)\n",
    "        except:\n",
    "            found = True\n",
    "    return rand\n",
    "\n",
    "# Returns only 9\n",
    "print(\"Should only return 9: \", \"returns: \", random_number(0, 10, [0,1,2,3,4,5,6,7,8,10]))\n",
    "\n",
    "def sections_to_analyze():\n",
    "    return [\"title_standardized\", \"description_standardized\"]\n",
    "\n",
    "print(sections_to_analyze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "alien-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Leave empty if all domains should be processed\n",
    "_domain = \"\"#[\"wsj.com\"]\n",
    "_folder = \"release\"\n",
    "_dateFrom = \"2020\"\n",
    "\n",
    "_filter = ['covid-19', 'covid']\n",
    "_testThreshold = 0\n",
    "_debug = False\n",
    "\n",
    "# Number of most popular named entities displayed\n",
    "_namedEntities = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "informational-terminal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20180201 20180228\n"
     ]
    }
   ],
   "source": [
    "# Get all domains if no domain is specified\n",
    "def get_domains():\n",
    "    if(_domain == \"\"):\n",
    "        domains = []\n",
    "        for name in listdir(_folder+\"/.\"):\n",
    "            domains.append(name)\n",
    "        return domains\n",
    "    else:\n",
    "        return _domain\n",
    "\n",
    "def convert_to_datetime(_date):\n",
    "    if(len(_date) <= 3 or len(_date) == 5 or len(_date) == 7 or len(_date) > 8):\n",
    "        raise Exception(f\"Input date cannot include {len(_date)} digits - it must contain 4,6 or 8\")\n",
    "    if(len(_date) == 4):\n",
    "        return date(year=int(_date[0:4]), month=1, day=1)\n",
    "    if(len(_date) == 6):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=1)\n",
    "    if(len(_date) == 8):\n",
    "        return date(year=int(_date[0:4]), month=int(_date[4:6]), day=int(_date[6:8]))\n",
    "    \n",
    "def compare_date_to_inputdate(_date, dateFrom, dateTo):\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        return True\n",
    "\n",
    "    criteria = convert_to_datetime(_date)\n",
    "    _dateFrom = convert_to_datetime(\"19000101\")\n",
    "    _dateTo = date.today()\n",
    "\n",
    "    if(dateFrom != \"\"):\n",
    "        _dateFrom = convert_to_datetime(dateFrom) \n",
    "    if(dateTo != \"\"):\n",
    "        _dateTo = convert_to_datetime(dateTo)     \n",
    "        \n",
    "    if(_dateFrom <= criteria and criteria <= _dateTo):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\"\"\" Creates an array with all necessary information for eah article\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "domains: list\n",
    "    Example: ['france24.com', 'bbc.com']\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "list\n",
    "    List of articles content with\n",
    "    {id, domain, title, description}\n",
    "\n",
    "\"\"\"\n",
    "def get_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    if(dateFrom == \"\"):\n",
    "        returnDateFrom = convert_to_datetime(\"19000101\")\n",
    "    else:\n",
    "        returnDateFrom = convert_to_datetime(dateFrom)\n",
    "\n",
    "    if(dateTo == \"\"):\n",
    "        returnDateTo = date.today()\n",
    "    else:\n",
    "        returnDateTo = convert_to_datetime(dateTo)\n",
    "\n",
    "    articles = []\n",
    "    article = {}\n",
    "    alreadyProcessed = []\n",
    "    getAll = False\n",
    "    if(dateFrom == \"\" and dateTo == \"\"):\n",
    "        getAll = True\n",
    "\n",
    "    for domain in domains:\n",
    "        for f in listdir(join(_folder+\"/\"+domain, \"per_day\")):\n",
    "            # Takes first 4 numbers from filename (e.g. filename: 20190104.gz)\n",
    "            if(getAll or compare_date_to_inputdate(f[0:8], dateFrom, dateTo)):\n",
    "                try:\n",
    "                    d = json.load(gzip.open(join(_folder+\"/\"+domain, \"per_day\", f)))\n",
    "                except:\n",
    "                    continue\n",
    "                for i in d:\n",
    "                    # Prevent articles to be added more than once\n",
    "                    if i not in alreadyProcessed:\n",
    "                        alreadyProcessed.append(i)         \n",
    "                        articles.append({\n",
    "                            \"id\": i,\n",
    "                            \"domain\": domain,\n",
    "                            \"title\": d[i][\"title\"],\n",
    "                            \"description\": d[i][\"description\"],\n",
    "                            \"date\": f[0:8]\n",
    "                        })\n",
    "                    else:\n",
    "                        continue\n",
    "    return articles, returnDateFrom, returnDateTo\n",
    "\n",
    "def get_articles_by_month(domains, year, month):\n",
    "    start, dayTo = calendar.monthrange(int(year), int(month))\n",
    "\n",
    "    if(dayTo < 10):\n",
    "        dayTo = \"0\"+str(dayTo)\n",
    "    else:\n",
    "        dayTo = str(dayTo)\n",
    "\n",
    "    # Adding in front of single digit\n",
    "    if(len(month) == 1):\n",
    "        month = \"0\"+month\n",
    "\n",
    "    dateFrom = year+month+\"01\"\n",
    "    dateTo = year+month+dayTo\n",
    "    \n",
    "    print(domains, dateFrom, dateTo)\n",
    "    return get_articles(domains, dateFrom, dateTo)\n",
    "\n",
    "articles = get_articles_by_month(get_domains(), \"2018\", \"2\")\n",
    "#get_articles_by_month([\"wsj.com\"], \"2020\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "young-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization NLTK encapsulation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "_stemmer = PorterStemmer()\n",
    "_wordnetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _sentence_tokenize_NLTK(text):\n",
    "    return sent_tokenize(text)\n",
    "    \n",
    "def _word_tokenize_NLTK(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# {able, possible + -ity → ability, possibility}\n",
    "def _stem_NLTK(text):\n",
    "    return _stemmer.stem(text)\n",
    "\n",
    "# {playing, plays, played = play}, {am, are, is = be}\n",
    "def _lemmatize_NLTK(text):\n",
    "    return _wordnetLemmatizer.lemmatize(text)\n",
    "\n",
    "def _speech_tag_NLTK(wordList):\n",
    "    return nltk.pos_tag(wordList)\n",
    "\n",
    "def _named_entities_chunk_NLTK(taggedList):\n",
    "    return nltk.ne_chunk(taggedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "hearing-ranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the']\n"
     ]
    }
   ],
   "source": [
    "_list = [\"This is the U.S. Coast Guard. Why do we care? Or what.\"]\n",
    "\n",
    "print([_list[0][:4],_list[0][5:7],_list[0][8:11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "limited-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'letter': 'A', 'split': True}\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "_dict = [\n",
    "    {\n",
    "        \"letter\": \"A\",\n",
    "        \"split\": False\n",
    "    },\n",
    "    {\n",
    "        \"letter\": \"B\",\n",
    "        \"split\": True\n",
    "    },\n",
    "    {\n",
    "        \"letter\": \"A\",\n",
    "        \"split\": True\n",
    "    },\n",
    "    {\n",
    "        \"letter\": \"C\",\n",
    "        \"split\": False\n",
    "    },\n",
    "]\n",
    "\n",
    "print(_dict[2])\n",
    "\n",
    "_dict[0][\"split\"] = True\n",
    "\n",
    "for x in _dict:\n",
    "  if x[\"letter\"] == \"A\":\n",
    "    print(x[\"split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "binary-cemetery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"America's\", 'most']\n"
     ]
    }
   ],
   "source": [
    "# Standardization default logic\n",
    "\n",
    "\n",
    "\n",
    "# Gets all letters before the index value\n",
    "def get_prefix(text, i):\n",
    "    return text[:i]\n",
    "\n",
    "# Gets all letters after the index value\n",
    "def get_postfix(text, i):\n",
    "    try:\n",
    "        return text[i+1:]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_postfix_including_current(text, i):\n",
    "    try:\n",
    "        return text[i:]\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def isUpper(text, i):\n",
    "    try:\n",
    "        if(text[i].isupper()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def isLower(text, i):\n",
    "    try:\n",
    "        if(text[i].islower()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def isNumber(text, i):\n",
    "    try:\n",
    "        if(text[i].isnumeric()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_punctuation_for_sentence_tokenize(letter):\n",
    "    punctuations = \".?!\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_punctuation_for_word_tokenize(letter):\n",
    "    punctuations = \".?!,\"\n",
    "    if letter in punctuations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_end_of_sentence(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "    # If this is the last character\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If we are at space character\n",
    "    if(text[i] == \" \"):\n",
    "        if(prefix != \"\"):\n",
    "            if(is_punctuation_for_sentence_tokenize(text[i-1])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def string_contains_only(_str, char):\n",
    "    for i in _str:\n",
    "        if(i != char):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Standardization default encapsulation\n",
    "def _sentence_tokenize_DEFAULT(text, _list=[]):\n",
    "    i = 0\n",
    "    for l in text:\n",
    "        if(is_end_of_sentence(text, i)):\n",
    "            _list.append(get_prefix(text, i+1).strip())\n",
    "            _sentence_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "def is_end_of_word(text, i):\n",
    "    prefix = get_prefix(text, i)\n",
    "    postfix = get_postfix(text, i)\n",
    "\n",
    "    # If last letter\n",
    "    if(postfix == \"\"):\n",
    "        return True\n",
    "\n",
    "    # If punctuation and next letter is space\n",
    "    if(is_punctuation_for_word_tokenize(text[i])):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "#    if(text[i] == \"’\"):\n",
    "#        if(prefix != \"\"):\n",
    "#            if(prefix[i-1] != \" \"):\n",
    "#                if(postfix[0] != \" \"):\n",
    "#                    return True\n",
    "        \n",
    "    # If we have a letter or number\n",
    "    if(text[i].isalpha() or text[i].isnumeric()):\n",
    "        if(postfix[0] == \" \"):\n",
    "            return True\n",
    "        # If next letter is a punctuation\n",
    "        elif(is_punctuation_for_word_tokenize(postfix[0])):            \n",
    "            # If second next letter is space (to cover someURL.com)\n",
    "            innerPostfix = get_postfix(text, i+1)\n",
    "            if(innerPostfix != \"\"):\n",
    "                if(innerPostfix[0] != \" \"):\n",
    "                    return False\n",
    "                return True\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def word_tokenize_DEFAULT(text, _list=[]):\n",
    "    i=0\n",
    "    word = \"\"\n",
    "    for l in text:\n",
    "        word+=l\n",
    "        if(is_end_of_word(text, i)):\n",
    "            if(word != \"\"):\n",
    "                _list.append(word.strip())\n",
    "            word_tokenize_DEFAULT(get_postfix(text, i), _list)\n",
    "            break\n",
    "        i+=1\n",
    "    return _list\n",
    "\n",
    "def word_tokenize_DEFAULT_2(text, hi):\n",
    "    _list = []\n",
    "    for word in text.split():\n",
    "        _list.append(word)\n",
    "    return _list\n",
    "\n",
    "def _word_tokenize_DEFAULT(text):\n",
    "    return word_tokenize_DEFAULT_2(text, [])\n",
    "\n",
    "def _stem_DEFAULT(text):\n",
    "    return 1\n",
    "\n",
    "# {playing, plays, played = play}, {am, are, is = be}\n",
    "def _lemmatize_DEFAULT(text):\n",
    "    return 1\n",
    "\n",
    "def _speech_tag_DEFAULT(wordList):\n",
    "    return 1\n",
    "\n",
    "def _named_entities_chunk_DEFAULT(taggedList):\n",
    "    return 1\n",
    "\n",
    "\n",
    "#print(is_end_of_sentence(\"Or what.\", 7))\n",
    "#print(is_punctuation(\".\"))\n",
    "#print(_sentence_tokenize_NLTK(\"This is the U.S. Coast Guard. Why do we care? Or what.\"))#\n",
    "#print(_sentence_tokenize_DEFAULT(\"This is the U.S. Coast Guard. Why do we care? Or what.\"))\n",
    "\n",
    "#print(_word_tokenize_DEFAULT(\"Trying. To hold the center can be lonely when colleagues disagree. Here are four lessons from the playbook of Supreme Court “umpire” John Roberts.\"))\n",
    "s1 = \"America's most\"\n",
    "\n",
    "print(_word_tokenize_DEFAULT(s1))\n",
    "#print(_word_tokenize_NLTK(\"That’s great\"))\n",
    "#print(_word_tokenize_NLTK(\"Trying to hold the center can be lonely when colleagues disagree. Here are four lessons from the playbook of Supreme Court “umpire” John Roberts.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "focal-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'This is another sentence, what about this?']\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "_useDefault = True\n",
    "\n",
    "def _sentence_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _sentence_tokenize_DEFAULT(text, [])\n",
    "    else:\n",
    "        return _sentence_tokenize_NLTK(text)\n",
    "\n",
    "def _word_tokenize(text):\n",
    "    if(_useDefault):\n",
    "        return _word_tokenize_DEFAULT(text)\n",
    "    else:\n",
    "        return _word_tokenize_NLTK(text)\n",
    "\n",
    "def _lemmatize(text):\n",
    "    if(_useDefault):\n",
    "        return _lemmatize_DEFAULT(text)\n",
    "    else:\n",
    "        return _lemmatize_NLTK(text)\n",
    "\n",
    "def _speech_tag(wordList):\n",
    "    if(False):\n",
    "        return _speech_tag_DEFAULT(wordList)\n",
    "    else:\n",
    "        return _speech_tag_NLTK(wordList)\n",
    "\n",
    "def _named_entities_chunk(taggedList):\n",
    "    if(False):\n",
    "        return _named_entities_chunk_DEFAULT(taggedList)\n",
    "    else:\n",
    "        return _named_entities_chunk_NLTK(taggedList)\n",
    "    \n",
    "print(_sentence_tokenize_NLTK(\"This is a sentence. This is another sentence, what about this?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "sexual-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Airlines', 'likely', 'have', 'enough', 'cash', 'to', 'withstand', 'a', 'prolonged', 'downturn,', 'analysts', 'say.', 'But', 'it', 'will', 'be', 'years', 'before', 'passenger', 'demand', 'recovers,', 'chief', 'United', 'and', 'Delta', 'executives', 'said', 'this', 'week.']\n",
      "['Airlines', 'likely', 'have', 'enough', 'cash', 'to', 'withstand', 'a', 'prolonged', 'downturn', ',', 'analysts', 'say', '.', 'But', 'it', 'will', 'be', 'years', 'before', 'passenger', 'demand', 'recovers', ',', 'chief', 'United', 'and', 'Delta', 'executives', 'said', 'this', 'week', '.']\n",
      "['View', 'membership', 'options', 'and', 'subscribe', 'to', \"America's\", 'most', 'trusted', 'newspaper', 'for', 'full', 'access', 'to', 'WSJ.com,', 'WSJ', 'apps,', 'WSJplus', 'premium', 'benefits', 'and', 'curated', 'newsletters.']\n",
      "['View', 'membership', 'options', 'and', 'subscribe', 'to', 'America', \"'s\", 'most', 'trusted', 'newspaper', 'for', 'full', 'access', 'to', 'WSJ.com', ',', 'WSJ', 'apps', ',', 'WSJplus', 'premium', 'benefits', 'and', 'curated', 'newsletters', '.']\n",
      "['Dow', 'Jones']\n",
      "['Dow', 'Jones']\n",
      "['To', 'stimulate', 'its', 'pandemic-hit', 'economy,', 'a', 'province', 'in', 'South', 'Korea', 'has', 'been', 'experimenting', 'with', 'universal', 'basic', 'income', 'programs', 'by', 'regularly', 'giving', 'out', 'cash,', 'no', 'questions', 'asked.', 'Now,', 'some', 'politicians', 'want', 'to', 'go', 'national', 'with', 'the', 'concept.', 'Illustration:', 'Crystal', 'Tai/WSJ']\n",
      "['To', 'stimulate', 'its', 'pandemic-hit', 'economy', ',', 'a', 'province', 'in', 'South', 'Korea', 'has', 'been', 'experimenting', 'with', 'universal', 'basic', 'income', 'programs', 'by', 'regularly', 'giving', 'out', 'cash', ',', 'no', 'questions', 'asked', '.', 'Now', ',', 'some', 'politicians', 'want', 'to', 'go', 'national', 'with', 'the', 'concept', '.', 'Illustration', ':', 'Crystal', 'Tai/WSJ']\n",
      "['The', 'U.S.', 'is', 'one', 'of', 'the', 'few', 'countries', 'that', 'requires', 'its', 'citizens', 'to', 'file', 'taxes', 'while', 'living', 'abroad.', 'For', 'some,', 'it', 'is', 'enough', 'to', 'push', 'them', 'to', 'give', 'up', 'their', 'citizenship.']\n",
      "['The', 'U.S.', 'is', 'one', 'of', 'the', 'few', 'countries', 'that', 'requires', 'its', 'citizens', 'to', 'file', 'taxes', 'while', 'living', 'abroad', '.', 'For', 'some', ',', 'it', 'is', 'enough', 'to', 'push', 'them', 'to', 'give', 'up', 'their', 'citizenship', '.']\n",
      "['Trying', 'to', 'hold', 'the', 'center', 'can', 'be', 'lonely', 'when', 'colleagues', 'disagree.', 'Here', 'are', 'four', 'lessons', 'from', 'the', 'playbook', 'of', 'Supreme', 'Court', '“umpire”', 'John', 'Roberts.']\n",
      "['Trying', 'to', 'hold', 'the', 'center', 'can', 'be', 'lonely', 'when', 'colleagues', 'disagree', '.', 'Here', 'are', 'four', 'lessons', 'from', 'the', 'playbook', 'of', 'Supreme', 'Court', '“', 'umpire', '”', 'John', 'Roberts', '.']\n",
      "['Latest', 'from', '“Future', 'View”', 'in', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Latest', 'from', '“', 'Future', 'View', '”', 'in', 'The', 'Wall', 'Street', 'Journal']\n",
      "['The', 'amendment,', 'passed', 'by', 'the', 'SEC', 'on', 'Friday,', 'gives', 'auditors', 'more', 'discretion', 'in', 'assessing', 'conflicts', 'of', 'interest', 'in', 'their', 'relationships', 'with', 'the', 'businesses', 'they', 'audit,', 'in', 'situations', 'such', 'as', 'those', 'involving', 'affiliates', 'of', 'their', 'clients', 'or', 'past', 'lenders.', 'The', 'rule', 'also', 'intends', 'to', 'lessen', 'the', 'burden', 'for', 'companies', 'trying', 'to', 'go', 'public.']\n",
      "['The', 'amendment', ',', 'passed', 'by', 'the', 'SEC', 'on', 'Friday', ',', 'gives', 'auditors', 'more', 'discretion', 'in', 'assessing', 'conflicts', 'of', 'interest', 'in', 'their', 'relationships', 'with', 'the', 'businesses', 'they', 'audit', ',', 'in', 'situations', 'such', 'as', 'those', 'involving', 'affiliates', 'of', 'their', 'clients', 'or', 'past', 'lenders', '.', 'The', 'rule', 'also', 'intends', 'to', 'lessen', 'the', 'burden', 'for', 'companies', 'trying', 'to', 'go', 'public', '.']\n",
      "['Wuhan,', 'the', 'city', 'at', 'the', 'center', 'of', 'the', 'coronavirus', 'pandemic,', 'had', 'the', 'most', 'tourists', 'of', 'any', 'Chinese', 'city', 'during', 'a', 'public', 'holiday', 'in', 'October.', 'Wuhan', 'is', 'overcoming', 'its', 'pandemic', 'past', 'and', 'benefiting', 'from', 'its', 'hero-city', 'status', 'to', 'become', 'a', 'top', 'travel', 'destination.', 'Photo:', 'Getty', 'Images']\n",
      "['Wuhan', ',', 'the', 'city', 'at', 'the', 'center', 'of', 'the', 'coronavirus', 'pandemic', ',', 'had', 'the', 'most', 'tourists', 'of', 'any', 'Chinese', 'city', 'during', 'a', 'public', 'holiday', 'in', 'October', '.', 'Wuhan', 'is', 'overcoming', 'its', 'pandemic', 'past', 'and', 'benefiting', 'from', 'its', 'hero-city', 'status', 'to', 'become', 'a', 'top', 'travel', 'destination', '.', 'Photo', ':', 'Getty', 'Images']\n",
      "['Welcome', 'to', 'Noted,', 'a', 'news', 'and', 'culture', 'magazine', 'from', 'The', 'Wall', 'Street', 'Journal.']\n",
      "['Welcome', 'to', 'Noted', ',', 'a', 'news', 'and', 'culture', 'magazine', 'from', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['Gerry', 'Baker', 'is', 'Editor-at-Large', 'of', 'The', 'Wall', 'Street', 'Journal.', 'His', 'weekly', 'column', 'for', 'the', 'editorial', 'page,', '“Free', 'Expression,”', 'appears', 'in', 'The', 'Wall', 'Street', 'Journal', 'each', 'Tuesday.', 'Mr.', 'Baker', 'is', 'also', 'host', 'of', '“WSJ', 'at', 'Large', 'with', 'Gerry', 'Baker,”', 'a', 'weekly', 'news', 'and', 'current', 'affairs', 'interview', 'show', 'on', 'the', 'Fox', 'Business', 'Network.']\n",
      "['Gerry', 'Baker', 'is', 'Editor-at-Large', 'of', 'The', 'Wall', 'Street', 'Journal', '.', 'His', 'weekly', 'column', 'for', 'the', 'editorial', 'page', ',', '“', 'Free', 'Expression', ',', '”', 'appears', 'in', 'The', 'Wall', 'Street', 'Journal', 'each', 'Tuesday', '.', 'Mr.', 'Baker', 'is', 'also', 'host', 'of', '“', 'WSJ', 'at', 'Large', 'with', 'Gerry', 'Baker', ',', '”', 'a', 'weekly', 'news', 'and', 'current', 'affairs', 'interview', 'show', 'on', 'the', 'Fox', 'Business', 'Network', '.']\n",
      "['Women', 'have', 'good', 'reason', 'to', 'be', 'more', 'anxious', 'about', 'the', 'size', 'of', 'a', 'nest', 'egg.']\n",
      "['Women', 'have', 'good', 'reason', 'to', 'be', 'more', 'anxious', 'about', 'the', 'size', 'of', 'a', 'nest', 'egg', '.']\n",
      "['As', 'companies', 'reopen', 'their', 'offices', 'and', 'we', 'shift', 'away', 'from', 'WFH,', 'there’s', 'a', 'lot', 'to', 'think', 'about—from', 'the', 'post-pandemic', 'dress', 'code', 'to', 'whether', 'your', 'boss', 'can', 'force', 'you', 'to', 'get', 'a', 'vaccine.', 'We’ve', 'tackled', 'it', 'all.']\n",
      "['As', 'companies', 'reopen', 'their', 'offices', 'and', 'we', 'shift', 'away', 'from', 'WFH', ',', 'there', '’', 's', 'a', 'lot', 'to', 'think', 'about—from', 'the', 'post-pandemic', 'dress', 'code', 'to', 'whether', 'your', 'boss', 'can', 'force', 'you', 'to', 'get', 'a', 'vaccine', '.', 'We', '’', 've', 'tackled', 'it', 'all', '.']\n",
      "['A', 'group', 'of', 'information', 'technology', 'companies', 'filed', 'a', 'lawsuit', 'challenging', 'a', 'Trump', 'administration', 'rule', 'raising', 'the', 'salaries', 'that', 'employers', 'must', 'pay', 'to', 'their', 'foreign', 'workers', 'on', 'H-1B', 'visas,', 'aimed', 'at', 'tightening', 'eligibility', 'for', 'the', 'visas.']\n",
      "['A', 'group', 'of', 'information', 'technology', 'companies', 'filed', 'a', 'lawsuit', 'challenging', 'a', 'Trump', 'administration', 'rule', 'raising', 'the', 'salaries', 'that', 'employers', 'must', 'pay', 'to', 'their', 'foreign', 'workers', 'on', 'H-1B', 'visas', ',', 'aimed', 'at', 'tightening', 'eligibility', 'for', 'the', 'visas', '.']\n",
      "['An', 'open', 'letter', 'criticized', 'the', 'nation’s', 'public-health', 'response', 'to', 'the', 'Covid-19', 'pandemic', 'and', 'called', 'for', 'the', 'federal', 'agency', 'to', 'play', 'a', 'more', 'central', 'role.']\n",
      "['An', 'open', 'letter', 'criticized', 'the', 'nation', '’', 's', 'public-health', 'response', 'to', 'the', 'Covid-19', 'pandemic', 'and', 'called', 'for', 'the', 'federal', 'agency', 'to', 'play', 'a', 'more', 'central', 'role', '.']\n",
      "[]\n",
      "[]\n",
      "['Keep', 'your', 'patrons', 'inspired', 'with', 'the', 'insights', 'they', 'need', 'from', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Keep', 'your', 'patrons', 'inspired', 'with', 'the', 'insights', 'they', 'need', 'from', 'The', 'Wall', 'Street', 'Journal']\n",
      "['In', 'an', 'already', 'strange', 'year', 'for', 'Broadway,', 'theatrical', 'veteran', 'Aaron', 'Tveit', 'is', 'the', 'only', 'nominee', 'for', 'best', 'lead', 'actor', 'in', 'musical;', '“a', 'huge', 'honor.”']\n",
      "['In', 'an', 'already', 'strange', 'year', 'for', 'Broadway', ',', 'theatrical', 'veteran', 'Aaron', 'Tveit', 'is', 'the', 'only', 'nominee', 'for', 'best', 'lead', 'actor', 'in', 'musical', ';', '“', 'a', 'huge', 'honor', '.', '”']\n",
      "['Police', 'shot', 'and', 'killed', 'a', 'suspected', 'assailant', 'after', 'he', 'attacked', 'the', 'teacher', 'with', 'a', 'knife', 'in', 'a', 'suburb', 'west', 'of', 'Paris.']\n",
      "['Police', 'shot', 'and', 'killed', 'a', 'suspected', 'assailant', 'after', 'he', 'attacked', 'the', 'teacher', 'with', 'a', 'knife', 'in', 'a', 'suburb', 'west', 'of', 'Paris', '.']\n",
      "['Research', 'shows', 'that', 'when', 'somebody', 'donates', 'to', 'a', 'candidate', 'they', 'are', 'likely', 'to', 'reduce', 'their', 'contributions', 'to', 'charities.']\n",
      "['Research', 'shows', 'that', 'when', 'somebody', 'donates', 'to', 'a', 'candidate', 'they', 'are', 'likely', 'to', 'reduce', 'their', 'contributions', 'to', 'charities', '.']\n",
      "['Read', 'the', 'latest', 'headlines', 'and', 'breaking', 'news', 'today', 'from', 'The', 'Wall', 'Street', 'Journal,', 'covering', 'U.S.', 'and', 'World', 'News,', 'Markets,', 'Business,', 'Economy', 'Politics,', 'Life', 'and', 'Arts', 'and', 'Opinion']\n",
      "['Read', 'the', 'latest', 'headlines', 'and', 'breaking', 'news', 'today', 'from', 'The', 'Wall', 'Street', 'Journal', ',', 'covering', 'U.S.', 'and', 'World', 'News', ',', 'Markets', ',', 'Business', ',', 'Economy', 'Politics', ',', 'Life', 'and', 'Arts', 'and', 'Opinion']\n",
      "['The', 'arrest', 'of', 'Mexico’s', 'former', 'defense', 'minister,', 'Salvador', 'Cienfuegos,', 'by', 'U.S.', 'authorities', 'in', 'Los', 'Angeles', 'shows', 'that', 'corruption', 'is', 'Mexico’s', 'biggest', 'problem,', 'President', 'Andrés', 'Manuel', 'López', 'Obrador', 'said.']\n",
      "['The', 'arrest', 'of', 'Mexico', '’', 's', 'former', 'defense', 'minister', ',', 'Salvador', 'Cienfuegos', ',', 'by', 'U.S.', 'authorities', 'in', 'Los', 'Angeles', 'shows', 'that', 'corruption', 'is', 'Mexico', '’', 's', 'biggest', 'problem', ',', 'President', 'Andrés', 'Manuel', 'López', 'Obrador', 'said', '.']\n",
      "['Nine', 'states', 'reported', 'a', 'record', 'tally', 'of', 'new', 'coronavirus', 'cases,', 'as', 'the', 'total', 'number', 'of', 'infections', 'detected', 'in', 'the', 'U.S.', 'since', 'the', 'pandemic', 'began', 'ticked', 'above', '8', 'million.']\n",
      "['Nine', 'states', 'reported', 'a', 'record', 'tally', 'of', 'new', 'coronavirus', 'cases', ',', 'as', 'the', 'total', 'number', 'of', 'infections', 'detected', 'in', 'the', 'U.S.', 'since', 'the', 'pandemic', 'began', 'ticked', 'above', '8', 'million', '.']\n",
      "['Apple', 'announced', 'four', 'new', 'iPhone', '12', 'models', 'at', 'a', 'virtual', 'event', 'on', 'Tuesday.', 'They', 'all', 'have', '5G,', 'fresh', 'designs', 'and', 'new', 'camera', 'tricks,', 'yet', 'they', 'differ', 'in', 'key', 'ways.', 'WSJ']\n",
      "['Apple', 'announced', 'four', 'new', 'iPhone', '12', 'models', 'at', 'a', 'virtual', 'event', 'on', 'Tuesday', '.', 'They', 'all', 'have', '5G', ',', 'fresh', 'designs', 'and', 'new', 'camera', 'tricks', ',', 'yet', 'they', 'differ', 'in', 'key', 'ways', '.', 'WSJ']\n",
      "['The', 'country’s', 'center-left', 'Labour', 'Party,', 'led', 'by', 'Jacinda', 'Ardern,', 'won', 'a', 'landslide', 'victory', 'in', 'national', 'elections', 'as', 'voters', 'endorsed', 'her', 'government’s', 'efforts', 'to', 'stamp', 'out', 'the', 'coronavirus.']\n",
      "['The', 'country', '’', 's', 'center-left', 'Labour', 'Party', ',', 'led', 'by', 'Jacinda', 'Ardern', ',', 'won', 'a', 'landslide', 'victory', 'in', 'national', 'elections', 'as', 'voters', 'endorsed', 'her', 'government', '’', 's', 'efforts', 'to', 'stamp', 'out', 'the', 'coronavirus', '.']\n",
      "['As', 'utilities', 'seek', 'approval', 'to', 'invest', 'millions', 'of', 'dollars', 'in', 'upgrading', 'their', 'infrastructure', 'to', 'accommodate', 'car', 'charging,', 'their', 'efforts', 'are', 'drawing', 'opposition', 'from', 'consumer', 'advocates,', 'oil', 'companies', 'and', 'startups.']\n",
      "['As', 'utilities', 'seek', 'approval', 'to', 'invest', 'millions', 'of', 'dollars', 'in', 'upgrading', 'their', 'infrastructure', 'to', 'accommodate', 'car', 'charging', ',', 'their', 'efforts', 'are', 'drawing', 'opposition', 'from', 'consumer', 'advocates', ',', 'oil', 'companies', 'and', 'startups', '.']\n",
      "['Earn', 'an', 'exclusive', 'reward', 'by', 'referring', 'your', 'friends.', 'Equip', 'your', 'peers', 'for', 'success', 'with', 'award-winning', 'insight', 'from', 'the', 'Wall', 'Street', 'Journal.']\n",
      "['Earn', 'an', 'exclusive', 'reward', 'by', 'referring', 'your', 'friends', '.', 'Equip', 'your', 'peers', 'for', 'success', 'with', 'award-winning', 'insight', 'from', 'the', 'Wall', 'Street', 'Journal', '.']\n",
      "['Nearly', '100', 'people', 'died', 'during', 'the', 'height', 'of', 'the', 'coronavirus', 'outbreak', 'at', 'the', 'Menlo', 'Park', 'Veterans', 'Memorial', 'Home', 'in', 'April,', 'more', 'than', '10', 'times', 'the', 'number', 'in', 'a', 'typical', 'month.', 'Among', 'those', 'who', 'died', 'were', '84-year-old', 'Isabella', 'Kovacs', 'and', '86-year-old', 'Joan', 'Williams.', 'Their', 'stories', 'provide', 'a', 'window', 'into', 'what', 'went', 'wrong', 'at', 'the', 'New', 'Jersey', 'facility.', 'Photo:', 'Shari', 'Davis/Julie', 'Diaz']\n",
      "['Nearly', '100', 'people', 'died', 'during', 'the', 'height', 'of', 'the', 'coronavirus', 'outbreak', 'at', 'the', 'Menlo', 'Park', 'Veterans', 'Memorial', 'Home', 'in', 'April', ',', 'more', 'than', '10', 'times', 'the', 'number', 'in', 'a', 'typical', 'month', '.', 'Among', 'those', 'who', 'died', 'were', '84-year-old', 'Isabella', 'Kovacs', 'and', '86-year-old', 'Joan', 'Williams', '.', 'Their', 'stories', 'provide', 'a', 'window', 'into', 'what', 'went', 'wrong', 'at', 'the', 'New', 'Jersey', 'facility', '.', 'Photo', ':', 'Shari', 'Davis/Julie', 'Diaz']\n",
      "['Topic', 'pages', 'collect', 'the', 'latest,', 'breaking', 'and', 'archive', 'news,', 'photos,', 'graphics,', 'audio', 'and', 'video', 'published', 'on', 'the', 'topic', 'in', 'The', 'Wall', 'Street', 'Journal.']\n",
      "['Topic', 'pages', 'collect', 'the', 'latest', ',', 'breaking', 'and', 'archive', 'news', ',', 'photos', ',', 'graphics', ',', 'audio', 'and', 'video', 'published', 'on', 'the', 'topic', 'in', 'The', 'Wall', 'Street', 'Journal', '.']\n",
      "['By', 'some', 'estimates,', 'about', '40%', 'of', 'U.S.', 'day', 'cares', 'are', 'closed', 'after', 'enrollment', 'fell', 'off', 'in', 'the', 'spring', 'and', 'never', 'fully', 'bounced', 'back.']\n",
      "['By', 'some', 'estimates', ',', 'about', '40', '%', 'of', 'U.S.', 'day', 'cares', 'are', 'closed', 'after', 'enrollment', 'fell', 'off', 'in', 'the', 'spring', 'and', 'never', 'fully', 'bounced', 'back', '.']\n",
      "['Interpublic', 'Group', 'of', 'Cos.', 'has', 'promoted', 'Bill', 'Kolb', 'to', 'chairman', 'and', 'CEO', 'of', 'creative', 'agency', 'network', 'McCann', 'Worldgroup,', 'the', 'company', 'said', 'Thursday.', 'He', 'will', 'succeed', 'Harris', 'Diamond,', 'who', 'plans', 'to', 'retire', 'at', 'the', 'end', 'of', 'the', 'year,', 'the', 'company', 'said.']\n",
      "['Interpublic', 'Group', 'of', 'Cos.', 'has', 'promoted', 'Bill', 'Kolb', 'to', 'chairman', 'and', 'CEO', 'of', 'creative', 'agency', 'network', 'McCann', 'Worldgroup', ',', 'the', 'company', 'said', 'Thursday', '.', 'He', 'will', 'succeed', 'Harris', 'Diamond', ',', 'who', 'plans', 'to', 'retire', 'at', 'the', 'end', 'of', 'the', 'year', ',', 'the', 'company', 'said', '.']\n",
      "['The', 'shipping', 'company', 'lifted', 'its', 'full-year', 'guidance,', 'noting', 'a', 'faster-than-expected', 'rebound', 'in', 'shipping', 'volumes', 'and', 'freight', 'rates,', 'but', 'it', 'will', 'take', 'a', '$100', 'million', 'restructuring', 'charge', 'in', 'the', 'third', 'quarter', 'to', 'cover', 'the', 'costs', 'of', 'cutting', '2,000', 'jobs.']\n",
      "['The', 'shipping', 'company', 'lifted', 'its', 'full-year', 'guidance', ',', 'noting', 'a', 'faster-than-expected', 'rebound', 'in', 'shipping', 'volumes', 'and', 'freight', 'rates', ',', 'but', 'it', 'will', 'take', 'a', '$', '100', 'million', 'restructuring', 'charge', 'in', 'the', 'third', 'quarter', 'to', 'cover', 'the', 'costs', 'of', 'cutting', '2,000', 'jobs', '.']\n",
      "[]\n",
      "[]\n",
      "['President', 'Trump', 'was', 'pressed', 'on', 'his', 'handling', 'of', 'the', 'pandemic', 'during', 'a', 'contentious', 'town', 'hall,', 'while', 'Joe', 'Biden', 'was', 'asked', 'in', 'a', 'separate', 'event', 'what', 'he', 'would', 'do', 'differently.']\n",
      "['President', 'Trump', 'was', 'pressed', 'on', 'his', 'handling', 'of', 'the', 'pandemic', 'during', 'a', 'contentious', 'town', 'hall', ',', 'while', 'Joe', 'Biden', 'was', 'asked', 'in', 'a', 'separate', 'event', 'what', 'he', 'would', 'do', 'differently', '.']\n",
      "['The', 'findings', 'point', 'to', 'a', 'vulnerability', 'for', 'the', 'economy', 'in', 'the', 'months', 'ahead:', 'With', 'smaller', 'savings', 'and', 'no', 'additional', 'economic', 'relief,', 'nearly', '11', 'million', 'jobless', 'workers', 'may', 'tighten', 'spending', 'even', 'further.']\n",
      "['The', 'findings', 'point', 'to', 'a', 'vulnerability', 'for', 'the', 'economy', 'in', 'the', 'months', 'ahead', ':', 'With', 'smaller', 'savings', 'and', 'no', 'additional', 'economic', 'relief', ',', 'nearly', '11', 'million', 'jobless', 'workers', 'may', 'tighten', 'spending', 'even', 'further', '.']\n",
      "['One', 'reason', 'many', 'Northeastern', 'colleges', 'have', 'experienced', 'few', 'coronavirus', 'outbreaks', 'has', 'been', 'a', 'testing', 'operation', 'run', 'by', 'Broad', 'Institute', 'of', 'MIT', 'and', 'Harvard', 'with', '14-hour', 'turnaround', 'times', 'and', 'low-cost', 'tests.']\n",
      "['One', 'reason', 'many', 'Northeastern', 'colleges', 'have', 'experienced', 'few', 'coronavirus', 'outbreaks', 'has', 'been', 'a', 'testing', 'operation', 'run', 'by', 'Broad', 'Institute', 'of', 'MIT', 'and', 'Harvard', 'with', '14-hour', 'turnaround', 'times', 'and', 'low-cost', 'tests', '.']\n",
      "['Senators', 'have', 'a', 'chance', 'to', 'explain', 'why', 'an', 'open', 'society', 'matters.']\n",
      "['Senators', 'have', 'a', 'chance', 'to', 'explain', 'why', 'an', 'open', 'society', 'matters', '.']\n",
      "['A', 'physical', 'therapist', 'for', 'Olympians', 'shares', 'exercises', 'to', 'improve', 'core', 'and', 'shoulder', 'stability.']\n",
      "['A', 'physical', 'therapist', 'for', 'Olympians', 'shares', 'exercises', 'to', 'improve', 'core', 'and', 'shoulder', 'stability', '.']\n",
      "['As', 'wealthier', 'countries', 'buy', 'up', 'supplies', 'of', 'Western', 'drugmakers’', 'Covid-19', 'vaccines', 'that', 'are', 'still', 'in', 'development,', 'China', 'and', 'Russia', 'are', 'offering', 'their', 'fast-tracked', 'shots', 'to', 'poorer', 'nations.', 'Here’s', 'what', \"they're\", 'hoping', 'to', 'get', 'in', 'return.', 'Illustration:', 'Ksenia', 'Shaikhutdinova']\n",
      "['As', 'wealthier', 'countries', 'buy', 'up', 'supplies', 'of', 'Western', 'drugmakers', '’', 'Covid-19', 'vaccines', 'that', 'are', 'still', 'in', 'development', ',', 'China', 'and', 'Russia', 'are', 'offering', 'their', 'fast-tracked', 'shots', 'to', 'poorer', 'nations', '.', 'Here', '’', 's', 'what', 'they', \"'re\", 'hoping', 'to', 'get', 'in', 'return', '.', 'Illustration', ':', 'Ksenia', 'Shaikhutdinova']\n",
      "['The', 'appeals', 'court', 'returned', 'the', 'deadline', 'to', '8', 'p.m.', 'on', 'Election', 'Day,', 'the', 'latest', 'decision', 'in', 'a', 'wave', 'of', 'partisan', 'court', 'battles', 'over', 'voting', 'by', 'mail.']\n",
      "['The', 'appeals', 'court', 'returned', 'the', 'deadline', 'to', '8', 'p.m.', 'on', 'Election', 'Day', ',', 'the', 'latest', 'decision', 'in', 'a', 'wave', 'of', 'partisan', 'court', 'battles', 'over', 'voting', 'by', 'mail', '.']\n",
      "['The', 'South', 'offers', 'a', 'stark', 'example', 'of', 'what', 'happens', 'to', 'an', 'economy', 'when', 'measures', 'are', 'less', 'stringent:', 'lower', 'unemployment', 'rates,', 'relatively', 'strong', 'job', 'openings—but', 'a', 'bigger', 'rise', 'in', 'infections', 'and', 'deaths.']\n",
      "['The', 'South', 'offers', 'a', 'stark', 'example', 'of', 'what', 'happens', 'to', 'an', 'economy', 'when', 'measures', 'are', 'less', 'stringent', ':', 'lower', 'unemployment', 'rates', ',', 'relatively', 'strong', 'job', 'openings—but', 'a', 'bigger', 'rise', 'in', 'infections', 'and', 'deaths', '.']\n",
      "['The', 'high', 'court', 'said', 'it', 'would', 'decide', 'whether', 'President', 'Trump', 'can', 'exclude', 'illegal', 'immigrants', 'from', 'the', '2020', 'census', 'count', 'used', 'to', 'allocate', 'congressional', 'seats,', 'expediting', 'the', 'case', 'with', 'an', 'argument', 'set', 'for', 'Nov.', '30.']\n",
      "['The', 'high', 'court', 'said', 'it', 'would', 'decide', 'whether', 'President', 'Trump', 'can', 'exclude', 'illegal', 'immigrants', 'from', 'the', '2020', 'census', 'count', 'used', 'to', 'allocate', 'congressional', 'seats', ',', 'expediting', 'the', 'case', 'with', 'an', 'argument', 'set', 'for', 'Nov.', '30', '.']\n",
      "['Shaq,', 'Playboy', 'and', 'Nikola', 'are', 'all', 'on', 'the', 'blank-check', 'bandwagon,', 'part', 'of', 'a', 'boom', 'in', 'SPACs', 'that', 'is', 'unsettling,', 'writes', 'columnist', 'James', 'Mackintosh.']\n",
      "['Shaq', ',', 'Playboy', 'and', 'Nikola', 'are', 'all', 'on', 'the', 'blank-check', 'bandwagon', ',', 'part', 'of', 'a', 'boom', 'in', 'SPACs', 'that', 'is', 'unsettling', ',', 'writes', 'columnist', 'James', 'Mackintosh', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'surveys', 'a', 'group', 'of', 'more', 'than', '60', 'economists', 'on', 'more', 'than', '10', 'major', 'economic', 'indicators', 'on', 'a', 'monthly', 'basis.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'surveys', 'a', 'group', 'of', 'more', 'than', '60', 'economists', 'on', 'more', 'than', '10', 'major', 'economic', 'indicators', 'on', 'a', 'monthly', 'basis', '.']\n",
      "['The', 'soda', 'company’s', 'first', 'diet', 'cola', 'was', 'a', 'pop-culture', 'icon', 'in', 'the', '1970s', 'and', 'early', '’80s,', 'then', 'faded', 'after', 'the', 'launch', 'of', 'Diet', 'Coke.', 'Coca-Cola', 'kept', 'the', 'brand', 'going', 'for', 'decades', 'to', 'appease', 'a', 'fiercely', 'devoted', 'base.']\n",
      "['The', 'soda', 'company', '’', 's', 'first', 'diet', 'cola', 'was', 'a', 'pop-culture', 'icon', 'in', 'the', '1970s', 'and', 'early', '’', '80s', ',', 'then', 'faded', 'after', 'the', 'launch', 'of', 'Diet', 'Coke', '.', 'Coca-Cola', 'kept', 'the', 'brand', 'going', 'for', 'decades', 'to', 'appease', 'a', 'fiercely', 'devoted', 'base', '.']\n",
      "['Some', '14.1', 'million', 'people', 'tuned', 'in', 'to', 'watch', 'Joe', 'Biden', 'on', 'ABC,', 'while', '13.5', 'million', 'watched', 'President', 'Trump', 'on', 'NBC', 'platforms,', 'according', 'to', 'Nielsen', 'ratings.']\n",
      "['Some', '14.1', 'million', 'people', 'tuned', 'in', 'to', 'watch', 'Joe', 'Biden', 'on', 'ABC', ',', 'while', '13.5', 'million', 'watched', 'President', 'Trump', 'on', 'NBC', 'platforms', ',', 'according', 'to', 'Nielsen', 'ratings', '.']\n",
      "['Two', 'avid', 'travelers', 'debate', 'the', 'merits', 'and', 'madness', 'of', 'vacationing', 'overseas', 'right', 'now.', 'Plus:', 'A', 'look', 'at', '5', 'of', 'the', 'few—and', 'especially', 'tempting—countries', 'that', 'now', 'welcome', 'U.S.', 'tourists.']\n",
      "['Two', 'avid', 'travelers', 'debate', 'the', 'merits', 'and', 'madness', 'of', 'vacationing', 'overseas', 'right', 'now', '.', 'Plus', ':', 'A', 'look', 'at', '5', 'of', 'the', 'few—and', 'especially', 'tempting—countries', 'that', 'now', 'welcome', 'U.S.', 'tourists', '.']\n",
      "['Loop', 'Industries', 'denied', 'accusations', 'by', 'short', 'seller', 'Hindenburg', 'Research', 'that', 'the', 'company', 'lied', 'about', 'its', 'recyclable-plastic', 'technology.']\n",
      "['Loop', 'Industries', 'denied', 'accusations', 'by', 'short', 'seller', 'Hindenburg', 'Research', 'that', 'the', 'company', 'lied', 'about', 'its', 'recyclable-plastic', 'technology', '.']\n",
      "['After', 'focusing', 'on', 'a', 'trade', 'deal,', 'President', 'Trump', 'has', 'toughened', 'his', 'stance', 'toward', 'China,', 'raising', 'the', 'prominence', 'of', 'hard-liners', 'in', 'his', 'administration.']\n",
      "['After', 'focusing', 'on', 'a', 'trade', 'deal', ',', 'President', 'Trump', 'has', 'toughened', 'his', 'stance', 'toward', 'China', ',', 'raising', 'the', 'prominence', 'of', 'hard-liners', 'in', 'his', 'administration', '.']\n",
      "['The', 'Dow', 'and', 'S&P', '500', 'ticked', 'up', 'as', 'retail', 'sales', 'data', 'beat', 'expectations,', 'capping', 'a', 'volatile', 'week', 'on', 'Wall', 'Street.']\n",
      "['The', 'Dow', 'and', 'S', '&', 'P', '500', 'ticked', 'up', 'as', 'retail', 'sales', 'data', 'beat', 'expectations', ',', 'capping', 'a', 'volatile', 'week', 'on', 'Wall', 'Street', '.']\n",
      "[\"We're\", 'here', 'to', 'tackle', 'your', 'questions', 'about', 'money', 'amid', 'the', 'coronavirus', 'pandemic.', 'Here', \"you'll\", 'find', 'the', 'latest', 'information', 'on', 'stimulus', 'money,', 'student', 'loans,', 'mortgages,', 'unemployment,', 'and', 'taxes.']\n",
      "['We', \"'re\", 'here', 'to', 'tackle', 'your', 'questions', 'about', 'money', 'amid', 'the', 'coronavirus', 'pandemic', '.', 'Here', 'you', \"'ll\", 'find', 'the', 'latest', 'information', 'on', 'stimulus', 'money', ',', 'student', 'loans', ',', 'mortgages', ',', 'unemployment', ',', 'and', 'taxes', '.']\n",
      "['Check', 'your', 'account', 'status,', 'create', 'a', 'vacation', 'hold,', 'update', 'your', 'address,', 'renew', 'your', 'subscription,', 'report', 'a', 'missed', 'delivery', 'and', 'find', 'support', 'for', 'other', 'customer', 'service', 'issues.']\n",
      "['Check', 'your', 'account', 'status', ',', 'create', 'a', 'vacation', 'hold', ',', 'update', 'your', 'address', ',', 'renew', 'your', 'subscription', ',', 'report', 'a', 'missed', 'delivery', 'and', 'find', 'support', 'for', 'other', 'customer', 'service', 'issues', '.']\n",
      "['Chief', 'information', 'officers', 'can', 'learn', 'many', 'lessons', 'from', 'the', 'coronavirus', 'pandemic', 'when', 'it', 'comes', 'to', 'managing', 'data,', 'according', 'to', 'DJ', 'Patil,', 'a', 'former', 'U.S.', 'chief', 'data,', 'including', 'the', 'need', 'for', 'preparing', 'for', 'constant', 'change', 'and', 'broadening', 'who', 'counts', 'as', 'data', 'stakeholders.']\n",
      "['Chief', 'information', 'officers', 'can', 'learn', 'many', 'lessons', 'from', 'the', 'coronavirus', 'pandemic', 'when', 'it', 'comes', 'to', 'managing', 'data', ',', 'according', 'to', 'DJ', 'Patil', ',', 'a', 'former', 'U.S.', 'chief', 'data', ',', 'including', 'the', 'need', 'for', 'preparing', 'for', 'constant', 'change', 'and', 'broadening', 'who', 'counts', 'as', 'data', 'stakeholders', '.']\n",
      "['Our', 'columnists', 'from', 'the', 'award-winning', 'opinion', 'pages', 'at', 'The', 'Wall', 'Street', 'Journal', 'examine', 'world', 'news', 'and', 'foreign', 'affairs', 'twice', 'a', 'week.', 'Get', 'the', 'information', 'you', 'need', 'to', 'understand', 'the', 'changing', 'world,', 'and', 'its', 'impact', 'on', 'U.S.', 'and', 'national', 'interests.', 'Published', 'several', 'times', 'weekly.']\n",
      "['Our', 'columnists', 'from', 'the', 'award-winning', 'opinion', 'pages', 'at', 'The', 'Wall', 'Street', 'Journal', 'examine', 'world', 'news', 'and', 'foreign', 'affairs', 'twice', 'a', 'week', '.', 'Get', 'the', 'information', 'you', 'need', 'to', 'understand', 'the', 'changing', 'world', ',', 'and', 'its', 'impact', 'on', 'U.S.', 'and', 'national', 'interests', '.', 'Published', 'several', 'times', 'weekly', '.']\n",
      "['The', 'Big', 'Four', 'accounting', 'concern', 'reviewed', 'the', 'books', 'of', 'Wirecard,', 'Luckin', 'Coffee', 'and', 'other', 'companies', 'where', 'investors', 'lost', 'billions', 'when', 'scandals', 'emerged.', 'The', 'firm,', 'which', 'caters', 'to', 'fast-growing', 'tech', 'startups,', 'says', 'it', 'unearthed', 'some', 'of', 'the', 'problems.']\n",
      "['The', 'Big', 'Four', 'accounting', 'concern', 'reviewed', 'the', 'books', 'of', 'Wirecard', ',', 'Luckin', 'Coffee', 'and', 'other', 'companies', 'where', 'investors', 'lost', 'billions', 'when', 'scandals', 'emerged', '.', 'The', 'firm', ',', 'which', 'caters', 'to', 'fast-growing', 'tech', 'startups', ',', 'says', 'it', 'unearthed', 'some', 'of', 'the', 'problems', '.']\n",
      "['The', 'inaugural', 'Asia', 'Society', 'Triennial', 'features', 'artists', 'and', 'performers', 'who', 'have', 'often', 'been', 'overlooked', 'in', 'the', 'U.S.']\n",
      "['The', 'inaugural', 'Asia', 'Society', 'Triennial', 'features', 'artists', 'and', 'performers', 'who', 'have', 'often', 'been', 'overlooked', 'in', 'the', 'U.S', '.']\n",
      "['New', 'Jersey', 'Gov.', 'Phil', 'Murphy', 'named', 'new', 'top', 'officials', 'at', 'the', 'state’s', 'veterans', 'affairs', 'agency,', 'less', 'than', 'two', 'weeks', 'after', 'a', 'Wall', 'Street', 'Journal', 'investigation', 'showed', 'deadly', 'lapses', 'at', 'one', 'of', 'its', 'facilities.']\n",
      "['New', 'Jersey', 'Gov', '.', 'Phil', 'Murphy', 'named', 'new', 'top', 'officials', 'at', 'the', 'state', '’', 's', 'veterans', 'affairs', 'agency', ',', 'less', 'than', 'two', 'weeks', 'after', 'a', 'Wall', 'Street', 'Journal', 'investigation', 'showed', 'deadly', 'lapses', 'at', 'one', 'of', 'its', 'facilities', '.']\n",
      "['How', 'historian', 'Fred', 'Siegel', 'came', 'to', 'appreciate', 'the', 'president’s', 'defense', 'of', '‘bourgeois', 'values’', 'against', 'the', '‘clerisy.’']\n",
      "['How', 'historian', 'Fred', 'Siegel', 'came', 'to', 'appreciate', 'the', 'president', '’', 's', 'defense', 'of', '‘', 'bourgeois', 'values', '’', 'against', 'the', '‘', 'clerisy', '.', '’']\n",
      "['Among', 'them:', 'What', 'happens', 'to', 'my', 'art', 'collection', 'after', 'I', 'die?']\n",
      "['Among', 'them', ':', 'What', 'happens', 'to', 'my', 'art', 'collection', 'after', 'I', 'die', '?']\n",
      "['The', 'social-media', 'giant', 'has', 'made', 'a', 'flurry', 'of', 'new', 'rules', 'designed', 'to', 'improve', 'the', 'discourse', 'on', 'its', 'platforms.', 'When', 'users', 'report', 'content', 'that', 'breaks', 'those', 'rules,', 'a', 'test', 'by', 'The', 'Wall', 'Street', 'Journal', 'found,', 'the', 'company', 'often', 'fails', 'to', 'enforce', 'them.']\n",
      "['The', 'social-media', 'giant', 'has', 'made', 'a', 'flurry', 'of', 'new', 'rules', 'designed', 'to', 'improve', 'the', 'discourse', 'on', 'its', 'platforms', '.', 'When', 'users', 'report', 'content', 'that', 'breaks', 'those', 'rules', ',', 'a', 'test', 'by', 'The', 'Wall', 'Street', 'Journal', 'found', ',', 'the', 'company', 'often', 'fails', 'to', 'enforce', 'them', '.']\n",
      "['The', 'prospect', 'of', 'cuts', 'and', 'burns,', 'or', 'just', 'baking', 'a', 'cake', 'that', 'fails', 'to', 'rise,', 'doesn’t', 'have', 'to', 'keep', 'us', 'out', 'of', 'the', 'kitchen.']\n",
      "['The', 'prospect', 'of', 'cuts', 'and', 'burns', ',', 'or', 'just', 'baking', 'a', 'cake', 'that', 'fails', 'to', 'rise', ',', 'doesn', '’', 't', 'have', 'to', 'keep', 'us', 'out', 'of', 'the', 'kitchen', '.']\n",
      "['The', 'drug', 'giant', 'laid', 'out', 'a', 'timetable', 'for', 'reaching', 'key', 'milestones', 'in', 'the', 'development', 'of', 'its', 'Covid-19', 'vaccine', 'that', 'could', 'mean', 'the', 'shots', 'start', 'becoming', 'available', 'in', 'the', 'U.S.', 'before', 'year’s', 'end.']\n",
      "['The', 'drug', 'giant', 'laid', 'out', 'a', 'timetable', 'for', 'reaching', 'key', 'milestones', 'in', 'the', 'development', 'of', 'its', 'Covid-19', 'vaccine', 'that', 'could', 'mean', 'the', 'shots', 'start', 'becoming', 'available', 'in', 'the', 'U.S.', 'before', 'year', '’', 's', 'end', '.']\n",
      "['Read', 'Reading', '&', 'Retreating', 'on', 'The', 'Wall', 'Street', 'Journal']\n",
      "['Read', 'Reading', '&', 'Retreating', 'on', 'The', 'Wall', 'Street', 'Journal']\n",
      "['The', 'sailors,', 'who', 'were', 'evacuated,', 'were', 'serving', 'aboard', 'the', 'aircraft', 'carrier', 'that', 'earlier', 'this', 'year', 'was', 'the', 'site', 'of', 'the', 'largest', 'novel', 'coronavirus', 'outbreak', 'to', 'strike', 'the', 'U.S.', 'military.']\n",
      "['The', 'sailors', ',', 'who', 'were', 'evacuated', ',', 'were', 'serving', 'aboard', 'the', 'aircraft', 'carrier', 'that', 'earlier', 'this', 'year', 'was', 'the', 'site', 'of', 'the', 'largest', 'novel', 'coronavirus', 'outbreak', 'to', 'strike', 'the', 'U.S.', 'military', '.']\n",
      "['The', 'findings', 'add', 'to', 'debate', 'over', 'the', 'utility', 'of', 'the', 'medicine', 'developed', 'by', 'Gilead', 'Sciences', 'in', 'treating', 'the', 'new', 'coronavirus.']\n",
      "['The', 'findings', 'add', 'to', 'debate', 'over', 'the', 'utility', 'of', 'the', 'medicine', 'developed', 'by', 'Gilead', 'Sciences', 'in', 'treating', 'the', 'new', 'coronavirus', '.']\n",
      "['As', 'we', 'settle', 'into', 'working', 'and', 'learning', 'from', 'home,', 'repurposed', 'backyard', 'garden', 'sheds', 'and', 'prefab', 'outbuildings', 'offer', 'a', 'pleasant', 'soft', 'divide', 'between', 'family', 'and', 'work', 'life.']\n",
      "['As', 'we', 'settle', 'into', 'working', 'and', 'learning', 'from', 'home', ',', 'repurposed', 'backyard', 'garden', 'sheds', 'and', 'prefab', 'outbuildings', 'offer', 'a', 'pleasant', 'soft', 'divide', 'between', 'family', 'and', 'work', 'life', '.']\n",
      "['Request', 'comes', 'during', 'a', 'historic', 'fire', 'season', 'with', 'an', 'unprecedented', 'number', 'of', 'blazes', 'fueled', 'by', 'extreme', 'heat,', 'gusty', 'winds', 'and', 'dry', 'conditions.']\n",
      "['Request', 'comes', 'during', 'a', 'historic', 'fire', 'season', 'with', 'an', 'unprecedented', 'number', 'of', 'blazes', 'fueled', 'by', 'extreme', 'heat', ',', 'gusty', 'winds', 'and', 'dry', 'conditions', '.']\n",
      "['The', 'red', 'rocks', 'of', 'Moab,', 'Utah', 'were', 'woven', 'into', 'the', 'interior', 'and', 'exterior', 'design', 'of', 'this', 'modern', 'house.', 'The', 'owners', 'are', 'selling', 'the', '6½', 'acre', 'property', 'for', '$3.85', 'million.']\n",
      "['The', 'red', 'rocks', 'of', 'Moab', ',', 'Utah', 'were', 'woven', 'into', 'the', 'interior', 'and', 'exterior', 'design', 'of', 'this', 'modern', 'house', '.', 'The', 'owners', 'are', 'selling', 'the', '6½', 'acre', 'property', 'for', '$', '3.85', 'million', '.']\n",
      "['A', 'recent', 'WSJ/NBC', 'News', 'poll', 'shows', 'women', 'voters', 'favor', 'Democratic', 'nominee', 'Joe', 'Biden', 'over', 'President', 'Trump', 'by', 'a', 'large', 'margin.', 'WSJ’s', 'Gerald', 'F.', 'Seib', 'explains', 'why', \"we're\", 'likely', 'headed', 'into', 'an', 'election', 'with', 'a', 'gender', 'gap', 'of', 'historic', 'proportions.', 'Photo:', 'Lori', 'King/Associated', 'Press']\n",
      "['A', 'recent', 'WSJ/NBC', 'News', 'poll', 'shows', 'women', 'voters', 'favor', 'Democratic', 'nominee', 'Joe', 'Biden', 'over', 'President', 'Trump', 'by', 'a', 'large', 'margin', '.', 'WSJ', '’', 's', 'Gerald', 'F.', 'Seib', 'explains', 'why', 'we', \"'re\", 'likely', 'headed', 'into', 'an', 'election', 'with', 'a', 'gender', 'gap', 'of', 'historic', 'proportions', '.', 'Photo', ':', 'Lori', 'King/Associated', 'Press']\n",
      "['Commercials', 'are', 'increasingly', 'starring', 'nonprofessionals,', 'including', 'family', 'members', 'of', 'the', 'people', 'making', 'them,', 'due', 'to', 'pandemic', 'restrictions.', '‘They’re', 'the', 'most', 'available,', 'but', 'definitely', 'not', 'the', 'most', 'cooperative.’']\n",
      "['Commercials', 'are', 'increasingly', 'starring', 'nonprofessionals', ',', 'including', 'family', 'members', 'of', 'the', 'people', 'making', 'them', ',', 'due', 'to', 'pandemic', 'restrictions', '.', '‘', 'They', '’', 're', 'the', 'most', 'available', ',', 'but', 'definitely', 'not', 'the', 'most', 'cooperative', '.', '’']\n",
      "['Russian', 'President', 'Vladimir', 'Putin', 'wanted', 'to', 'extend', 'the', 'New', 'START', 'treaty,', 'but', 'the', 'Trump', 'administration', 'called', 'on', 'Moscow', 'to', 'agree', 'first', 'to', 'a', 'warhead', 'freeze.']\n",
      "['Russian', 'President', 'Vladimir', 'Putin', 'wanted', 'to', 'extend', 'the', 'New', 'START', 'treaty', ',', 'but', 'the', 'Trump', 'administration', 'called', 'on', 'Moscow', 'to', 'agree', 'first', 'to', 'a', 'warhead', 'freeze', '.']\n",
      "['The', 'White', 'House', 'and', 'Senate', 'Republicans', 'are', 'divided', 'over', 'the', 'size', 'of', 'last-minute', 'coronavirus', 'aid,', 'with', 'President', 'Trump', 'signaling', 'he', 'would', 'support', 'a', 'package', 'approaching', '$2', 'trillion,', 'narrowing', 'the', 'distance', 'with', 'House', 'Democrats.']\n",
      "['The', 'White', 'House', 'and', 'Senate', 'Republicans', 'are', 'divided', 'over', 'the', 'size', 'of', 'last-minute', 'coronavirus', 'aid', ',', 'with', 'President', 'Trump', 'signaling', 'he', 'would', 'support', 'a', 'package', 'approaching', '$', '2', 'trillion', ',', 'narrowing', 'the', 'distance', 'with', 'House', 'Democrats', '.']\n",
      "['From', 'multi-day', 'experiences', 'to', 'online', 'intimate', 'invitation-only', 'gatherings,', 'these', 'exclusive', 'events', 'are', 'where', 'top', 'executives', 'convene', 'to', 'hear', 'game-changing', 'leaders', 'and', 'senior', 'Journal', 'editors', 'discuss', 'the', 'critical', 'topics', 'making', 'headlines—', 'all', 'through', 'the', 'insightful,', 'analytical', 'lens', 'the', 'Journal', 'is', 'known', 'for.']\n",
      "['From', 'multi-day', 'experiences', 'to', 'online', 'intimate', 'invitation-only', 'gatherings', ',', 'these', 'exclusive', 'events', 'are', 'where', 'top', 'executives', 'convene', 'to', 'hear', 'game-changing', 'leaders', 'and', 'senior', 'Journal', 'editors', 'discuss', 'the', 'critical', 'topics', 'making', 'headlines—', 'all', 'through', 'the', 'insightful', ',', 'analytical', 'lens', 'the', 'Journal', 'is', 'known', 'for', '.']\n",
      "['Daily', 'new', 'coronavirus', 'infections', 'in', 'the', 'U.S.', 'surged', 'to', 'their', 'highest', 'level', 'since', 'late', 'July,', 'approaching', 'numbers', 'seen', 'during', 'the', 'outbreak’s', 'midsummer', 'peak.']\n",
      "['Daily', 'new', 'coronavirus', 'infections', 'in', 'the', 'U.S.', 'surged', 'to', 'their', 'highest', 'level', 'since', 'late', 'July', ',', 'approaching', 'numbers', 'seen', 'during', 'the', 'outbreak', '’', 's', 'midsummer', 'peak', '.']\n",
      "['And', 'why', 'a', 'Portuguese', 'scholar', 'says', 'the', 'reality', 'principle', 'is', 'liberalism’s', 'last', 'nemesis.']\n",
      "['And', 'why', 'a', 'Portuguese', 'scholar', 'says', 'the', 'reality', 'principle', 'is', 'liberalism', '’', 's', 'last', 'nemesis', '.']\n",
      "['Electronic', 'trading', 'giant', 'Citadel', 'Securities', 'sued', 'the', 'Securities', 'and', 'Exchange', 'Commission', 'over', 'the', 'agency’s', 'decision', 'to', 'approve', 'a', 'new', 'mechanism', 'for', 'trading', 'stocks', 'at', 'upstart', 'exchange', 'operator', 'IEX', 'Group.']\n",
      "['Electronic', 'trading', 'giant', 'Citadel', 'Securities', 'sued', 'the', 'Securities', 'and', 'Exchange', 'Commission', 'over', 'the', 'agency', '’', 's', 'decision', 'to', 'approve', 'a', 'new', 'mechanism', 'for', 'trading', 'stocks', 'at', 'upstart', 'exchange', 'operator', 'IEX', 'Group', '.']\n",
      "['Supply-chain', 'software', 'provider', 'E2open', 'LLC', 'plans', 'to', 'go', 'public', 'through', 'an', 'agreement', 'with', 'a', 'blank-check', 'company,', 'making', 'it', 'the', 'latest', 'in', 'a', 'stream', 'of', 'companies', 'bypassing', 'the', 'traditional', 'path', 'to', 'a', 'Wall', 'Street', 'listing.']\n",
      "['Supply-chain', 'software', 'provider', 'E2open', 'LLC', 'plans', 'to', 'go', 'public', 'through', 'an', 'agreement', 'with', 'a', 'blank-check', 'company', ',', 'making', 'it', 'the', 'latest', 'in', 'a', 'stream', 'of', 'companies', 'bypassing', 'the', 'traditional', 'path', 'to', 'a', 'Wall', 'Street', 'listing', '.']\n",
      "['A', 'surge', 'of', 'federal', 'spending', 'to', 'combat', 'the', 'coronavirus', 'and', 'cushion', 'the', 'U.S.', 'economy,', 'coupled', 'with', 'a', 'drop-off', 'in', 'federal', 'revenues', 'amid', 'widespread', 'shutdowns', 'and', 'layoffs,', 'contributed', 'to', 'the', 'widening', 'deficit.']\n",
      "['A', 'surge', 'of', 'federal', 'spending', 'to', 'combat', 'the', 'coronavirus', 'and', 'cushion', 'the', 'U.S.', 'economy', ',', 'coupled', 'with', 'a', 'drop-off', 'in', 'federal', 'revenues', 'amid', 'widespread', 'shutdowns', 'and', 'layoffs', ',', 'contributed', 'to', 'the', 'widening', 'deficit', '.']\n",
      "['CEOs', 'of', 'Facebook,', 'Twitter', 'and', 'YouTube-parent', 'Alphabet', 'Inc.', 'will', 'appear', 'before', 'a', 'Senate', 'committee', 'on', 'Oct.', '28', 'to', 'face', 'questioning', 'about', 'their', 'policies', 'for', 'moderating', 'content', 'on', 'their', 'internet', 'platforms.']\n",
      "['CEOs', 'of', 'Facebook', ',', 'Twitter', 'and', 'YouTube-parent', 'Alphabet', 'Inc.', 'will', 'appear', 'before', 'a', 'Senate', 'committee', 'on', 'Oct.', '28', 'to', 'face', 'questioning', 'about', 'their', 'policies', 'for', 'moderating', 'content', 'on', 'their', 'internet', 'platforms', '.']\n",
      "['Even', 'when', 'the', 'world', 'returns', 'to', '“normal,”', 'the', 'legacy', 'of', 'Covid-19', 'will', 'transform', 'everything', 'from', 'wages', 'and', 'health', 'care', 'to', 'political', 'attitudes', 'and', 'global', 'supply', 'chains.']\n",
      "['Even', 'when', 'the', 'world', 'returns', 'to', '“', 'normal', ',', '”', 'the', 'legacy', 'of', 'Covid-19', 'will', 'transform', 'everything', 'from', 'wages', 'and', 'health', 'care', 'to', 'political', 'attitudes', 'and', 'global', 'supply', 'chains', '.']\n",
      "['Stocks', 'are', 'booming', 'while', 'companies', 'shed', 'millions', 'of', 'workers', 'from', 'payrolls.', 'WSJ', 'explains', 'why', 'the', 'stock', 'market', 'seems', 'disconnected', 'from', 'economic', 'reality', 'in', 'the', 'U.S.', 'Photo', 'Illustration', 'by', 'Carlos', 'Waters/WSJ']\n",
      "['Stocks', 'are', 'booming', 'while', 'companies', 'shed', 'millions', 'of', 'workers', 'from', 'payrolls', '.', 'WSJ', 'explains', 'why', 'the', 'stock', 'market', 'seems', 'disconnected', 'from', 'economic', 'reality', 'in', 'the', 'U.S.', 'Photo', 'Illustration', 'by', 'Carlos', 'Waters/WSJ']\n",
      "['Organic', 'revenue', 'fell', '5.6%', 'in', 'the', 'third', 'quarter,', 'less', 'than', 'the', '13%', 'drop', 'in', 'the', 'second', 'quarter.', 'Covid-19', 'could', 'make', 'the', 'fourth', 'quarter', 'very', 'difficult,', 'CEO', 'Arthur', 'Sadoun', 'says.']\n",
      "['Organic', 'revenue', 'fell', '5.6', '%', 'in', 'the', 'third', 'quarter', ',', 'less', 'than', 'the', '13', '%', 'drop', 'in', 'the', 'second', 'quarter', '.', 'Covid-19', 'could', 'make', 'the', 'fourth', 'quarter', 'very', 'difficult', ',', 'CEO', 'Arthur', 'Sadoun', 'says', '.']\n",
      "['Settlements', 'between', 'Brazil’s', 'J&F', 'Investimentos', 'and', 'U.S.', 'authorities', 'to', 'resolve', 'charges', 'arising', 'from', 'an', 'alleged', 'bribery', 'scheme', 'illustrate', 'the', 'importance', 'of', 'maintaining', 'anticorruption', 'programs', 'and', 'making', 'sure', 'top', 'executives', 'undergo', 'compliance', 'training.']\n",
      "['Settlements', 'between', 'Brazil', '’', 's', 'J', '&', 'F', 'Investimentos', 'and', 'U.S.', 'authorities', 'to', 'resolve', 'charges', 'arising', 'from', 'an', 'alleged', 'bribery', 'scheme', 'illustrate', 'the', 'importance', 'of', 'maintaining', 'anticorruption', 'programs', 'and', 'making', 'sure', 'top', 'executives', 'undergo', 'compliance', 'training', '.']\n",
      "['As', 'U.S.', 'jobless', 'claims', 'swell', 'during', 'the', 'pandemic,', 'authorities', 'are', 'urging', 'financial', 'institutions', 'to', 'watch', 'for', 'schemes', 'involving', 'unemployment', 'insurance', 'payments.']\n",
      "['As', 'U.S.', 'jobless', 'claims', 'swell', 'during', 'the', 'pandemic', ',', 'authorities', 'are', 'urging', 'financial', 'institutions', 'to', 'watch', 'for', 'schemes', 'involving', 'unemployment', 'insurance', 'payments', '.']\n",
      "['Finance', 'executives', 'are', 'weighing', 'the', 'potential', 'impact', 'of', 'higher', 'taxes', 'on', 'their', 'companies’', 'cash', 'flows,', 'debt', 'and', 'other', 'core', 'items', 'on', 'the', 'balance', 'sheet', 'ahead', 'of', 'the', 'Nov.', '3', 'U.S.', 'elections.']\n",
      "['Finance', 'executives', 'are', 'weighing', 'the', 'potential', 'impact', 'of', 'higher', 'taxes', 'on', 'their', 'companies', '’', 'cash', 'flows', ',', 'debt', 'and', 'other', 'core', 'items', 'on', 'the', 'balance', 'sheet', 'ahead', 'of', 'the', 'Nov.', '3', 'U.S.', 'elections', '.']\n",
      "['Making', 'more', 'money', 'can', 'mean', 'much', 'higher', 'Medicare', 'premiums', 'for', 'some', 'people.']\n",
      "['Making', 'more', 'money', 'can', 'mean', 'much', 'higher', 'Medicare', 'premiums', 'for', 'some', 'people', '.']\n",
      "['A', 'comprehensive', 'list', 'of', 'companies', 'available', 'on', 'stock', 'exchanges', 'that', 'can', 'be', 'browsed', 'alphabetically,', 'by', 'sector,', 'or', 'by', 'country.']\n",
      "['A', 'comprehensive', 'list', 'of', 'companies', 'available', 'on', 'stock', 'exchanges', 'that', 'can', 'be', 'browsed', 'alphabetically', ',', 'by', 'sector', ',', 'or', 'by', 'country', '.']\n",
      "['Gen.', 'Salvador', 'Cienfuegos', 'received', 'bribes', 'and', 'passed', 'information', 'on', 'investigations', 'to', 'crime', 'bosses,', 'U.S.', 'prosecutors', 'allege.']\n",
      "['Gen.', 'Salvador', 'Cienfuegos', 'received', 'bribes', 'and', 'passed', 'information', 'on', 'investigations', 'to', 'crime', 'bosses', ',', 'U.S.', 'prosecutors', 'allege', '.']\n",
      "['The', 'prioritizing', 'put', 'underserved', 'businesses,', 'including', 'those', 'owned', 'by', 'women', 'and', 'minorities,', 'at', 'a', 'disadvantage', 'when', 'applying', 'for', 'a', '$670', 'billion', 'small-business', 'coronavirus', 'aid', 'program,', 'a', 'Democratic-led', 'congressional', 'oversight', 'subcommittee’s', 'report', 'says.']\n",
      "['The', 'prioritizing', 'put', 'underserved', 'businesses', ',', 'including', 'those', 'owned', 'by', 'women', 'and', 'minorities', ',', 'at', 'a', 'disadvantage', 'when', 'applying', 'for', 'a', '$', '670', 'billion', 'small-business', 'coronavirus', 'aid', 'program', ',', 'a', 'Democratic-led', 'congressional', 'oversight', 'subcommittee', '’', 's', 'report', 'says', '.']\n",
      "['Stetson', 'Bennett', 'IV', 'is', 'the', 'unlikely', 'starter', 'at', 'quarterback', 'for', 'No.', '3', 'Georgia', 'against', 'No.', '2', 'Alabama.']\n",
      "['Stetson', 'Bennett', 'IV', 'is', 'the', 'unlikely', 'starter', 'at', 'quarterback', 'for', 'No', '.', '3', 'Georgia', 'against', 'No', '.', '2', 'Alabama', '.']\n",
      "['A', '104-seat', 'Upper', 'Chamber', 'is', 'on', 'the', 'agenda', 'if', 'Democrats', 'sweep', 'the', 'election.']\n",
      "['A', '104-seat', 'Upper', 'Chamber', 'is', 'on', 'the', 'agenda', 'if', 'Democrats', 'sweep', 'the', 'election', '.']\n",
      "['Some', 'of', 'the', 'strategies', 'stem', 'from', 'the', 'economic-relief', 'legislation', 'passed', 'earlier', 'in', 'the', 'year.']\n",
      "['Some', 'of', 'the', 'strategies', 'stem', 'from', 'the', 'economic-relief', 'legislation', 'passed', 'earlier', 'in', 'the', 'year', '.']\n",
      "['Corrections', '&', 'Amplifications', 'for', 'the', 'edition', 'of', 'Oct.', '17-18,', '2020.']\n",
      "['Corrections', '&', 'Amplifications', 'for', 'the', 'edition', 'of', 'Oct.', '17-18', ',', '2020', '.']\n",
      "['Tactics', 'Google', 'and', 'other', 'large', 'online-ad', 'players', 'use', 'in', 'digital', 'ad', 'auctions', 'violate', 'EU', 'privacy', 'law,', 'investigators', 'for', 'Belgium’s', 'privacy', 'regulator', 'wrote', 'in', 'an', 'internal', 'report,', 'a', 'preliminary', 'finding', 'with', 'implications', 'across', 'the', 'continent.']\n",
      "['Tactics', 'Google', 'and', 'other', 'large', 'online-ad', 'players', 'use', 'in', 'digital', 'ad', 'auctions', 'violate', 'EU', 'privacy', 'law', ',', 'investigators', 'for', 'Belgium', '’', 's', 'privacy', 'regulator', 'wrote', 'in', 'an', 'internal', 'report', ',', 'a', 'preliminary', 'finding', 'with', 'implications', 'across', 'the', 'continent', '.']\n",
      "['European', 'privacy', 'regulators', 'are', 'unlikely', 'to', 'issue', 'a', 'final', 'ruling', 'on', 'Twitter’s', 'handling', 'of', 'a', '2019', 'data', 'breach', 'before', 'the', 'end', 'of', 'the', 'year,', 'Ireland’s', 'data', 'commissioner', 'said.']\n",
      "['European', 'privacy', 'regulators', 'are', 'unlikely', 'to', 'issue', 'a', 'final', 'ruling', 'on', 'Twitter', '’', 's', 'handling', 'of', 'a', '2019', 'data', 'breach', 'before', 'the', 'end', 'of', 'the', 'year', ',', 'Ireland', '’', 's', 'data', 'commissioner', 'said', '.']\n",
      "['The', 'new', 'offering', 'will', 'combine', 'ServiceNow’s', 'IT', 'Service', 'Management', 'and', 'IT', 'Operations', 'Management', 'systems', 'with', 'IBM’s', 'recently', 'introduced', 'Watson', 'AIOps.']\n",
      "['The', 'new', 'offering', 'will', 'combine', 'ServiceNow', '’', 's', 'IT', 'Service', 'Management', 'and', 'IT', 'Operations', 'Management', 'systems', 'with', 'IBM', '’', 's', 'recently', 'introduced', 'Watson', 'AIOps', '.']\n",
      "['From', 'unrest', 'in', 'Belarus', 'and', 'Kyrgyzstan', 'to', 'the', 'Armenia-Azerbaijan', 'conflict,', 'WSJ', 'explores', 'how', 'the', 'crises', 'unfolding', 'in', 'Russia’s', 'backyard', 'mark', 'a', 'turning', 'point', 'in', 'Vladimir', 'Putin’s', 'rule', 'and', 'put', 'him', 'at', 'risk', 'of', 'losing', 'influence', 'in', 'the', 'former', 'Soviet', 'Union.', 'Video/Photo', 'Composite:', 'Michelle', 'Inez', 'Simon']\n",
      "['From', 'unrest', 'in', 'Belarus', 'and', 'Kyrgyzstan', 'to', 'the', 'Armenia-Azerbaijan', 'conflict', ',', 'WSJ', 'explores', 'how', 'the', 'crises', 'unfolding', 'in', 'Russia', '’', 's', 'backyard', 'mark', 'a', 'turning', 'point', 'in', 'Vladimir', 'Putin', '’', 's', 'rule', 'and', 'put', 'him', 'at', 'risk', 'of', 'losing', 'influence', 'in', 'the', 'former', 'Soviet', 'Union', '.', 'Video/Photo', 'Composite', ':', 'Michelle', 'Inez', 'Simon']\n",
      "['Facebook’s', 'CEO', 'long', 'left', 'politics', 'and', 'policy', 'to', 'deputies.', 'No', 'more.', 'As', 'the', 'company’s', 'influence', 'has', 'grown,', 'and', 'with', 'it', 'controversies,', 'political', 'acumen', 'has', 'become', 'an', 'essential', 'tool.']\n",
      "['Facebook', '’', 's', 'CEO', 'long', 'left', 'politics', 'and', 'policy', 'to', 'deputies', '.', 'No', 'more', '.', 'As', 'the', 'company', '’', 's', 'influence', 'has', 'grown', ',', 'and', 'with', 'it', 'controversies', ',', 'political', 'acumen', 'has', 'become', 'an', 'essential', 'tool', '.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'offers', 'discounted', 'membership', 'options', 'for', 'students.', 'Explore', 'the', 'tools', 'you', 'need', 'to', 'turn', 'what', 'you', 'love', 'into', 'a', 'career.']\n",
      "['The', 'Wall', 'Street', 'Journal', 'offers', 'discounted', 'membership', 'options', 'for', 'students', '.', 'Explore', 'the', 'tools', 'you', 'need', 'to', 'turn', 'what', 'you', 'love', 'into', 'a', 'career', '.']\n",
      "['When', 'the', 'New', 'York', 'Post', 'published', 'articles', 'based', 'on', 'email', 'exchanges', 'with', 'Hunter', 'Biden,', 'social-media', 'companies', 'saw', 'the', 'situation', 'as', 'one', 'they', 'spent', 'years', 'preparing', 'for.', 'Their', 'actions', 'drew', 'a', 'mixture', 'of', 'support', 'and', 'criticism.']\n",
      "['When', 'the', 'New', 'York', 'Post', 'published', 'articles', 'based', 'on', 'email', 'exchanges', 'with', 'Hunter', 'Biden', ',', 'social-media', 'companies', 'saw', 'the', 'situation', 'as', 'one', 'they', 'spent', 'years', 'preparing', 'for', '.', 'Their', 'actions', 'drew', 'a', 'mixture', 'of', 'support', 'and', 'criticism', '.']\n",
      "['Technology', 'companies', 'are', 'set', 'to', 'end', 'the', 'year', 'with', 'their', 'greatest', 'share', 'of', 'the', 'stock', 'market', 'ever,', 'topping', 'a', 'dot-com', 'era', 'peak', 'in', 'the', 'latest', 'illustration', 'of', 'their', 'growing', 'influence', 'on', 'global', 'consumers.']\n",
      "['Technology', 'companies', 'are', 'set', 'to', 'end', 'the', 'year', 'with', 'their', 'greatest', 'share', 'of', 'the', 'stock', 'market', 'ever', ',', 'topping', 'a', 'dot-com', 'era', 'peak', 'in', 'the', 'latest', 'illustration', 'of', 'their', 'growing', 'influence', 'on', 'global', 'consumers', '.']\n",
      "['American', 'shoppers', 'boosted', 'their', 'spending', 'on', 'vehicles,', 'clothing', 'and', 'many', 'other', 'goods,', 'a', 'bright', 'spot', 'amid', 'signs', 'the', 'economic', 'recovery', 'remains', 'fragile.']\n",
      "['American', 'shoppers', 'boosted', 'their', 'spending', 'on', 'vehicles', ',', 'clothing', 'and', 'many', 'other', 'goods', ',', 'a', 'bright', 'spot', 'amid', 'signs', 'the', 'economic', 'recovery', 'remains', 'fragile', '.']\n",
      "['The', 'economic', 'outlook', 'has', 'worsened', 'since', 'the', 'credit-rating', 'firm', 'downgraded', 'the', 'country’s', 'sovereign', 'debt', 'in', 'September', '2017.']\n",
      "['The', 'economic', 'outlook', 'has', 'worsened', 'since', 'the', 'credit-rating', 'firm', 'downgraded', 'the', 'country', '’', 's', 'sovereign', 'debt', 'in', 'September', '2017', '.']\n"
     ]
    }
   ],
   "source": [
    "articles, df, dt = get_articles(['wsj.com'])\n",
    "\n",
    "for a in articles:\n",
    "    print(_word_tokenize(a[\"description\"]))    \n",
    "    print(_word_tokenize_NLTK(a[\"description\"]))\n",
    "\n",
    "\n",
    "#for a in articles:\n",
    "#    print(_sentence_tokenize(a[\"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "healthy-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk standardization\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return _sentence_tokenize(text)\n",
    "\n",
    "# Expects a list from sentence tokenizing\n",
    "def word_list_tokenize(sentenceList):\n",
    "    tokenized = []\n",
    "    for bulk in sentenceList:\n",
    "        for w in _word_tokenize(bulk):\n",
    "            tokenized.append(w)\n",
    "    return tokenized\n",
    "\n",
    "def stemming(textList):\n",
    "    stemmedList = []\n",
    "    for w in textList:\n",
    "        stemmedList.append(_stem(w))\n",
    "    return stemmedList\n",
    "\n",
    "def lemmatization(stemmedList):\n",
    "    lemmaList = []\n",
    "    for w in stemmedList:\n",
    "        lemmaList.append(_lemmatize(w))\n",
    "    return lemmaList\n",
    "\n",
    "def remove_stopwords(wordList):\n",
    "    returnList = []\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    for w in wordList:\n",
    "        if w not in stopWords:  \n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def remove_punctuations(wordList):\n",
    "    punctuations = \".?:!,;‘-’|\"\n",
    "    returnList = []\n",
    "    for w in wordList:\n",
    "        if w not in punctuations:\n",
    "            returnList.append(w)\n",
    "    return returnList\n",
    "\n",
    "def standardize(text):\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    #stemmed = stemming(wordTokenized)\n",
    "    lemma = lemmatization(wordTokenized)\n",
    "    noStopwords = remove_stopwords(lemma)\n",
    "    noPunctuations = remove_punctuations(noStopwords)\n",
    "    return noPunctuations\n",
    "\n",
    "\"\"\" Standardize the model\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "same list as in the input with additional attributes:\n",
    "    Example: {\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "\"\"\"\n",
    "def standardizeList(articles):\n",
    "    for article in articles:\n",
    "        article[\"title_standardized\"] = standardize(article[\"title\"])\n",
    "        #article[\"title_entities\"] = entity_extraction(article[\"title\"])\n",
    "        article[\"description_standardized\"] = standardize(article[\"description\"])\n",
    "        #article[\"description_entities\"] = entity_extraction(article[\"description\"])\n",
    "    return articles\n",
    "\n",
    "articles, dateFrom, dateTo = get_articles(['wsj.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "confused-circuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pfizer',\n",
       " 'is',\n",
       " 'now',\n",
       " 'producing',\n",
       " 'vaccination',\n",
       " 'against',\n",
       " 'Covid-19',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'text',\n",
       " \"We're\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'competition',\n",
       " 'In',\n",
       " \"tonight's\",\n",
       " 'edition:',\n",
       " 'Amnesty',\n",
       " 'International',\n",
       " 'says',\n",
       " 'the',\n",
       " 'Nigerian',\n",
       " 'army',\n",
       " 'ignored',\n",
       " 'repeated',\n",
       " 'warnings',\n",
       " 'of',\n",
       " 'an',\n",
       " 'attack',\n",
       " 'on',\n",
       " 'Dapchi',\n",
       " 'town;',\n",
       " 'as',\n",
       " 'central',\n",
       " 'Mali',\n",
       " 'increasingly',\n",
       " 'becomes',\n",
       " 'a',\n",
       " 'target',\n",
       " 'for',\n",
       " 'jihadist',\n",
       " 'attacks,',\n",
       " 'frustrate.',\n",
       " 'INTERNATIONAL',\n",
       " 'PAPERS',\n",
       " '-',\n",
       " 'Thursday,',\n",
       " 'March',\n",
       " '22,',\n",
       " '2018:',\n",
       " 'Mark',\n",
       " \"Zuckerberg's\",\n",
       " 'slow',\n",
       " 'apology',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Facebook',\n",
       " 'data',\n",
       " 'scandal',\n",
       " 'is',\n",
       " 'getting',\n",
       " 'few',\n",
       " 'likes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'press.',\n",
       " 'An',\n",
       " '\"inevitable',\n",
       " 'mess,\"',\n",
       " 'says',\n",
       " 'one',\n",
       " 'Guardian',\n",
       " 'columnist,',\n",
       " 'who...',\n",
       " 'Spoof',\n",
       " \"children's\",\n",
       " 'book',\n",
       " 'about',\n",
       " 'Mike',\n",
       " \"Pence's\",\n",
       " 'gay',\n",
       " 'bunny',\n",
       " 'tops',\n",
       " 'Amazon',\n",
       " 'bestsellers',\n",
       " '-',\n",
       " 'In',\n",
       " 'the',\n",
       " 'press',\n",
       " 'May',\n",
       " '1968:',\n",
       " \"'It\",\n",
       " 'was',\n",
       " 'a',\n",
       " 'revolt',\n",
       " 'against',\n",
       " \"capitalism'\",\n",
       " '-',\n",
       " 'In',\n",
       " 'the',\n",
       " 'press']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test standardization\n",
    "\n",
    "def get_test_cases():\n",
    "    return [\"Pfizer is now producing vaccination against Covid-19\",\n",
    "           \"This is a text\",\n",
    "           \"We're going to win the competition\",\n",
    "           \"In tonight's edition: Amnesty International says the Nigerian army ignored repeated warnings of an attack on Dapchi town; as central Mali increasingly becomes a target for jihadist attacks, frustrate.\",\n",
    "           \"INTERNATIONAL PAPERS - Thursday, March 22, 2018: Mark Zuckerberg's slow apology for the Facebook data scandal is getting few likes in the press. An \\\"inevitable mess,\\\" says one Guardian columnist, who...\",\n",
    "           \"Spoof children's book about Mike Pence's gay bunny tops Amazon bestsellers - In the press\",\n",
    "           \"May 1968: 'It was a revolt against capitalism' - In the press\"]\n",
    "\n",
    "def TEST_word_list_tokenize(testCases):\n",
    "    return word_list_tokenize(testCases)\n",
    "    \n",
    "TEST_word_list_tokenize(get_test_cases())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "exterior-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Google']\n"
     ]
    }
   ],
   "source": [
    "# Entity extraction\n",
    "from itertools import islice\n",
    "\n",
    "def entity_extraction(text):\n",
    "    extracted = []\n",
    "    sentTokenized = sentence_tokenize(text)\n",
    "    wordTokenized = word_list_tokenize(sentTokenized)\n",
    "    \n",
    "    # Tagging each word\n",
    "    tagged = _speech_tag(wordTokenized)\n",
    "    chunk = _named_entities_chunk(tagged)\n",
    "\n",
    "    for c in chunk:\n",
    "        if type(c) == Tree:\n",
    "            newItems = \" \".join([token for token, pos in c.leaves()])\n",
    "            if newItems not in extracted:\n",
    "                extracted.append(newItems)\n",
    "    return extracted\n",
    "\n",
    "def count_elements(elements):\n",
    "    _dict = {}\n",
    "    if(len(elements) > 0):\n",
    "        for words in elements:\n",
    "            for w in words:\n",
    "                try:\n",
    "                    _dict[w] += 1\n",
    "                except:\n",
    "                    _dict[w] = 1    \n",
    "    return _dict\n",
    "\n",
    "def sort_dictionary_desc(_dict):\n",
    "    return sorted(_dict.items(), key=lambda x: x[1], reverse=True)   \n",
    "\n",
    "def filter_named_properties(elements):\n",
    "    filtered = []\n",
    "    _sorted = sort_dictionary_desc(elements)\n",
    "    i = 0\n",
    "    for w in _sorted:\n",
    "        filtered.append(w)\n",
    "        i += 1\n",
    "        if(i >= _namedEntities):\n",
    "            return filtered\n",
    "    return filtered\n",
    "    \n",
    "def get_named_properties(positive):\n",
    "    entities = []\n",
    "    for article in positive:\n",
    "        entities.append(entity_extraction(article[\"title\"]))\n",
    "        entities.append(entity_extraction(article[\"description\"]))    \n",
    "    return count_elements(entities)\n",
    "\n",
    "def get_most_popular_named_properties(positive):\n",
    "    namedProperties = get_named_properties(positive)\n",
    "    return filter_named_properties(namedProperties)\n",
    "\n",
    "print(entity_extraction(\"Apple is better than Google and Microsoft. Apple is also a fruit.\"))\n",
    "#a = get_most_popular_named_properties([{\"title\": \"Apple has some great products Pfizer, Tesla\", \"description\": \"Google is a good company and so is Apple\"}])\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "documentary-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set if we want maximum number of samples, used for debugging\n",
    "_samplesThreshold = 0\n",
    "\n",
    "def array_similarity(arr1, arr2):\n",
    "    for a1 in arr1:\n",
    "        for a2 in arr2:\n",
    "            if(a1 == a2):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def create_testing_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i in samples]\n",
    "\n",
    "def create_training_data(samples, articles):\n",
    "    return [articles[i] for i in range(0, len(articles)) if i not in samples]\n",
    "\n",
    "def create_testing_and_training_data(articles):\n",
    "    # The threshold cannot be higher than the actual data\n",
    "    if(_testThreshold == 0 or len(articles) < _testThreshold):\n",
    "        testThreshold = len(articles)\n",
    "    else:\n",
    "        testThreshold = _testThreshold\n",
    "\n",
    "    #samples = random.sample(range(0,len(articles)), testThreshold)\n",
    "    \n",
    "    train, test = train_test_split(articles, test_size=0.2)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "# Positive (articles that include '_filter' in title or description) will have index = 0\n",
    "# Negative will have index = 1\n",
    "def split_to_positive_and_negative(articles):\n",
    "    positiveResults = []\n",
    "    negativeResults = []\n",
    "    processedIndexes = []\n",
    "    \n",
    "    for a in articles:\n",
    "        if(array_similarity(a[sections_to_analyze()[0]], _filter) == True):\n",
    "            positiveResults.append(a)\n",
    "        elif(array_similarity(a[sections_to_analyze()[1]], _filter) == True):\n",
    "            positiveResults.append(a)\n",
    "        else:\n",
    "            negativeResults.append(a)\n",
    "\n",
    "        if(_samplesThreshold > 0):\n",
    "            if(len(positiveResults) > _samplesThreshold):\n",
    "                break;\n",
    "\n",
    "    return positiveResults, negativeResults\n",
    "\n",
    "\n",
    "\"\"\" Convert to object for classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "classified: list\n",
    "    Example: {  'id': 'fb5e74aaa23103c9e97af27fee1a6be3'\n",
    "                'domain': 'aljazeera.com'\n",
    "                'title': ...\n",
    "                'description': ...\n",
    "                'title_standardized': ['pfizer', 'covid-19'..]\n",
    "                'description_standardized': ['report', 'news'..]\n",
    "             }\n",
    "\n",
    "Returns\n",
    "----------------\n",
    "classified: object\n",
    "    Example: {  \"testing\":  {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}}},\n",
    "                \"training\": {\"positive\": [{article1},{article2}]},\n",
    "                            {\"negative\": [{article1},{article2}]}\n",
    "             }\n",
    "\"\"\"\n",
    "def convert_to_classification_object(articles):\n",
    "    testAndTrain = create_testing_and_training_data(articles)\n",
    "    \n",
    "    if(_debug):\n",
    "        print(\"All test:\", len(testAndTrain[0]))\n",
    "        print(\"All training: \", len(testAndTrain[1]))\n",
    "        print(\"==\")\n",
    "\n",
    "    test = split_to_positive_and_negative(testAndTrain[0])\n",
    "    train = split_to_positive_and_negative(testAndTrain[1])\n",
    "\n",
    "    if(_debug):\n",
    "        print(\"Test, positive: \", len(test[0]))\n",
    "        print(\"Test, negative: \",len(test[1]))\n",
    "        print(\"Train, positive: \", len(train[0]))\n",
    "        print(\"Train, negative: \", len(train[1]))\n",
    "\n",
    "    testing = {\"positive\":{}, \"negative\":{}}\n",
    "    testing[\"positive\"] = test[0]\n",
    "    testing[\"negative\"] = test[1]\n",
    "\n",
    "    training = {\"positive\":{}, \"negative\":{}}\n",
    "    training[\"positive\"] = train[0]\n",
    "    training[\"negative\"] = train[1]\n",
    "\n",
    "    classified = {\"testing\": {}, \"training\": {}}\n",
    "    classified[\"testing\"] = testing\n",
    "    classified[\"training\"] = training\n",
    "    \n",
    "    return classified\n",
    "\n",
    "def count_frequencies(data, _class, freq):\n",
    "    for word in data:\n",
    "        try:\n",
    "            freq[_class][word] += 1\n",
    "        except:\n",
    "            freq[_class][word] = 1\n",
    "    return freq\n",
    "\n",
    "def count_total_frequencies(frequencies):\n",
    "    count = 0\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            count += frequencies[_class][w]\n",
    "    return count\n",
    "\n",
    "# Input: Frequency object (described below)\n",
    "# Output: {'positive': {'word':'likelihood', 'word2':'likelihood'}, 'negative': {...} }\n",
    "def calculate_likelihood(frequencies):\n",
    "    p_w = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in frequencies:\n",
    "        for w in frequencies[_class]:\n",
    "            p_w[_class][w] = float(frequencies[_class][w]) / float(len(frequencies[_class]))\n",
    "            #p_w[_class][w] = float(frequencies[_class][w]) / count_total_frequencies(frequencies)\n",
    "    return p_w\n",
    "\n",
    "\"\"\" Calculate frequencies from the classification\n",
    "\n",
    "Parameters\n",
    "----------------\n",
    "Returns\n",
    "----------------\n",
    "frequencies: object\n",
    "    Example: {  \"positive\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "                \"negative\": {\"word\", frequency},{\"word\", frequency}...,\n",
    "             }\n",
    "\"\"\"\n",
    "def word_counter(trainingData):\n",
    "    frequencies = {\"positive\": {}, \"negative\": {}}\n",
    "    for _class in trainingData:\n",
    "        for article in trainingData[_class]:\n",
    "            for section in sections_to_analyze():\n",
    "                count_frequencies(article[section], _class, frequencies)\n",
    "    return frequencies\n",
    "\n",
    "# Gets the prior probability of P(type1)\n",
    "def calculate_prior_propabilities(frequencies, type1, type2):\n",
    "    return float(len(frequencies[type1])) / float(len(frequencies[type1]) + len(frequencies[type2]))\n",
    "\n",
    "def add_to_predictiveValues(predictiveValues, _type):\n",
    "    try:\n",
    "        predictiveValues[_type] += 1\n",
    "    except:\n",
    "        predictiveValues[_type] = 1\n",
    "\n",
    "def find_predictive_parameter(testClass, articleWeight):\n",
    "    pWeight = articleWeight[\"positive\"]\n",
    "    nWeight = articleWeight[\"negative\"]\n",
    "\n",
    "    if(pWeight >= nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"TP\"\n",
    "        else:\n",
    "            return \"FP\"\n",
    "\n",
    "    if(pWeight < nWeight):\n",
    "        if(testClass == \"positive\"):\n",
    "            return \"FN\"\n",
    "        if(testClass == \"negative\"):\n",
    "            return \"TN\"\n",
    "\n",
    "def calculate_weigh_by_article(article, trainingDataLikelihood):\n",
    "    articleWeight = {}\n",
    "    for _trainingClass in trainingDataLikelihood:\n",
    "        for section in sections_to_analyze():\n",
    "            weight = 1\n",
    "            for word in article[section]:\n",
    "                try:\n",
    "                    weight *= trainingDataLikelihood[_trainingClass][word]\n",
    "                except:\n",
    "                    weight *= 0.00001\n",
    "        articleWeight[_trainingClass] = weight\n",
    "\n",
    "    return articleWeight #find_predictive_parameter(_class, combinedWeight)\n",
    "\n",
    "def calculate_accuracy(TP, FP, FN, TN):\n",
    "    numerator = TP + TN\n",
    "    denominator = TP + TN + FP + FN\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def calculate_positive_vs_negative(articlesWeight):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    for article in articlesWeight:\n",
    "        if(article['positive'] >= article['negative']):\n",
    "            countPositive += 1\n",
    "        else:\n",
    "            countNegative += 1\n",
    "    \n",
    "    return countPositive, countNegative\n",
    "\n",
    "def display_classification_results(accuracy, positive, negative, entities, displayInfo):\n",
    "    if(displayInfo is None or displayInfo == \"\"):\n",
    "        dateFrom = \"\"\n",
    "        dateTo = \"\"\n",
    "        domains = \"\"\n",
    "    else:\n",
    "        dateFrom = displayInfo[\"dateFrom\"]\n",
    "        dateTo = displayInfo[\"dateTo\"]\n",
    "        domains = displayInfo[\"domains\"]\n",
    "\n",
    "    displayAccuracy = \"{:.2f}\".format(accuracy * 100)\n",
    "    displayPositive = \"{:.2f}\".format(float(positive)/(float(negative)+float(positive)) * 100)\n",
    "\n",
    "    print(\"========\")\n",
    "    print(f\"Finished processing articles:\")\n",
    "    print(f\"DateFrom: {dateFrom}\")\n",
    "    print(f\"DateTo: {dateTo}\")\n",
    "    print(f\"Domains: {domains}\")\n",
    "    print(f\"Accuracy: {displayAccuracy} %\")\n",
    "    print(f\"Portion of positive: {displayPositive} % \")\n",
    "    print(f\"{_namedEntities} most popular named entities: {entities}\")\n",
    "    print(\"========\")\n",
    "\n",
    "'''\n",
    "testingData:{'positive': [  'id': '..', 'domain': '..', 'title': '..', 'title_standardized': '..',\n",
    "                            'description': '...', 'description_standardized': '...']}\n",
    "trainingDataLikelihood: {'positive': {'word': likelihood}, {'word2', likelihood}...,\n",
    "                         'negative': {'word': likelihood}, {'word2', likelihood}...}\n",
    "pPropability: {'positive': float_value, 'negative': float_value}\n",
    "'''\n",
    "def classification_result(testingData, trainingDataLikelihood, pPropability):\n",
    "    predictiveValues = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "    articlesWeight = []\n",
    "    \n",
    "    for _class in testingData:\n",
    "        for article in testingData[_class]:\n",
    "            articleWeight = calculate_weigh_by_article(article, trainingDataLikelihood)\n",
    "\n",
    "            predictiveParameter = find_predictive_parameter(_class, articleWeight)\n",
    "            add_to_predictiveValues(predictiveValues, predictiveParameter)\n",
    "            \n",
    "            articlesWeight.append(articleWeight)\n",
    "\n",
    "    positive, negative = calculate_positive_vs_negative(articlesWeight)\n",
    "    accuracy = calculate_accuracy(predictiveValues[\"TP\"],\n",
    "                       predictiveValues[\"FP\"],\n",
    "                       predictiveValues[\"FN\"],\n",
    "                       predictiveValues[\"TN\"])\n",
    "\n",
    "    return accuracy, positive, negative\n",
    "\n",
    "\n",
    "# Input: Standardized article object\n",
    "def process_classification(articles, displayInfo):\n",
    "    classificationObject = convert_to_classification_object(articles)\n",
    "    frequencies = word_counter(classificationObject[\"training\"])\n",
    "\n",
    "    # Prior propabilities for positive and negative\n",
    "    pPropability = {'positive': 0.0, 'negative': 0.0}\n",
    "    pPropability['positive'] = calculate_prior_propabilities(frequencies, \"positive\", \"negative\")\n",
    "    pPropability['negative'] = calculate_prior_propabilities(frequencies, \"negative\", \"positive\")\n",
    "\n",
    "    accuracy, positive, negative = classification_result(classificationObject[\"testing\"],\n",
    "                                                          calculate_likelihood(frequencies),\n",
    "                                                          pPropability)\n",
    "\n",
    "    allPositive = [*classificationObject[\"training\"][\"positive\"], *classificationObject[\"testing\"][\"positive\"]]\n",
    "    \n",
    "    display_classification_results(accuracy,\n",
    "                                   positive,\n",
    "                                   negative,\n",
    "                                   get_most_popular_named_properties(allPositive),\n",
    "                                   displayInfo)\n",
    "\n",
    "#articles = get_articles([\"wsj.com\"])\n",
    "#standardized = standardizeList(articles[0])\n",
    "\n",
    "#process_classification(standardized, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "enormous-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200101 20200131\n",
      "No articles found for year: 2020, month: 1, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200201 20200229\n",
      "No articles found for year: 2020, month: 2, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200301 20200331\n",
      "No articles found for year: 2020, month: 3, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200401 20200430\n",
      "No articles found for year: 2020, month: 4, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200501 20200531\n",
      "No articles found for year: 2020, month: 5, for domains ['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com']\n",
      "['wsj.com', 'aljazeera.com', 'bfmtv.com', 'france24.com'] 20200601 20200630\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-b5468871068f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Mine all articles for each month 2020\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtext_mine_articles_by_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2020\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Mine all artiles by an outlet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-b5468871068f>\u001b[0m in \u001b[0;36mtext_mine_articles_by_month\u001b[0;34m(domains, year, month)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No articles found for year: {year}, month: {month}, for domains {domains}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mstandardize_and_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_display_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateTo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Mine all articles from 2020\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-b5468871068f>\u001b[0m in \u001b[0;36mstandardize_and_classify\u001b[0;34m(articles, displayInfo)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandardize_and_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstandardized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardizeList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprocess_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-198c2c7c6ff8>\u001b[0m in \u001b[0;36mstandardizeList\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandardizeList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title_standardized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;31m#article[\"title_entities\"] = entity_extraction(article[\"title\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description_standardized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-198c2c7c6ff8>\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordTokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mnoStopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mnoPunctuations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoStopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnoPunctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-198c2c7c6ff8>\u001b[0m in \u001b[0;36mremove_punctuations\u001b[0;34m(wordList)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mreturnList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mreturnList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturnList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not int"
     ]
    }
   ],
   "source": [
    "def create_display_info(domains, dateFrom, dateTo):\n",
    "    return {\n",
    "        \"domains\": domains,\n",
    "        \"dateFrom\": dateFrom,\n",
    "        \"dateTo\": dateTo\n",
    "    }\n",
    "    \n",
    "\n",
    "def standardize_and_classify(articles, displayInfo):\n",
    "    standardized = standardizeList(articles)\n",
    "    process_classification(standardized, displayInfo)\n",
    "\n",
    "def text_mine_articles(domains, dateFrom=\"\", dateTo=\"\"):\n",
    "    articles, dateFrom, dateTo = get_articles(domains,\"20200101\")\n",
    "\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found from {dateFrom} to {dateTo}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles, create_display_info(domains, dateFrom, dateTo))\n",
    "\n",
    "def text_mine_articles_by_month(domains, year, month):\n",
    "    articles, dateFrom, dateTo = get_articles_by_month(get_domains(), year, month)\n",
    "\n",
    "    if(articles is None or len(articles) == 0):\n",
    "        print(f\"No articles found for year: {year}, month: {month}, for domains {domains}\")\n",
    "    else:\n",
    "        standardize_and_classify(articles, create_display_info(domains, dateFrom, dateTo))\n",
    "\n",
    "# Mine all articles from 2020\n",
    "#text_mine_articles(get_domains(), \"20200101\")\n",
    "\n",
    "# Mine all articles for each month 2020\n",
    "for a in list(range(1, 13)):\n",
    "    text_mine_articles_by_month(get_domains(), \"2020\", str(a))\n",
    "\n",
    "# Mine all artiles by an outlet\n",
    "#text_mine_articles([\"wsj.com\"])\n",
    "\n",
    "#for m in [{\"20200101\", \"20200131\"}]\n",
    "#text_mine_articles(\"20190101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
